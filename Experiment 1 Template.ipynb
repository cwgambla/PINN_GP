{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8421e303-15ed-4e79-befe-89f2d9e3430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import qmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6b40f8e-796c-4f9b-b12c-bf274a877370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the Berger Viscous Equation parameters\n",
    "c = 1.0    # Wave speed\n",
    "mu = 0.1   # Viscosity coefficient\n",
    "lam = 1.0  # Nonlinearity coefficient\n",
    "\n",
    "# Define the neural network\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(*[\n",
    "            nn.Sequential(nn.Linear(layers[i], layers[i+1]), nn.Tanh())\n",
    "            for i in range(len(layers)-2)\n",
    "        ] + [nn.Linear(layers[-2], layers[-1])])\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inputs = torch.cat((x, t), dim=1)\n",
    "        return self.net(inputs)\n",
    "\n",
    "# Generate collocation points\n",
    "def generate_collocation_points(N_f, L=1.0, T=1.0):\n",
    "    # Use Latin Hypercube Sampling for better distribution\n",
    "    sampler = qmc.LatinHypercube(d=2)  # 2D (x, t) space\n",
    "    sample = sampler.random(N_f)  # Generate N_f samples in [0, 1]^2\n",
    "\n",
    "    # Scale samples to [0, L] and [0, T]\n",
    "    x_f = torch.tensor(sample[:, 0] * L, dtype=torch.float32).reshape(-1, 1)\n",
    "    t_f = torch.tensor(sample[:, 1] * T, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    return x_f.to(device), t_f.to(device)\n",
    "\n",
    "# Compute PDE residual using automatic differentiation\n",
    "def compute_pde_residual(model, x, t):\n",
    "    x.requires_grad = True\n",
    "    t.requires_grad = True\n",
    "\n",
    "    u = model(x, t)  # Predict u(x, t)\n",
    "    \n",
    "    u_t = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True)[0]\n",
    "    u_tt = torch.autograd.grad(u_t, t, torch.ones_like(u), create_graph=True)[0]\n",
    "    \n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    f = u_tt - c**2 * u_xx - mu * u_xx + lam * u * u_xx  # PDE residual\n",
    "    return f\n",
    "\n",
    "# Define loss function\n",
    "def loss_function(model, x_f, t_f, x_bc, t_bc, u_bc):\n",
    "    f_residual = compute_pde_residual(model, x_f, t_f)\n",
    "    loss_pde = torch.mean(f_residual**2)\n",
    "\n",
    "    u_pred_bc = model(x_bc, t_bc)\n",
    "    loss_bc = torch.mean((u_pred_bc - u_bc)**2)\n",
    "\n",
    "    return loss_pde + loss_bc\n",
    "\n",
    "# Training loop\n",
    "def train(model, optimizer, x_f, t_f, x_bc, t_bc, u_bc, epochs=5000):\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c76a43de-0523-49f7-9ae4-96cf18e0d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance sampling: Generate collocation points based on residuals\n",
    "def gen_points_import(model, N_f, L=1.0, T=1.0):\n",
    "    x_f = torch.rand(N_f, 1, device=device, requires_grad=True) * L  # x in [0, L]\n",
    "    t_f = torch.rand(N_f, 1, device=device, requires_grad=True) * T  # t in [0, T]\n",
    "    \n",
    "    if model is not None:  # Perform importance sampling\n",
    "        residuals = compute_pde_residual(model, x_f, t_f).detach()\n",
    "        probabilities = residuals.abs() / torch.sum(residuals.abs())  # Normalize to form a probability distribution\n",
    "        sampled_indices = torch.multinomial(probabilities.view(-1), N_f, replacement=True)\n",
    "        x_f, t_f = x_f[sampled_indices], t_f[sampled_indices]\n",
    "    \n",
    "    return x_f, t_f\n",
    "\n",
    "def train_import(model, optimizer, N_f, x_f,t_f,x_bc, t_bc, u_bc, epochs=10000, resample_every=500):\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % resample_every == 0 and epoch >=500:\n",
    "            x_f, t_f = gen_points_import(model, N_f)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "37484f6d-556d-4dcc-bb69-a08999975634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gaussian Process Model for Importance Sampling\n",
    "class ResidualGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ResidualGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Importance sampling using Gaussian Process\n",
    "def generate_collocation_points_with_gp(model, N_f, L=1.0, T=1.0):\n",
    "    x_f = torch.rand(N_f, 1, device=device, requires_grad=True) * L  # x in [0, L]\n",
    "    t_f = torch.rand(N_f, 1, device=device, requires_grad=True) * T  # t in [0, T]\n",
    "    \n",
    "    if model is not None:\n",
    "        residuals = compute_pde_residual(model, x_f, t_f).detach()  # Detach from computational graph\n",
    "        residuals = residuals.view(-1)  # Ensure residuals are 1D for GP\n",
    "\n",
    "        # Train Gaussian Process to model residuals\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        gp_model = ResidualGP(torch.cat([x_f, t_f], dim=1), residuals, likelihood).to(device)\n",
    "\n",
    "        gp_model.train()\n",
    "        likelihood.train()\n",
    "        optimizer = torch.optim.Adam(gp_model.parameters(), lr=0.001)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
    "\n",
    "        for _ in range(50):  # GP training (keep small for efficiency)\n",
    "            optimizer.zero_grad()\n",
    "            output = gp_model(torch.cat([x_f, t_f], dim=1))\n",
    "            loss = -mll(output, residuals)\n",
    "            loss.mean().backward(retain_graph=True)  # Ensure backward works properly\n",
    "            optimizer.step()\n",
    "\n",
    "        # Predict residual mean and variance\n",
    "        gp_model.eval()\n",
    "        likelihood.eval()\n",
    "        with torch.no_grad():\n",
    "            test_x = torch.cat([x_f, t_f], dim=1)\n",
    "            gp_prediction = gp_model(test_x)  # Directly use GP output\n",
    "            residual_mean = gp_prediction.mean  # Mean prediction\n",
    "            residual_std = gp_prediction.variance.sqrt()  # Correct way to compute uncertainty\n",
    "\n",
    "        # Define importance score = |mean residual| + Î± * uncertainty\n",
    "        alpha = 0.9  # Trade-off between exploitation and exploration\n",
    "        importance_score = residual_mean.abs() + alpha * residual_std\n",
    "        probabilities = importance_score / importance_score.sum()  # Normalize\n",
    "        \n",
    "        # Resample points based on importance scores\n",
    "        sampled_indices = torch.multinomial(probabilities.view(-1), N_f, replacement=True)\n",
    "        x_f, t_f = x_f[sampled_indices], t_f[sampled_indices]\n",
    "\n",
    "    return x_f, t_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ab7884cc-8df1-4190-9460-107c83af5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GP(model, optimizer, N_f, x_f,t_f,x_bc, t_bc, u_bc, epochs=5000, resample_every=500):\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % resample_every == 0 and epoch >=500:\n",
    "            x_f, t_f = gen_points_import(model, N_f)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            # x_f,t_f = fit_GP(x_f,t_f)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            x_uncertain, t_uncertain = generate_collocation_points_with_gp(model,N_f,x_f, t_f) \n",
    "            x_f = x_uncertain\n",
    "            t_f = t_uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa2106d3-581f-42ed-a9dc-ce85aa139140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_residual(model, x, t):\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    t = t.clone().detach().requires_grad_(True)\n",
    "\n",
    "    u = model(x, t)  # Predict u(x, t)\n",
    "    \n",
    "    u_t = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    u_tt = torch.autograd.grad(u_t, t, torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    \n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    f = u_tt - c**2 * u_xx - mu * u_xx + lam * u * u_xx  # PDE residual\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e259b57d-2896-42ba-a676-d29978919577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#different initial/boundary conditions\n",
    "def initial_condition(x, condition_type=\"sin\"):\n",
    "    if condition_type == \"sin\":\n",
    "        return torch.sin(np.pi * x.cpu()).to(device)\n",
    "    elif condition_type == \"gaussian\":\n",
    "        return torch.exp(-10 * (x - 0.5) ** 2).to(device)\n",
    "    elif condition_type == \"step\":\n",
    "        return torch.where(x < 0.5, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown initial condition type\")\n",
    "\n",
    "def boundary_condition(x, t, boundary_type=\"dirichlet\"):\n",
    "    if boundary_type == \"dirichlet\":\n",
    "        return torch.zeros_like(x).to(device)  # u(0,t) = 0, u(L,t) = 0\n",
    "    elif boundary_type == \"neumann\":\n",
    "        return torch.autograd.grad(model(x, t), x, torch.ones_like(x), create_graph=True)[0]\n",
    "    elif boundary_type == \"periodic\":\n",
    "        return model(x, t) - model(x + L, t)  # u(0,t) = u(L,t)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown boundary condition type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "46ceaef5-ced1-40cf-a558-3fec60f301bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with IC: sin, BC: dirichlet\n",
      "Epoch 0, Loss: 0.6114212274551392\n",
      "Epoch 500, Loss: 0.008377736434340477\n",
      "Epoch 1000, Loss: 0.0015274215256795287\n",
      "Epoch 1500, Loss: 0.0005380272632464767\n",
      "Epoch 2000, Loss: 0.004294271115213633\n",
      "Epoch 2500, Loss: 0.00013752855011262\n",
      "Epoch 3000, Loss: 9.729313023854047e-05\n",
      "Epoch 3500, Loss: 7.361101597780362e-05\n",
      "Epoch 4000, Loss: 5.8993955462938175e-05\n",
      "Epoch 4500, Loss: 4.8845584387890995e-05\n",
      "Validation loss base: 0.000504104420542717\n",
      "Epoch 0, Loss: 0.5037978291511536\n",
      "Epoch 500, Loss: 0.031285036355257034\n",
      "Epoch 1000, Loss: 0.005235261283814907\n",
      "Epoch 1500, Loss: 0.005714646074920893\n",
      "Epoch 2000, Loss: 0.0015815077349543571\n",
      "Epoch 2500, Loss: 0.0008865875424817204\n",
      "Epoch 3000, Loss: 0.0012352521298453212\n",
      "Epoch 3500, Loss: 0.0004845723742619157\n",
      "Epoch 4000, Loss: 0.0004020536143798381\n",
      "Epoch 4500, Loss: 0.0014945417642593384\n",
      "Validation loss import: 9.415707609150559e-05\n",
      "Epoch 0, Loss: 0.00010406879300717264\n",
      "Epoch 500, Loss: 0.000168681115610525\n",
      "Epoch 1000, Loss: 0.00019706189050339162\n",
      "Epoch 1500, Loss: 0.00016718982078600675\n",
      "Epoch 2000, Loss: 0.0001545467966934666\n",
      "Epoch 2500, Loss: 0.0004055397293996066\n",
      "Epoch 3000, Loss: 0.00020223094907123595\n",
      "Epoch 3500, Loss: 0.00023698393488302827\n",
      "Epoch 4000, Loss: 0.0001328369980910793\n",
      "Epoch 4500, Loss: 0.0002555459795985371\n",
      "Validation loss gauss: 1.0163946151733398\n",
      "Training with IC: sin, BC: neumann\n",
      "Epoch 0, Loss: 0.3678014874458313\n",
      "Epoch 500, Loss: 0.012009837664663792\n",
      "Epoch 1000, Loss: 0.0042219702154397964\n",
      "Epoch 1500, Loss: 0.001841895398683846\n",
      "Epoch 2000, Loss: 0.017686419188976288\n",
      "Epoch 2500, Loss: 0.0009021421428769827\n",
      "Epoch 3000, Loss: 0.0006641040672548115\n",
      "Epoch 3500, Loss: 0.000575573300011456\n",
      "Epoch 4000, Loss: 0.0006207377300597727\n",
      "Epoch 4500, Loss: 0.0006988375680521131\n",
      "Validation loss base: 0.007266659755259752\n",
      "Epoch 0, Loss: 0.4111076891422272\n",
      "Epoch 500, Loss: 0.016132552176713943\n",
      "Epoch 1000, Loss: 0.00890779122710228\n",
      "Epoch 1500, Loss: 0.0203772634267807\n",
      "Epoch 2000, Loss: 0.00254610413685441\n",
      "Epoch 2500, Loss: 0.005153632257133722\n",
      "Epoch 3000, Loss: 0.0016660263063386083\n",
      "Epoch 3500, Loss: 0.00043215916957706213\n",
      "Epoch 4000, Loss: 0.0008662582840770483\n",
      "Epoch 4500, Loss: 0.0006732706096954644\n",
      "Validation loss import: 0.0007297953707166016\n",
      "Epoch 0, Loss: 0.0008098029065877199\n",
      "Epoch 500, Loss: 0.0012896739644929767\n",
      "Epoch 1000, Loss: 0.001598394475877285\n",
      "Epoch 1500, Loss: 0.0009569533867761493\n",
      "Epoch 2000, Loss: 0.003219806356355548\n",
      "Epoch 2500, Loss: 0.0011688289232552052\n",
      "Epoch 3000, Loss: 0.0014708914095535874\n",
      "Epoch 3500, Loss: 0.0015593614662066102\n",
      "Epoch 4000, Loss: 0.0011949000181630254\n",
      "Epoch 4500, Loss: 0.0011111173080280423\n",
      "Validation loss gauss: 0.3628251552581787\n",
      "Training with IC: sin, BC: periodic\n",
      "Epoch 0, Loss: 0.5627121925354004\n",
      "Epoch 500, Loss: 0.005870733875781298\n",
      "Epoch 1000, Loss: 0.0017889610026031733\n",
      "Epoch 1500, Loss: 0.0008077924139797688\n",
      "Epoch 2000, Loss: 0.0005326616810634732\n",
      "Epoch 2500, Loss: 0.0005502300919033587\n",
      "Epoch 3000, Loss: 0.0002700746990740299\n",
      "Epoch 3500, Loss: 0.0002133444359060377\n",
      "Epoch 4000, Loss: 0.00017443951219320297\n",
      "Epoch 4500, Loss: 0.00022397292195819318\n",
      "Validation loss base: 0.00013123096141498536\n",
      "Epoch 0, Loss: 0.30765780806541443\n",
      "Epoch 500, Loss: 0.014647361822426319\n",
      "Epoch 1000, Loss: 0.0024830279871821404\n",
      "Epoch 1500, Loss: 0.000997169059701264\n",
      "Epoch 2000, Loss: 0.0008140501449815929\n",
      "Epoch 2500, Loss: 0.0005022834520787001\n",
      "Epoch 3000, Loss: 0.00045527343172580004\n",
      "Epoch 3500, Loss: 0.00965703371912241\n",
      "Epoch 4000, Loss: 0.0007401774055324495\n",
      "Epoch 4500, Loss: 0.00027843931457027793\n",
      "Validation loss import: 9.072692773770541e-05\n",
      "Epoch 0, Loss: 8.48908384796232e-05\n",
      "Epoch 500, Loss: 0.00019448016246315092\n",
      "Epoch 1000, Loss: 0.00025367093621753156\n",
      "Epoch 1500, Loss: 0.00026337700546719134\n",
      "Epoch 2000, Loss: 0.00033954152604565024\n",
      "Epoch 2500, Loss: 0.0003968870732933283\n",
      "Epoch 3000, Loss: 0.0003731134638655931\n",
      "Epoch 3500, Loss: 0.0002964931773021817\n",
      "Epoch 4000, Loss: 0.00019495839660521597\n",
      "Epoch 4500, Loss: 0.00028872385155409575\n",
      "Validation loss gauss: 0.5754805207252502\n",
      "Training with IC: gaussian, BC: dirichlet\n",
      "Epoch 0, Loss: 0.43693724274635315\n",
      "Epoch 500, Loss: 0.004390344955027103\n",
      "Epoch 1000, Loss: 0.002570750657469034\n",
      "Epoch 1500, Loss: 0.0020596878603100777\n",
      "Epoch 2000, Loss: 0.0016833047848194838\n",
      "Epoch 2500, Loss: 0.001255257404409349\n",
      "Epoch 3000, Loss: 0.0011275305878371\n",
      "Epoch 3500, Loss: 0.0006019111024215817\n",
      "Epoch 4000, Loss: 0.0004961963277310133\n",
      "Epoch 4500, Loss: 0.00042822666000574827\n",
      "Validation loss base: 0.0004037104663439095\n",
      "Epoch 0, Loss: 0.15310579538345337\n",
      "Epoch 500, Loss: 0.00622903648763895\n",
      "Epoch 1000, Loss: 0.010852077975869179\n",
      "Epoch 1500, Loss: 0.0026630586944520473\n",
      "Epoch 2000, Loss: 0.0016674046637490392\n",
      "Epoch 2500, Loss: 0.002294651698321104\n",
      "Epoch 3000, Loss: 0.0013696058886125684\n",
      "Epoch 3500, Loss: 0.00102612201590091\n",
      "Epoch 4000, Loss: 0.03709866479039192\n",
      "Epoch 4500, Loss: 0.0026076696813106537\n",
      "Validation loss import: 0.0010237301466986537\n",
      "Epoch 0, Loss: 0.0010164480190724134\n",
      "Epoch 500, Loss: 0.0017334988806396723\n",
      "Epoch 1000, Loss: 0.002066686050966382\n",
      "Epoch 1500, Loss: 0.0017309659160673618\n",
      "Epoch 2000, Loss: 0.0017949726898223162\n",
      "Epoch 2500, Loss: 0.0014983115252107382\n",
      "Epoch 3000, Loss: 0.0020377470646053553\n",
      "Epoch 3500, Loss: 0.0017314082942903042\n",
      "Epoch 4000, Loss: 0.00174276577308774\n",
      "Epoch 4500, Loss: 0.0018946141935884953\n",
      "Validation loss gauss: 0.25295883417129517\n",
      "Training with IC: gaussian, BC: neumann\n",
      "Epoch 0, Loss: 0.30717021226882935\n",
      "Epoch 500, Loss: 0.013047042302787304\n",
      "Epoch 1000, Loss: 0.005619808100163937\n",
      "Epoch 1500, Loss: 0.0036789888981729746\n",
      "Epoch 2000, Loss: 0.0027819452807307243\n",
      "Epoch 2500, Loss: 0.002418649150058627\n",
      "Epoch 3000, Loss: 0.0022740554995834827\n",
      "Epoch 3500, Loss: 0.0052804481238126755\n",
      "Epoch 4000, Loss: 0.001767719048075378\n",
      "Epoch 4500, Loss: 0.0015677784103900194\n",
      "Validation loss base: 0.0015172421699389815\n",
      "Epoch 0, Loss: 0.21434497833251953\n",
      "Epoch 500, Loss: 0.015926416963338852\n",
      "Epoch 1000, Loss: 0.00727506261318922\n",
      "Epoch 1500, Loss: 0.0227254256606102\n",
      "Epoch 2000, Loss: 0.003724100533872843\n",
      "Epoch 2500, Loss: 0.002960022073239088\n",
      "Epoch 3000, Loss: 0.009193383157253265\n",
      "Epoch 3500, Loss: 0.0020463168621063232\n",
      "Epoch 4000, Loss: 0.0016570317093282938\n",
      "Epoch 4500, Loss: 0.001546721439808607\n",
      "Validation loss import: 0.0007454796577803791\n",
      "Epoch 0, Loss: 0.0007391910767182708\n",
      "Epoch 500, Loss: 0.0010148824658244848\n",
      "Epoch 1000, Loss: 0.001216113450936973\n",
      "Epoch 1500, Loss: 0.0011200315784662962\n",
      "Epoch 2000, Loss: 0.000983593286946416\n",
      "Epoch 2500, Loss: 0.0009044423932209611\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m model_GP \u001b[38;5;241m=\u001b[39m PINN(layers)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_GP\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m train_GP(model_import, optimizer, N_f, x_f,t_f,x_bc, t_bc, u_bc, epochs\u001b[38;5;241m=\u001b[39mepochs, resample_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     58\u001b[0m t_val_bc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(x_bc)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     59\u001b[0m u_val_bc \u001b[38;5;241m=\u001b[39m initial_condition(x_bc, ic)\n",
      "Cell \u001b[0;32mIn[78], line 7\u001b[0m, in \u001b[0;36mtrain_GP\u001b[0;34m(model, optimizer, N_f, x_f, t_f, x_bc, t_bc, u_bc, epochs, resample_every)\u001b[0m\n\u001b[1;32m      4\u001b[0m     x_f, t_f \u001b[38;5;241m=\u001b[39m gen_points_import(model, N_f)\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[59], line 52\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(model, x_f, t_f, x_bc, t_bc, u_bc)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_function\u001b[39m(model, x_f, t_f, x_bc, t_bc, u_bc):\n\u001b[0;32m---> 52\u001b[0m     f_residual \u001b[38;5;241m=\u001b[39m compute_pde_residual(model, x_f, t_f)\n\u001b[1;32m     53\u001b[0m     loss_pde \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(f_residual\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     55\u001b[0m     u_pred_bc \u001b[38;5;241m=\u001b[39m model(x_bc, t_bc)\n",
      "Cell \u001b[0;32mIn[79], line 10\u001b[0m, in \u001b[0;36mcompute_pde_residual\u001b[0;34m(model, x, t)\u001b[0m\n\u001b[1;32m      7\u001b[0m u_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u, t, torch\u001b[38;5;241m.\u001b[39mones_like(u), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m u_tt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u_t, t, torch\u001b[38;5;241m.\u001b[39mones_like(u), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m u_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u, x, torch\u001b[38;5;241m.\u001b[39mones_like(u), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m u_xx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u_x, x, torch\u001b[38;5;241m.\u001b[39mones_like(u), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m f \u001b[38;5;241m=\u001b[39m u_tt \u001b[38;5;241m-\u001b[39m c\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m u_xx \u001b[38;5;241m-\u001b[39m mu \u001b[38;5;241m*\u001b[39m u_xx \u001b[38;5;241m+\u001b[39m lam \u001b[38;5;241m*\u001b[39m u \u001b[38;5;241m*\u001b[39m u_xx  \u001b[38;5;66;03m# PDE residual\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Data_Science_V2/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m _engine_run_backward(\n\u001b[1;32m    497\u001b[0m         outputs,\n\u001b[1;32m    498\u001b[0m         grad_outputs_,\n\u001b[1;32m    499\u001b[0m         retain_graph,\n\u001b[1;32m    500\u001b[0m         create_graph,\n\u001b[1;32m    501\u001b[0m         inputs,\n\u001b[1;32m    502\u001b[0m         allow_unused,\n\u001b[1;32m    503\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/Data_Science_V2/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Testing multiple conditions\n",
    "layers = [2, 50, 50, 50, 1]  # Input (x,t) -> Hidden layers -> Output (u)\n",
    "model = PINN(layers).to(device)\n",
    "N_f = 1000  # Collocation points\n",
    "L, T = 1.0, 1.0  # Spatial and time limits\n",
    "N_bc = 100\n",
    "epochs = 5000\n",
    "x_f, t_f = generate_collocation_points(N_f, L, T)\n",
    "initial_conditions = [\"sin\", \"gaussian\", \"step\"]\n",
    "boundary_conditions = [\"dirichlet\", \"neumann\", \"periodic\"]\n",
    "\n",
    "\n",
    "#validation\n",
    "x_val, t_val = generate_collocation_points(10000,L,T)\n",
    "x_bc = torch.linspace(0, L, N_bc).view(-1, 1).to(device)\n",
    "\n",
    "results_base = []\n",
    "results_import = []\n",
    "results_gauss = []\n",
    "# loss_function(model, x_f, t_f, x_bc, t_bc, u_bc).item()\n",
    "for ic in initial_conditions:\n",
    "    for bc in boundary_conditions:\n",
    "        print(f\"Training with IC: {ic}, BC: {bc}\")\n",
    "\n",
    "        # Generate boundary and initial condition data\n",
    "        x_bc = torch.linspace(0, L, N_bc).view(-1, 1).to(device)\n",
    "        t_bc = torch.zeros_like(x_bc).to(device)\n",
    "        u_bc = initial_condition(x_bc, ic)\n",
    "\n",
    "        # Train the model\n",
    "        model_base = PINN(layers).to(device)\n",
    "        optimizer = optim.Adam(model_base.parameters(), lr=1e-3)\n",
    "        train(model_base, optimizer, x_f, t_f, x_bc, t_bc, u_bc, epochs=epochs)\n",
    "\n",
    "        \n",
    "        t_val_bc = torch.zeros_like(x_bc).to(device)\n",
    "        u_val_bc = initial_condition(x_bc, ic)\n",
    "        loss_val = loss_function(model_base, x_val, t_val, x_bc, t_val_bc, u_val_bc).item()\n",
    "\n",
    "        print(\"Validation loss base:\",loss_val)\n",
    "        results_base.append([ic,bc,loss_val])\n",
    "        \n",
    "        model_import = PINN(layers).to(device)\n",
    "        optimizer = optim.Adam(model_import.parameters(), lr=1e-3)\n",
    "        train_import(model_import, optimizer, N_f, x_f,t_f,x_bc, t_bc, u_bc, epochs=epochs, resample_every=100)\n",
    "\n",
    "        t_val_bc = torch.zeros_like(x_bc).to(device)\n",
    "        u_val_bc = initial_condition(x_bc, ic)\n",
    "        loss_val = loss_function(model_import, x_val, t_val, x_bc, t_val_bc, u_val_bc).item()\n",
    "\n",
    "        print(\"Validation loss import:\",loss_val)\n",
    "        results_import.append([ic,bc,loss_val])\n",
    "\n",
    "        model_GP = PINN(layers).to(device)\n",
    "        optimizer = optim.Adam(model_GP.parameters(), lr=1e-3)\n",
    "        train_GP(model_import, optimizer, N_f, x_f,t_f,x_bc, t_bc, u_bc, epochs=epochs, resample_every=100)\n",
    "\n",
    "        t_val_bc = torch.zeros_like(x_bc).to(device)\n",
    "        u_val_bc = initial_condition(x_bc, ic)\n",
    "        loss_val = loss_function(model_GP, x_val, t_val, x_bc, t_val_bc, u_val_bc).item()\n",
    "\n",
    "        print(\"Validation loss gauss:\",loss_val)\n",
    "        results_gauss.append([ic,bc,loss_val])\n",
    "        # (Optional) Save results for later comparison\n",
    "        # np.save(f\"results_{ic}_{bc}.npy\", u_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd9c8b-18e1-4793-95a1-b494af56a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results_base:\n",
    "    print(result[0],result[1],\"loss\",result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e872a-7e8d-480c-b79e-48cee647c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results_import:\n",
    "    print(result[0],result[1],\"loss\",result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8771a42-c82a-42f9-b0d7-8eb2cf2021e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results_gauss:\n",
    "    print(result[0],result[1],\"loss\",result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e0ee8-9813-49e2-9a8c-67ee9e4ebed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
