{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8421e303-15ed-4e79-befe-89f2d9e3430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cadmin/anaconda3/envs/Data_Science_V2/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import qmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b40f8e-796c-4f9b-b12c-bf274a877370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the Berger Viscous Equation parameters\n",
    "c = 1.0    # Wave speed\n",
    "mu = 0.1   # Viscosity coefficient\n",
    "lam = 1.0  # Nonlinearity coefficient\n",
    "\n",
    "# Define the neural network\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(*[\n",
    "            nn.Sequential(nn.Linear(layers[i], layers[i+1]), nn.Tanh())\n",
    "            for i in range(len(layers)-2)\n",
    "        ] + [nn.Linear(layers[-2], layers[-1])])\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inputs = torch.cat((x, t), dim=1)\n",
    "        return self.net(inputs)\n",
    "\n",
    "def generate_collocation_points(N_f, L=1.0, T=1.0):\n",
    "    # Use Latin Hypercube Sampling for better distribution\n",
    "    sampler = qmc.LatinHypercube(d=2)  # 2D (x, t) space\n",
    "    sample = sampler.random(N_f)  # Generate N_f samples in [0, 1]^2\n",
    "\n",
    "    # Scale samples: x in [-L, L], t in [0, T]\n",
    "    x_f = torch.tensor((sample[:, 0] * 2 - 1) * L, dtype=torch.float32).reshape(-1, 1)\n",
    "    t_f = torch.tensor(sample[:, 1] * T, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    return x_f.to(device), t_f.to(device)\n",
    "def compute_pde_residual(model, x, t):\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    t = t.clone().detach().requires_grad_(True)\n",
    "\n",
    "    u = model(x, t)  # Predict u(x, t)\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    # Inviscid Burgers' equation: u_t + u * u_x = 0\n",
    "    f = u_t + u * u_x\n",
    "    return f\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "def loss_function(model, x_f, t_f, x_bc, t_bc, u_bc):\n",
    "    f_residual = compute_pde_residual(model, x_f, t_f)\n",
    "    loss_pde = torch.mean(f_residual**2)\n",
    "\n",
    "    u_pred_bc = model(x_bc, t_bc)\n",
    "    loss_bc = torch.mean((u_pred_bc - u_bc)**2)\n",
    "\n",
    "    return loss_pde + loss_bc\n",
    "\n",
    "def val_loss(model, x_f, t_f, x_bc, t_bc, u_bc):\n",
    "    f_residual = compute_pde_residual(model, x_f, t_f)\n",
    "    loss_pde = torch.mean(f_residual**2)\n",
    "\n",
    "    u_pred_bc = model(x_bc, t_bc)\n",
    "    loss_bc = torch.mean((u_pred_bc - u_bc)**2)\n",
    "\n",
    "    return loss_pde + loss_bc\n",
    "\n",
    "# Training loop\n",
    "def train(model, optimizer, N_f, x_f, t_f, valX, valT, x_bc, t_bc, u_bc, epochs=5000,threshold = 0.001):\n",
    "    val_scores = []\n",
    "    thresh_e = epochs\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = val_loss(model, valX, valT, x_bc, t_bc, u_bc)\n",
    "\n",
    "        if loss_val.item() < threshold and thresh_e >= epochs:\n",
    "            print(\"Threshold reach at:\",epoch)\n",
    "            print(\"Val loss:\",loss_val)\n",
    "            thresh_e = epoch\n",
    "\n",
    "        if epoch % 500 == 0 or epoch == epochs-1:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            val_scores.append(loss_val.item())\n",
    "        x_f,t_f = generate_collocation_points(N_f)\n",
    "\n",
    "    return thresh_e, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c76a43de-0523-49f7-9ae4-96cf18e0d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_points_import(model, N_f, L=1.0, T=1.0):\n",
    "    x_f = (torch.rand(N_f, 1, device=device, requires_grad=True) * 2 - 1) * L  # x in [-L, L]\n",
    "    t_f = torch.rand(N_f, 1, device=device, requires_grad=True) * T  # t in [0, T]\n",
    "    \n",
    "    if model is not None:  # Perform importance sampling\n",
    "        residuals = compute_pde_residual(model, x_f, t_f).detach()\n",
    "        probabilities = residuals.abs() / torch.sum(residuals.abs())\n",
    "        sampled_indices = torch.multinomial(probabilities.view(-1), N_f, replacement=True)\n",
    "        x_f, t_f = x_f[sampled_indices], t_f[sampled_indices]\n",
    "    \n",
    "    return x_f, t_f\n",
    "\n",
    "def train_import(model, optimizer, N_f, x_f,t_f,valX ,valT ,x_bc, t_bc, u_bc, epochs=10000,threshold = 0.001):\n",
    "\n",
    "    val_scores = []\n",
    "    thresh_e = epochs\n",
    "    for epoch in range(epochs):\n",
    "        # if epoch % resample_every == 0 and epoch >=500:\n",
    "        #     x_f, t_f = gen_points_import(model, N_f)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = val_loss(model, valX, valT, x_bc, t_bc, u_bc)\n",
    "\n",
    "        if loss_val.item() < threshold and thresh_e >= epochs:\n",
    "            print(\"Threshold reach at:\",epoch)\n",
    "            print(\"Val loss:\",loss_val)\n",
    "            thresh_e = epoch\n",
    "        if epoch % 500 == 0 or epoch == epochs-1:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            val_scores.append(loss_val.item())\n",
    "        x_f, t_f = gen_points_import(model, N_f)\n",
    "    return thresh_e, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37484f6d-556d-4dcc-bb69-a08999975634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Process Model for Importance Sampling\n",
    "class ResidualGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ResidualGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "    gpytorch.kernels.MaternKernel(nu=1.5)\n",
    ")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089f663f-0f93-4830-b833-771880b76128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def generate_collocation_points_with_gp(model, N_f, x_f, t_f, x_bc=None, t_bc=None, u_bc=None, \n",
    "                                        L=1.0, T=1.0, alpha=0.5, fraction_gp=0.5, residual_thresh=1e-3):\n",
    "    device = x_f.device\n",
    "\n",
    "    # === Step 1: Prepare GP training data ===\n",
    "    x_f,t_f = gen_points_import(model, N_f)\n",
    "    x_train = x_f\n",
    "    t_train = t_f\n",
    "    xt_train = torch.cat([x_train, t_train], dim=1).detach()\n",
    "\n",
    "    if model is not None:\n",
    "        with torch.no_grad():\n",
    "            u_train = model(x_train, t_train).detach().view(-1)\n",
    "            xt_all = xt_train\n",
    "            u_all = u_train\n",
    "\n",
    "            if x_bc is not None and t_bc is not None and u_bc is not None:\n",
    "                xt_bc = torch.cat([x_bc, t_bc], dim=1).detach()\n",
    "                u_bc = u_bc.detach().view(-1)\n",
    "                xt_all = torch.cat([xt_all, xt_bc], dim=0)\n",
    "                u_all = torch.cat([u_all, u_bc], dim=0)\n",
    "\n",
    "        # === Train GP ===\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        gp_model = ResidualGP(xt_all, u_all, likelihood).to(device)\n",
    "\n",
    "        gp_model.train()\n",
    "        likelihood.train()\n",
    "        optimizer = torch.optim.Adam(gp_model.parameters(), lr=0.01)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
    "\n",
    "        for _ in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            output = gp_model(xt_all)\n",
    "            loss = -mll(output, u_all)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        gp_model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # === Step 2: Generate candidate points ===\n",
    "        sampler = qmc.LatinHypercube(d=2)\n",
    "        sample = sampler.random(10 * N_f)\n",
    "        x_cand = torch.tensor(sample[:, 0] * L, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "        t_cand = torch.tensor(sample[:, 1] * T, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "        xt_cand = torch.cat([x_cand, t_cand], dim=1)\n",
    "\n",
    "        # === Step 3: Sample from GP posterior ===\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            dist = gp_model(xt_cand)\n",
    "            gp_samples = dist.rsample(torch.Size([1])) # shape: [1, N_cand]\n",
    "            gp_sample_abs = gp_samples.squeeze(0).abs().detach()  # [N_cand]\n",
    "\n",
    "        # === Step 4: Compute PDE residuals ===\n",
    "        x_cand.requires_grad_()\n",
    "        t_cand.requires_grad_()\n",
    "        residual = compute_pde_residual(model, x_cand, t_cand).detach().abs().view(-1)\n",
    "\n",
    "        # === Step 5: Normalize and combine scores ===\n",
    "        residual = torch.where(residual < residual_thresh, torch.tensor(0.0, device=device), residual)\n",
    "\n",
    "        sample_score = gp_sample_abs / (gp_sample_abs.sum() + 1e-8)\n",
    "        residual_score = residual / (residual.sum() + 1e-8)\n",
    "\n",
    "        sampling_score = alpha * sample_score + (1 - alpha) * residual_score\n",
    "        sampling_score = sampling_score / (sampling_score.sum() + 1e-8)\n",
    "\n",
    "        # === Step 6: Hybrid sampling ===\n",
    "        N_gp = int(fraction_gp * N_f)\n",
    "        N_rand = N_f - N_gp\n",
    "\n",
    "        sampled_indices_gp = torch.multinomial(sampling_score, N_gp, replacement=False)\n",
    "        sampled_indices_rand = torch.randint(0, len(x_cand), (N_rand,), device=device)\n",
    "        sampled_indices = torch.cat([sampled_indices_gp, sampled_indices_rand], dim=0)\n",
    "\n",
    "        x_f_new = x_cand[sampled_indices].detach().clone().requires_grad_()\n",
    "        t_f_new = t_cand[sampled_indices].detach().clone().requires_grad_()\n",
    "        uncertainty_top = gp_sample_abs[sampled_indices].detach().cpu()\n",
    "\n",
    "    else:\n",
    "        x_f_new = x_f.clone().detach().requires_grad_()\n",
    "        t_f_new = t_f.clone().detach().requires_grad_()\n",
    "        uncertainty_top = None\n",
    "        gp_model = None\n",
    "\n",
    "    return x_f_new, t_f_new, uncertainty_top, gp_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab7884cc-8df1-4190-9460-107c83af5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GP(model, optimizer, N_f, x_f,t_f,valX, valT,x_bc, t_bc, u_bc, epochs=5000, resample_every=500,threshold = 0.001):\n",
    "    thresh_e = epochs\n",
    "    val_scores = []\n",
    "    for epoch in range(epochs):\n",
    "        # if epoch % resample_every == 0 and epoch >=500:\n",
    "        #     x_f, t_f = gen_points_import(model, N_f)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = val_loss(model, valX, valT, x_bc, t_bc, u_bc)\n",
    "\n",
    "        if loss_val.item() < threshold and thresh_e >= epochs:\n",
    "            print(\"Threshold reach at:\",epoch)\n",
    "            print(\"Val loss:\",loss_val)\n",
    "            thresh_e = epoch\n",
    "\n",
    "        if epoch % 500 == 0 or epoch == epochs-1:\n",
    "            # x_f,t_f = fit_GP(x_f,t_f)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            val_scores.append(loss_val.item())\n",
    "            # x_uncertain, t_uncertain,x,g = generate_collocation_points_with_gp(model,N_f,x_f, t_f) \n",
    "        if epoch % resample_every == 0 and epoch > 0 :\n",
    "            # x_f, t_f = gen_points_import(model, N_f)\n",
    "            x_uncertain, t_uncertain,uncertainties,gp_model = generate_collocation_points_with_gp(model,N_f,x_f, t_f,x_bc,t_bc,u_bc)\n",
    "\n",
    "            x_f = x_uncertain\n",
    "            t_f = t_uncertain\n",
    "\n",
    "    return thresh_e, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b83f874c-9b12-44f8-a6a6-6d27357837ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def generate_collocation_points_with_gp_res(model, N_f, x_f, t_f, x_bc=None, t_bc=None, u_bc=None, \n",
    "                                            L=2.0, T=1.0, alpha=0.5, fraction_gp=0.5, residual_thresh=1e-3):\n",
    "\n",
    "    device = x_f.device\n",
    "    # x_f,t_f = generate_collocation_points(N_f)\n",
    "    x_f,t_f = gen_points_import(model, N_f)\n",
    "    x_train = x_f\n",
    "    t_train = t_f\n",
    "    xt_train = torch.cat([x_train, t_train], dim=1).detach()\n",
    "\n",
    "    if model is not None:\n",
    "        # ✅ Compute residuals (no torch.no_grad here)\n",
    "        residual_train = compute_pde_residual(model, x_train.requires_grad_(), t_train.requires_grad_()).detach().view(-1)\n",
    "\n",
    "        xt_all = xt_train\n",
    "        residual_all = residual_train\n",
    "\n",
    "        if x_bc is not None and t_bc is not None and u_bc is not None:\n",
    "            xt_bc = torch.cat([x_bc, t_bc], dim=1).detach()\n",
    "            residual_bc = compute_pde_residual(model, x_bc.requires_grad_(), t_bc.requires_grad_()).detach().view(-1)\n",
    "\n",
    "            xt_all = torch.cat([xt_all, xt_bc], dim=0)\n",
    "            residual_all = torch.cat([residual_all, residual_bc], dim=0)\n",
    "\n",
    "        # === Train GP on residuals ===\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        gp_model = ResidualGP(xt_all, residual_all, likelihood).to(device)\n",
    "\n",
    "        gp_model.train()\n",
    "        likelihood.train()\n",
    "        optimizer = torch.optim.Adam(gp_model.parameters(), lr=0.01)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
    "\n",
    "        for _ in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            output = gp_model(xt_all)\n",
    "            loss = -mll(output, residual_all)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        gp_model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # === Step 2: Candidate points ===\n",
    "        sampler = qmc.LatinHypercube(d=2)\n",
    "        sample = sampler.random(10 * N_f)\n",
    "\n",
    "        # x ∈ [-L/2, L/2], t ∈ [0, T]\n",
    "        x_cand = torch.tensor((sample[:, 0] * 2 - 1) * (L/2), dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "        t_cand = torch.tensor(sample[:, 1] * T, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "        xt_cand = torch.cat([x_cand, t_cand], dim=1)\n",
    "\n",
    "        # === Step 3 (Modified): Sample from GP posterior ===\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            dist = gp_model(xt_cand)\n",
    "            gp_samples = dist.rsample(torch.Size([1]))  # [1, N_cand]\n",
    "            gp_sample_abs = gp_samples.squeeze(0).abs().detach()  # [N_cand]\n",
    "  # [N_cand]\n",
    "\n",
    "        # === Step 4: Compute PDE residuals at candidate points\n",
    "        x_cand.requires_grad_()\n",
    "        t_cand.requires_grad_()\n",
    "        residual = compute_pde_residual(model, x_cand, t_cand).detach().abs().view(-1)\n",
    "\n",
    "        # === Step 5: Normalize and threshold ===\n",
    "        residual = torch.where(residual < residual_thresh, torch.tensor(0.0, device=device), residual)\n",
    "\n",
    "        sample_score = gp_sample_abs / (gp_sample_abs.sum() + 1e-8)\n",
    "        residual_score = residual / (residual.sum() + 1e-8)\n",
    "\n",
    "        sampling_score = alpha * sample_score + (1 - alpha) * residual_score\n",
    "        sampling_score = sampling_score / (sampling_score.sum() + 1e-8)\n",
    "\n",
    "        # === Step 6: Hybrid sampling ===\n",
    "        N_gp = int(fraction_gp * N_f)\n",
    "        N_rand = N_f - N_gp\n",
    "\n",
    "        sampled_indices_gp = torch.multinomial(sampling_score, N_gp, replacement=False)\n",
    "        sampled_indices_rand = torch.randint(0, len(x_cand), (N_rand,), device=device)\n",
    "        sampled_indices = torch.cat([sampled_indices_gp, sampled_indices_rand], dim=0)\n",
    "\n",
    "        x_f_new = x_cand[sampled_indices].detach().clone().requires_grad_()\n",
    "        t_f_new = t_cand[sampled_indices].detach().clone().requires_grad_()\n",
    "        uncertainty_top = gp_sample_abs[sampled_indices].detach().cpu()\n",
    "\n",
    "    else:\n",
    "        x_f_new = x_f.clone().detach().requires_grad_()\n",
    "        t_f_new = t_f.clone().detach().requires_grad_()\n",
    "        uncertainty_top = None\n",
    "        gp_model = None\n",
    "\n",
    "    return x_f_new, t_f_new, uncertainty_top, gp_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51db3c46-3c5a-4bc4-a7d7-81558bf68576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GP_res(model, optimizer, N_f, x_f,t_f,valX, valT,x_bc, t_bc, u_bc, epochs=5000, resample_every=500,threshold = 0.001):\n",
    "    val_scores = []\n",
    "    thresh_e = epochs\n",
    "    for epoch in range(epochs):        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = val_loss(model, valX, valT, x_bc, t_bc, u_bc)\n",
    "\n",
    "        if loss_val.item() < threshold and thresh_e >= epochs:\n",
    "            print(\"Threshold reach at:\",epoch)\n",
    "            print(\"Val loss:\",loss_val)\n",
    "            thresh_e = epoch\n",
    "\n",
    "        if epoch % 500 == 0 or epoch == epochs-1:\n",
    "            # x_f,t_f = fit_GP(x_f,t_f)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            val_scores.append(loss_val.item())\n",
    "            # x_uncertain, t_uncertain,x,g = generate_collocation_points_with_gp(model,N_f,x_f, t_f) \n",
    "        if epoch % resample_every == 0 and epoch > 0 :\n",
    "            x_f, t_f = gen_points_import(model, N_f)\n",
    "            x_uncertain, t_uncertain,uncertainties,gp_model = generate_collocation_points_with_gp_res(model,N_f,x_f, t_f,x_bc,t_bc,u_bc)\n",
    "\n",
    "            x_f = x_uncertain\n",
    "            t_f = t_uncertain\n",
    "\n",
    "    return thresh_e,val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a960c9a-7afd-4977-9793-aea4195061f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_residual(model, x, t):\n",
    "    x.requires_grad = True\n",
    "    t.requires_grad = True\n",
    "\n",
    "    u = model(x, t)  # Predict u(x, t)\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    # Inviscid Burgers' equation: u_t + u * u_x = 0\n",
    "    f = u_t + u * u_x\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "305b85cc-9b23-49de-897a-2dad4f3c2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_residual(model, x, t):\n",
    "    x = x.detach().requires_grad_()\n",
    "    t = t.detach().requires_grad_()\n",
    "\n",
    "    u = model(x, t)  # Predict u(x, t)\n",
    "\n",
    "    # Compute ∂u/∂t\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    # Compute ∂u/∂x\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    # Compute ∂²u/∂x²\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    # Burgers' equation residual: u_t + u * u_x - ν * u_xx\n",
    "    nu = 0.01 / np.pi\n",
    "    # residual = u_t + u * u_x - nu * u_xx\n",
    "    residual = u_t + u * u_x \n",
    "    return residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e259b57d-2896-42ba-a676-d29978919577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#different initial/boundary conditions\n",
    "def initial_condition(x, condition_type=\"sin\"):\n",
    "    if condition_type == \"sin\":\n",
    "        return (torch.sin(np.pi * x.cpu())).to(device)\n",
    "    elif condition_type == \"gaussian\":\n",
    "        # return torch.exp(-10 * (x - 0.5) ** 2).to(device)\n",
    "        mu=0.5\n",
    "        sigma=0.1\n",
    "        return torch.exp(-((x - mu)**2) / (2 * sigma**2))\n",
    "    elif condition_type == \"step\":\n",
    "        return torch.where(x < 0.5, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown initial condition type\")\n",
    "\n",
    "def boundary_condition(x, t, boundary_type=\"dirichlet\"):\n",
    "    if boundary_type == \"dirichlet\":\n",
    "        return torch.zeros_like(x).to(device)  # u(0,t) = 0, u(L,t) = 0\n",
    "    elif boundary_type == \"neumann\":\n",
    "        return torch.autograd.grad(model(x, t), x, torch.ones_like(x), create_graph=True)[0]\n",
    "    elif boundary_type == \"periodic\":\n",
    "        return model(x, t) - model(x + L, t)  # u(0,t) = u(L,t)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown boundary condition type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2d54f7c-761e-4574-8a9c-fae8981bc435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Running experiments for IC: sin, BC: dirichlet ====\n",
      "--- Experiment 1/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cadmin/anaconda3/envs/Data_Science_V2/lib/python3.11/site-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3819037079811096\n",
      "Epoch 500, Loss: 0.007066420279443264\n",
      "Epoch 1000, Loss: 0.003041023388504982\n",
      "Epoch 1500, Loss: 0.0016339098801836371\n",
      "Epoch 2000, Loss: 0.0013910778798162937\n",
      "Threshold reach at: 2321\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0011884781997650862\n",
      "Epoch 3000, Loss: 0.0006122581544332206\n",
      "Epoch 3500, Loss: 0.0004267664044164121\n",
      "Epoch 4000, Loss: 0.0007234815275296569\n",
      "Epoch 4500, Loss: 0.0009468795033171773\n",
      "Epoch 4999, Loss: 0.000689165957737714\n",
      "Epoch 0, Loss: 0.6155887842178345\n",
      "Epoch 500, Loss: 0.01669834554195404\n",
      "Epoch 1000, Loss: 0.010408066213130951\n",
      "Epoch 1500, Loss: 0.005557235796004534\n",
      "Epoch 2000, Loss: 0.004058976657688618\n",
      "Epoch 2500, Loss: 0.003212453331798315\n",
      "Epoch 3000, Loss: 0.0027661207132041454\n",
      "Epoch 3500, Loss: 0.002150405663996935\n",
      "Epoch 4000, Loss: 0.0018732609460130334\n",
      "Epoch 4500, Loss: 0.0019012223929166794\n",
      "Threshold reach at: 4698\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.0032520522363483906\n",
      "Epoch 0, Loss: 0.37002870440483093\n",
      "Epoch 500, Loss: 0.008511727675795555\n",
      "Epoch 1000, Loss: 0.004371621645987034\n",
      "Epoch 1500, Loss: 0.003291384782642126\n",
      "Epoch 2000, Loss: 0.002199670299887657\n",
      "Epoch 2500, Loss: 0.0016604315023869276\n",
      "Epoch 3000, Loss: 0.0014937752857804298\n",
      "Epoch 3500, Loss: 0.0012633842416107655\n",
      "Epoch 4000, Loss: 0.0009216811740770936\n",
      "Epoch 4500, Loss: 0.0008351326687261462\n",
      "Epoch 4999, Loss: 0.000792501145042479\n",
      "Epoch 0, Loss: 0.701740562915802\n",
      "Epoch 500, Loss: 0.010806326754391193\n",
      "Epoch 1000, Loss: 0.003719788510352373\n",
      "Epoch 1500, Loss: 0.0021180594339966774\n",
      "Epoch 2000, Loss: 0.0018113439437001944\n",
      "Epoch 2500, Loss: 0.0014294981956481934\n",
      "Threshold reach at: 2502\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0012011773651465774\n",
      "Epoch 3500, Loss: 0.0008857434149831533\n",
      "Epoch 4000, Loss: 0.0010097321355715394\n",
      "Epoch 4500, Loss: 0.0007536796620115638\n",
      "Epoch 4999, Loss: 0.0007460786146111786\n",
      "--- Experiment 2/20 ---\n",
      "Epoch 0, Loss: 0.39572378993034363\n",
      "Epoch 500, Loss: 0.00716985110193491\n",
      "Epoch 1000, Loss: 0.002654462121427059\n",
      "Epoch 1500, Loss: 0.0019283303990960121\n",
      "Epoch 2000, Loss: 0.001647924305871129\n",
      "Epoch 2500, Loss: 0.0019224929856136441\n",
      "Threshold reach at: 2591\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0009531100513413548\n",
      "Epoch 3500, Loss: 0.0006046445923857391\n",
      "Epoch 4000, Loss: 0.0007220504339784384\n",
      "Epoch 4500, Loss: 0.0007354296976700425\n",
      "Epoch 4999, Loss: 0.000368761015124619\n",
      "Epoch 0, Loss: 0.29209104180336\n",
      "Epoch 500, Loss: 0.016457565128803253\n",
      "Epoch 1000, Loss: 0.009676487185060978\n",
      "Epoch 1500, Loss: 0.006446245592087507\n",
      "Epoch 2000, Loss: 0.004915313795208931\n",
      "Epoch 2500, Loss: 0.00530538335442543\n",
      "Epoch 3000, Loss: 0.004139693919569254\n",
      "Epoch 3500, Loss: 0.0024352441541850567\n",
      "Epoch 4000, Loss: 0.002168318023905158\n",
      "Epoch 4500, Loss: 0.0017917947843670845\n",
      "Threshold reach at: 4532\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.001699746586382389\n",
      "Epoch 0, Loss: 0.5840592384338379\n",
      "Epoch 500, Loss: 0.013577206060290337\n",
      "Epoch 1000, Loss: 0.003362376242876053\n",
      "Epoch 1500, Loss: 0.0011576011311262846\n",
      "Epoch 2000, Loss: 0.0008546386379748583\n",
      "Epoch 2500, Loss: 0.0006584656657651067\n",
      "Epoch 3000, Loss: 0.000577920232899487\n",
      "Epoch 3500, Loss: 0.00046963663771748543\n",
      "Epoch 4000, Loss: 0.0003958729503210634\n",
      "Epoch 4500, Loss: 0.0002225129137514159\n",
      "Epoch 4999, Loss: 0.00030099472496658564\n",
      "Epoch 0, Loss: 0.4551544785499573\n",
      "Epoch 500, Loss: 0.009343883022665977\n",
      "Epoch 1000, Loss: 0.002960039535537362\n",
      "Epoch 1500, Loss: 0.0027052387595176697\n",
      "Epoch 2000, Loss: 0.001855478505603969\n",
      "Epoch 2500, Loss: 0.0014757532626390457\n",
      "Threshold reach at: 2672\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0013135697226971388\n",
      "Epoch 3500, Loss: 0.0009335695067420602\n",
      "Epoch 4000, Loss: 0.0006238989299163222\n",
      "Epoch 4500, Loss: 0.0006663042004220188\n",
      "Epoch 4999, Loss: 0.0005385010153986514\n",
      "--- Experiment 3/20 ---\n",
      "Epoch 0, Loss: 0.6044608950614929\n",
      "Epoch 500, Loss: 0.007365098223090172\n",
      "Epoch 1000, Loss: 0.003896052483469248\n",
      "Epoch 1500, Loss: 0.002871214412152767\n",
      "Epoch 2000, Loss: 0.0017440873198211193\n",
      "Epoch 2500, Loss: 0.0016707153990864754\n",
      "Epoch 3000, Loss: 0.0014915541978552938\n",
      "Epoch 3500, Loss: 0.0009477660059928894\n",
      "Epoch 4000, Loss: 0.001479079364798963\n",
      "Threshold reach at: 4261\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.0014500976540148258\n",
      "Epoch 4999, Loss: 0.0005957726389169693\n",
      "Epoch 0, Loss: 0.46495819091796875\n",
      "Epoch 500, Loss: 0.020779229700565338\n",
      "Epoch 1000, Loss: 0.01310061290860176\n",
      "Epoch 1500, Loss: 0.00939120166003704\n",
      "Epoch 2000, Loss: 0.006379307247698307\n",
      "Epoch 2500, Loss: 0.0047681378200650215\n",
      "Epoch 3000, Loss: 0.0046746935695409775\n",
      "Epoch 3500, Loss: 0.003622201271355152\n",
      "Epoch 4000, Loss: 0.0033227303065359592\n",
      "Epoch 4500, Loss: 0.003348476253449917\n",
      "Epoch 4999, Loss: 0.0022268318571150303\n",
      "Epoch 0, Loss: 0.6597878336906433\n",
      "Epoch 500, Loss: 0.014779866673052311\n",
      "Epoch 1000, Loss: 0.006240487098693848\n",
      "Epoch 1500, Loss: 0.0033899317495524883\n",
      "Epoch 2000, Loss: 0.002145393518730998\n",
      "Epoch 2500, Loss: 0.001450952491723001\n",
      "Epoch 3000, Loss: 0.0011072802590206265\n",
      "Epoch 3500, Loss: 0.00105873285792768\n",
      "Epoch 4000, Loss: 0.0009155664592981339\n",
      "Epoch 4500, Loss: 0.0007386255310848355\n",
      "Epoch 4999, Loss: 0.0007145680719986558\n",
      "Epoch 0, Loss: 0.3412953019142151\n",
      "Epoch 500, Loss: 0.005948021076619625\n",
      "Epoch 1000, Loss: 0.0019052340649068356\n",
      "Threshold reach at: 1410\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 1500, Loss: 0.0009956429712474346\n",
      "Epoch 2000, Loss: 0.0007775709964334965\n",
      "Epoch 2500, Loss: 0.0007122597307898104\n",
      "Epoch 3000, Loss: 0.0005640642484650016\n",
      "Epoch 3500, Loss: 0.00041225794120691717\n",
      "Epoch 4000, Loss: 0.000342960876878351\n",
      "Epoch 4500, Loss: 0.0003475869307294488\n",
      "Epoch 4999, Loss: 0.0002922199491877109\n",
      "--- Experiment 4/20 ---\n",
      "Epoch 0, Loss: 0.6531205773353577\n",
      "Epoch 500, Loss: 0.01137362327426672\n",
      "Epoch 1000, Loss: 0.005346783436834812\n",
      "Epoch 1500, Loss: 0.003570231143385172\n",
      "Epoch 2000, Loss: 0.0028814258985221386\n",
      "Epoch 2500, Loss: 0.002368026878684759\n",
      "Epoch 3000, Loss: 0.0016215802170336246\n",
      "Epoch 3500, Loss: 0.0012158178724348545\n",
      "Epoch 4000, Loss: 0.0009179967455565929\n",
      "Threshold reach at: 4181\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.001093892497010529\n",
      "Epoch 4999, Loss: 0.000809724791906774\n",
      "Epoch 0, Loss: 0.4832001030445099\n",
      "Epoch 500, Loss: 0.017943378537893295\n",
      "Epoch 1000, Loss: 0.015074482187628746\n",
      "Epoch 1500, Loss: 0.007232301868498325\n",
      "Epoch 2000, Loss: 0.0062972549349069595\n",
      "Epoch 2500, Loss: 0.012126750312745571\n",
      "Epoch 3000, Loss: 0.0038565536960959435\n",
      "Epoch 3500, Loss: 0.0033110124059021473\n",
      "Epoch 4000, Loss: 0.005248238332569599\n",
      "Epoch 4500, Loss: 0.00514868414029479\n",
      "Epoch 4999, Loss: 0.0029133781790733337\n",
      "Epoch 0, Loss: 0.31412240862846375\n",
      "Epoch 500, Loss: 0.0060735223814845085\n",
      "Epoch 1000, Loss: 0.0016061307396739721\n",
      "Epoch 1500, Loss: 0.0010129939764738083\n",
      "Epoch 2000, Loss: 0.0008722036145627499\n",
      "Epoch 2500, Loss: 0.0005768828559666872\n",
      "Epoch 3000, Loss: 0.0005191526142880321\n",
      "Epoch 3500, Loss: 0.0003926056670024991\n",
      "Epoch 4000, Loss: 0.00033568317303434014\n",
      "Epoch 4500, Loss: 0.00028169029974378645\n",
      "Epoch 4999, Loss: 0.0002395342744421214\n",
      "Epoch 0, Loss: 0.4680770933628082\n",
      "Epoch 500, Loss: 0.008330131880939007\n",
      "Epoch 1000, Loss: 0.0022260411642491817\n",
      "Epoch 1500, Loss: 0.0017573995282873511\n",
      "Threshold reach at: 1709\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0009250066941604018\n",
      "Epoch 2500, Loss: 0.0008822835516184568\n",
      "Epoch 3000, Loss: 0.0006784061552025378\n",
      "Epoch 3500, Loss: 0.0006062864558771253\n",
      "Epoch 4000, Loss: 0.0005771593423560262\n",
      "Epoch 4500, Loss: 0.0006381451385095716\n",
      "Epoch 4999, Loss: 0.000449598504928872\n",
      "--- Experiment 5/20 ---\n",
      "Epoch 0, Loss: 0.5759192705154419\n",
      "Epoch 500, Loss: 0.00786462239921093\n",
      "Epoch 1000, Loss: 0.004135306924581528\n",
      "Epoch 1500, Loss: 0.0023603057488799095\n",
      "Epoch 2000, Loss: 0.0014856370398774743\n",
      "Epoch 2500, Loss: 0.001709097996354103\n",
      "Epoch 3000, Loss: 0.0016711913049221039\n",
      "Threshold reach at: 3400\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0013803057372570038\n",
      "Epoch 4000, Loss: 0.0011372612789273262\n",
      "Epoch 4500, Loss: 0.0008819700451567769\n",
      "Epoch 4999, Loss: 0.0007342472672462463\n",
      "Epoch 0, Loss: 0.48211440443992615\n",
      "Epoch 500, Loss: 0.01600143499672413\n",
      "Epoch 1000, Loss: 0.0075780777260661125\n",
      "Epoch 1500, Loss: 0.007735664024949074\n",
      "Epoch 2000, Loss: 0.00375044671818614\n",
      "Epoch 2500, Loss: 0.0025156934279948473\n",
      "Epoch 3000, Loss: 0.0022007860243320465\n",
      "Threshold reach at: 3285\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.001596613205038011\n",
      "Epoch 4000, Loss: 0.0020383126102387905\n",
      "Epoch 4500, Loss: 0.001714568119496107\n",
      "Epoch 4999, Loss: 0.001099629676900804\n",
      "Epoch 0, Loss: 0.7677960395812988\n",
      "Epoch 500, Loss: 0.014975647442042828\n",
      "Epoch 1000, Loss: 0.004061603918671608\n",
      "Epoch 1500, Loss: 0.002223707502707839\n",
      "Epoch 2000, Loss: 0.0015369837637990713\n",
      "Epoch 2500, Loss: 0.001133897341787815\n",
      "Epoch 3000, Loss: 0.001013545785099268\n",
      "Epoch 3500, Loss: 0.0007580934907309711\n",
      "Epoch 4000, Loss: 0.0007822929183021188\n",
      "Epoch 4500, Loss: 0.0005813451716676354\n",
      "Epoch 4999, Loss: 0.0005986008909530938\n",
      "Epoch 0, Loss: 0.3247644603252411\n",
      "Epoch 500, Loss: 0.009227544069290161\n",
      "Epoch 1000, Loss: 0.0030614733695983887\n",
      "Epoch 1500, Loss: 0.002396349096670747\n",
      "Epoch 2000, Loss: 0.001342877047136426\n",
      "Threshold reach at: 2244\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0012978771701455116\n",
      "Epoch 3000, Loss: 0.0009755143546499312\n",
      "Epoch 3500, Loss: 0.0006968576926738024\n",
      "Epoch 4000, Loss: 0.0006324758287519217\n",
      "Epoch 4500, Loss: 0.0006458435673266649\n",
      "Epoch 4999, Loss: 0.0004658346879296005\n",
      "--- Experiment 6/20 ---\n",
      "Epoch 0, Loss: 0.6344566941261292\n",
      "Epoch 500, Loss: 0.009822709485888481\n",
      "Epoch 1000, Loss: 0.004652740433812141\n",
      "Epoch 1500, Loss: 0.0030187617521733046\n",
      "Epoch 2000, Loss: 0.0021609794348478317\n",
      "Epoch 2500, Loss: 0.0013176368083804846\n",
      "Epoch 3000, Loss: 0.00154712307266891\n",
      "Threshold reach at: 3321\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0007471064454875886\n",
      "Epoch 4000, Loss: 0.0007955071050673723\n",
      "Epoch 4500, Loss: 0.000564980786293745\n",
      "Epoch 4999, Loss: 0.00042093207594007254\n",
      "Epoch 0, Loss: 0.5845797061920166\n",
      "Epoch 500, Loss: 0.01780201494693756\n",
      "Epoch 1000, Loss: 0.00722920149564743\n",
      "Epoch 1500, Loss: 0.005234409123659134\n",
      "Epoch 2000, Loss: 0.0031647777650505304\n",
      "Epoch 2500, Loss: 0.002443460049107671\n",
      "Epoch 3000, Loss: 0.0018771751783788204\n",
      "Threshold reach at: 3182\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0013109124265611172\n",
      "Epoch 4000, Loss: 0.001991072902455926\n",
      "Epoch 4500, Loss: 0.0016263846773654222\n",
      "Epoch 4999, Loss: 0.0019725137390196323\n",
      "Epoch 0, Loss: 0.759079098701477\n",
      "Epoch 500, Loss: 0.012814205139875412\n",
      "Epoch 1000, Loss: 0.003947708290070295\n",
      "Epoch 1500, Loss: 0.0026077781803905964\n",
      "Epoch 2000, Loss: 0.0019933711737394333\n",
      "Epoch 2500, Loss: 0.001355883781798184\n",
      "Epoch 3000, Loss: 0.001503477105870843\n",
      "Epoch 3500, Loss: 0.0008624357869848609\n",
      "Epoch 4000, Loss: 0.0009127195808105171\n",
      "Epoch 4500, Loss: 0.0008050104370340705\n",
      "Epoch 4999, Loss: 0.0006961149047128856\n",
      "Epoch 0, Loss: 0.5431085228919983\n",
      "Epoch 500, Loss: 0.009284188970923424\n",
      "Epoch 1000, Loss: 0.0028099941555410624\n",
      "Threshold reach at: 1373\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 1500, Loss: 0.0015647532418370247\n",
      "Epoch 2000, Loss: 0.0010533721651881933\n",
      "Epoch 2500, Loss: 0.0008311773417517543\n",
      "Epoch 3000, Loss: 0.0006462045712396502\n",
      "Epoch 3500, Loss: 0.0005939957918599248\n",
      "Epoch 4000, Loss: 0.0005181711167097092\n",
      "Epoch 4500, Loss: 0.00041754532139748335\n",
      "Epoch 4999, Loss: 0.0002498459361959249\n",
      "--- Experiment 7/20 ---\n",
      "Epoch 0, Loss: 0.6705162525177002\n",
      "Epoch 500, Loss: 0.008003072813153267\n",
      "Epoch 1000, Loss: 0.002964689629152417\n",
      "Epoch 1500, Loss: 0.0014717159792780876\n",
      "Epoch 2000, Loss: 0.0012448711786419153\n",
      "Threshold reach at: 2223\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0006988086970523\n",
      "Epoch 3000, Loss: 0.0007483754307031631\n",
      "Epoch 3500, Loss: 0.0005325557431206107\n",
      "Epoch 4000, Loss: 0.0010334750404581428\n",
      "Epoch 4500, Loss: 0.0003812084614764899\n",
      "Epoch 4999, Loss: 0.0005229968810454011\n",
      "Epoch 0, Loss: 0.6476096510887146\n",
      "Epoch 500, Loss: 0.021441953256726265\n",
      "Epoch 1000, Loss: 0.014115428552031517\n",
      "Epoch 1500, Loss: 0.009918738156557083\n",
      "Epoch 2000, Loss: 0.0068806130439043045\n",
      "Epoch 2500, Loss: 0.0039267041720449924\n",
      "Epoch 3000, Loss: 0.00349352671764791\n",
      "Epoch 3500, Loss: 0.002699894830584526\n",
      "Epoch 4000, Loss: 0.004236261360347271\n",
      "Epoch 4500, Loss: 0.0015614982694387436\n",
      "Threshold reach at: 4980\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.006449149455875158\n",
      "Epoch 0, Loss: 0.3367633521556854\n",
      "Epoch 500, Loss: 0.013673416338860989\n",
      "Epoch 1000, Loss: 0.003542098682373762\n",
      "Epoch 1500, Loss: 0.001967943739145994\n",
      "Epoch 2000, Loss: 0.0010576583445072174\n",
      "Epoch 2500, Loss: 0.0009933210676535964\n",
      "Epoch 3000, Loss: 0.0007521435036323965\n",
      "Epoch 3500, Loss: 0.0006999235483817756\n",
      "Epoch 4000, Loss: 0.0005096490494906902\n",
      "Epoch 4500, Loss: 0.0003465812187641859\n",
      "Epoch 4999, Loss: 0.0003098643501289189\n",
      "Epoch 0, Loss: 0.44369828701019287\n",
      "Epoch 500, Loss: 0.008329249918460846\n",
      "Epoch 1000, Loss: 0.0036646032240241766\n",
      "Epoch 1500, Loss: 0.001877536647953093\n",
      "Threshold reach at: 1719\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0010207813465967774\n",
      "Epoch 2500, Loss: 0.0008723995415493846\n",
      "Epoch 3000, Loss: 0.0008529057959094644\n",
      "Epoch 3500, Loss: 0.0007166700670495629\n",
      "Epoch 4000, Loss: 0.0004936002660542727\n",
      "Epoch 4500, Loss: 0.0004950783913955092\n",
      "Epoch 4999, Loss: 0.0004265674215275794\n",
      "--- Experiment 8/20 ---\n",
      "Epoch 0, Loss: 0.7036706805229187\n",
      "Epoch 500, Loss: 0.009187597781419754\n",
      "Epoch 1000, Loss: 0.003602632088586688\n",
      "Epoch 1500, Loss: 0.003685158211737871\n",
      "Epoch 2000, Loss: 0.0033732198644429445\n",
      "Epoch 2500, Loss: 0.0025334698148071766\n",
      "Epoch 3000, Loss: 0.001925129909068346\n",
      "Epoch 3500, Loss: 0.001536400755867362\n",
      "Epoch 4000, Loss: 0.0010877106105908751\n",
      "Threshold reach at: 4267\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.0009493058314546943\n",
      "Epoch 4999, Loss: 0.001014120876789093\n",
      "Epoch 0, Loss: 0.31822654604911804\n",
      "Epoch 500, Loss: 0.016119925305247307\n",
      "Epoch 1000, Loss: 0.00894856546074152\n",
      "Epoch 1500, Loss: 0.004578212276101112\n",
      "Epoch 2000, Loss: 0.0037657630164176226\n",
      "Epoch 2500, Loss: 0.0030381963588297367\n",
      "Epoch 3000, Loss: 0.001699045766144991\n",
      "Threshold reach at: 3228\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0015787964221090078\n",
      "Epoch 4000, Loss: 0.0016063293442130089\n",
      "Epoch 4500, Loss: 0.002838596934452653\n",
      "Epoch 4999, Loss: 0.001049528131261468\n",
      "Epoch 0, Loss: 0.5721673965454102\n",
      "Epoch 500, Loss: 0.012887371703982353\n",
      "Epoch 1000, Loss: 0.004780780989676714\n",
      "Epoch 1500, Loss: 0.0028959696646779776\n",
      "Epoch 2000, Loss: 0.0019391661044210196\n",
      "Epoch 2500, Loss: 0.0011707904050126672\n",
      "Epoch 3000, Loss: 0.0011472790502011776\n",
      "Epoch 3500, Loss: 0.0013840522151440382\n",
      "Epoch 4000, Loss: 0.0007100901566445827\n",
      "Epoch 4500, Loss: 0.0006207355763763189\n",
      "Epoch 4999, Loss: 0.0005817183409817517\n",
      "Epoch 0, Loss: 0.3133990466594696\n",
      "Epoch 500, Loss: 0.00809408538043499\n",
      "Epoch 1000, Loss: 0.002115454524755478\n",
      "Epoch 1500, Loss: 0.0016215762589126825\n",
      "Threshold reach at: 1711\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0010111569426953793\n",
      "Epoch 2500, Loss: 0.0007958619389683008\n",
      "Epoch 3000, Loss: 0.0005604353500530124\n",
      "Epoch 3500, Loss: 0.0006662047235295177\n",
      "Epoch 4000, Loss: 0.0005816302727907896\n",
      "Epoch 4500, Loss: 0.00042478248360566795\n",
      "Epoch 4999, Loss: 0.00046080685569904745\n",
      "--- Experiment 9/20 ---\n",
      "Epoch 0, Loss: 0.43803340196609497\n",
      "Epoch 500, Loss: 0.012916110455989838\n",
      "Epoch 1000, Loss: 0.004494996275752783\n",
      "Epoch 1500, Loss: 0.003027294296771288\n",
      "Epoch 2000, Loss: 0.0021958816796541214\n",
      "Epoch 2500, Loss: 0.002214598236605525\n",
      "Epoch 3000, Loss: 0.0020007011480629444\n",
      "Epoch 3500, Loss: 0.001565400743857026\n",
      "Epoch 4000, Loss: 0.0013303712476044893\n",
      "Epoch 4500, Loss: 0.0011140047572553158\n",
      "Threshold reach at: 4627\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.0008970815106295049\n",
      "Epoch 0, Loss: 0.2776030898094177\n",
      "Epoch 500, Loss: 0.01357695646584034\n",
      "Epoch 1000, Loss: 0.007174599915742874\n",
      "Epoch 1500, Loss: 0.006086291745305061\n",
      "Epoch 2000, Loss: 0.005605973303318024\n",
      "Epoch 2500, Loss: 0.004947465844452381\n",
      "Epoch 3000, Loss: 0.0050573148764669895\n",
      "Epoch 3500, Loss: 0.0031856142450124025\n",
      "Epoch 4000, Loss: 0.0029014397878199816\n",
      "Epoch 4500, Loss: 0.0055650752037763596\n",
      "Epoch 4999, Loss: 0.0023871357552707195\n",
      "Epoch 0, Loss: 0.4350641667842865\n",
      "Epoch 500, Loss: 0.013889335095882416\n",
      "Epoch 1000, Loss: 0.0049205198884010315\n",
      "Epoch 1500, Loss: 0.0036598779261112213\n",
      "Epoch 2000, Loss: 0.002958146622404456\n",
      "Epoch 2500, Loss: 0.0022494615986943245\n",
      "Epoch 3000, Loss: 0.0019332320662215352\n",
      "Epoch 3500, Loss: 0.0016297573456540704\n",
      "Epoch 4000, Loss: 0.0013089700369164348\n",
      "Epoch 4500, Loss: 0.0011088275350630283\n",
      "Epoch 4999, Loss: 0.0009869573405012488\n",
      "Epoch 0, Loss: 0.5997459292411804\n",
      "Epoch 500, Loss: 0.010883948765695095\n",
      "Epoch 1000, Loss: 0.003918644972145557\n",
      "Epoch 1500, Loss: 0.002397773554548621\n",
      "Epoch 2000, Loss: 0.001436201622709632\n",
      "Threshold reach at: 2159\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0010150368325412273\n",
      "Epoch 3000, Loss: 0.0010038113687187433\n",
      "Epoch 3500, Loss: 0.0007853225688450038\n",
      "Epoch 4000, Loss: 0.0006402212893590331\n",
      "Epoch 4500, Loss: 0.0004211669438518584\n",
      "Epoch 4999, Loss: 0.0005790475988760591\n",
      "--- Experiment 10/20 ---\n",
      "Epoch 0, Loss: 0.41439288854599\n",
      "Epoch 500, Loss: 0.006998606026172638\n",
      "Epoch 1000, Loss: 0.0045784832909703255\n",
      "Epoch 1500, Loss: 0.002377818338572979\n",
      "Epoch 2000, Loss: 0.001657966640777886\n",
      "Epoch 2500, Loss: 0.0016807048814371228\n",
      "Epoch 3000, Loss: 0.0011084242723882198\n",
      "Threshold reach at: 3037\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0007518072961829603\n",
      "Epoch 4000, Loss: 0.0008096008095890284\n",
      "Epoch 4500, Loss: 0.000492726918309927\n",
      "Epoch 4999, Loss: 0.0005460877437144518\n",
      "Epoch 0, Loss: 0.42788752913475037\n",
      "Epoch 500, Loss: 0.02926229126751423\n",
      "Epoch 1000, Loss: 0.014134924858808517\n",
      "Epoch 1500, Loss: 0.008696770295500755\n",
      "Epoch 2000, Loss: 0.006101682316511869\n",
      "Epoch 2500, Loss: 0.004920411854982376\n",
      "Epoch 3000, Loss: 0.00520860543474555\n",
      "Epoch 3500, Loss: 0.005255752708762884\n",
      "Epoch 4000, Loss: 0.003947151824831963\n",
      "Epoch 4500, Loss: 0.0027280659414827824\n",
      "Epoch 4999, Loss: 0.0027189438696950674\n",
      "Epoch 0, Loss: 0.625028133392334\n",
      "Epoch 500, Loss: 0.0146395955234766\n",
      "Epoch 1000, Loss: 0.004504406824707985\n",
      "Epoch 1500, Loss: 0.002708745189011097\n",
      "Epoch 2000, Loss: 0.0020588908810168505\n",
      "Epoch 2500, Loss: 0.00143830431625247\n",
      "Epoch 3000, Loss: 0.0011518155224621296\n",
      "Epoch 3500, Loss: 0.0008982231374830008\n",
      "Epoch 4000, Loss: 0.0010608416050672531\n",
      "Epoch 4500, Loss: 0.0006049437215551734\n",
      "Epoch 4999, Loss: 0.0006849037017673254\n",
      "Epoch 0, Loss: 0.6154478192329407\n",
      "Epoch 500, Loss: 0.016471868380904198\n",
      "Epoch 1000, Loss: 0.007029389031231403\n",
      "Epoch 1500, Loss: 0.003460048697888851\n",
      "Epoch 2000, Loss: 0.002401175908744335\n",
      "Epoch 2500, Loss: 0.0014066208386793733\n",
      "Threshold reach at: 2513\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0009742426918819547\n",
      "Epoch 3500, Loss: 0.0009441248257644475\n",
      "Epoch 4000, Loss: 0.0005474350764416158\n",
      "Epoch 4500, Loss: 0.0007420543697662652\n",
      "Epoch 4999, Loss: 0.0005629340885207057\n",
      "--- Experiment 11/20 ---\n",
      "Epoch 0, Loss: 0.6008400917053223\n",
      "Epoch 500, Loss: 0.007243484258651733\n",
      "Epoch 1000, Loss: 0.00402343925088644\n",
      "Epoch 1500, Loss: 0.0026511044707149267\n",
      "Epoch 2000, Loss: 0.001823865226469934\n",
      "Epoch 2500, Loss: 0.0020462912507355213\n",
      "Epoch 3000, Loss: 0.0013801591703668237\n",
      "Threshold reach at: 3452\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.001491930102929473\n",
      "Epoch 4000, Loss: 0.0010640970431268215\n",
      "Epoch 4500, Loss: 0.0007766386261209846\n",
      "Epoch 4999, Loss: 0.0010980286169797182\n",
      "Epoch 0, Loss: 0.6409664154052734\n",
      "Epoch 500, Loss: 0.022320324555039406\n",
      "Epoch 1000, Loss: 0.011108700186014175\n",
      "Epoch 1500, Loss: 0.0080783162266016\n",
      "Epoch 2000, Loss: 0.006110356189310551\n",
      "Epoch 2500, Loss: 0.005523571744561195\n",
      "Epoch 3000, Loss: 0.0032616863027215004\n",
      "Epoch 3500, Loss: 0.0035570531617850065\n",
      "Epoch 4000, Loss: 0.006357356905937195\n",
      "Epoch 4500, Loss: 0.0023155631497502327\n",
      "Epoch 4999, Loss: 0.0021253065206110477\n",
      "Epoch 0, Loss: 0.4408770799636841\n",
      "Epoch 500, Loss: 0.008812268264591694\n",
      "Epoch 1000, Loss: 0.003959440626204014\n",
      "Epoch 1500, Loss: 0.002287072828039527\n",
      "Epoch 2000, Loss: 0.0014946003211662173\n",
      "Epoch 2500, Loss: 0.0011864855187013745\n",
      "Epoch 3000, Loss: 0.0008012630860321224\n",
      "Epoch 3500, Loss: 0.0008138448465615511\n",
      "Epoch 4000, Loss: 0.0007604010170325637\n",
      "Epoch 4500, Loss: 0.0006534986896440387\n",
      "Epoch 4999, Loss: 0.0006485023186542094\n",
      "Epoch 0, Loss: 0.6593056917190552\n",
      "Epoch 500, Loss: 0.011771254241466522\n",
      "Epoch 1000, Loss: 0.006415372248739004\n",
      "Epoch 1500, Loss: 0.0021135197021067142\n",
      "Threshold reach at: 1874\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0010471197310835123\n",
      "Epoch 2500, Loss: 0.0005672804545611143\n",
      "Epoch 3000, Loss: 0.0005915823858231306\n",
      "Epoch 3500, Loss: 0.000342711282428354\n",
      "Epoch 4000, Loss: 0.00030348802101798356\n",
      "Epoch 4500, Loss: 0.001760217361152172\n",
      "Epoch 4999, Loss: 0.0006897152052260935\n",
      "--- Experiment 12/20 ---\n",
      "Epoch 0, Loss: 0.48469772934913635\n",
      "Epoch 500, Loss: 0.010417100042104721\n",
      "Epoch 1000, Loss: 0.004058117512613535\n",
      "Epoch 1500, Loss: 0.003223973559215665\n",
      "Epoch 2000, Loss: 0.0019074303563684225\n",
      "Epoch 2500, Loss: 0.0017232811078429222\n",
      "Epoch 3000, Loss: 0.0016990962903946638\n",
      "Epoch 3500, Loss: 0.0010317160049453378\n",
      "Epoch 4000, Loss: 0.0012166177621111274\n",
      "Threshold reach at: 4070\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.0010656395461410284\n",
      "Epoch 4999, Loss: 0.0006755925132893026\n",
      "Epoch 0, Loss: 0.7279461622238159\n",
      "Epoch 500, Loss: 0.02268490195274353\n",
      "Epoch 1000, Loss: 0.014459064230322838\n",
      "Epoch 1500, Loss: 0.008353597484529018\n",
      "Epoch 2000, Loss: 0.00541897676885128\n",
      "Epoch 2500, Loss: 0.004467717371881008\n",
      "Epoch 3000, Loss: 0.0028954423032701015\n",
      "Epoch 3500, Loss: 0.0024744574911892414\n",
      "Epoch 4000, Loss: 0.004330478608608246\n",
      "Epoch 4500, Loss: 0.0029511149041354656\n",
      "Threshold reach at: 4571\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.0017538655083626509\n",
      "Epoch 0, Loss: 0.8289941549301147\n",
      "Epoch 500, Loss: 0.017064962536096573\n",
      "Epoch 1000, Loss: 0.006052942015230656\n",
      "Epoch 1500, Loss: 0.003917510621249676\n",
      "Epoch 2000, Loss: 0.0027478747069835663\n",
      "Epoch 2500, Loss: 0.0018699741922318935\n",
      "Epoch 3000, Loss: 0.0014278539456427097\n",
      "Epoch 3500, Loss: 0.001322413794696331\n",
      "Epoch 4000, Loss: 0.0011558339465409517\n",
      "Epoch 4500, Loss: 0.0010365222115069628\n",
      "Epoch 4999, Loss: 0.000912122312001884\n",
      "Epoch 0, Loss: 0.5466848611831665\n",
      "Epoch 500, Loss: 0.010118858888745308\n",
      "Epoch 1000, Loss: 0.0035801175981760025\n",
      "Epoch 1500, Loss: 0.0019205689895898104\n",
      "Threshold reach at: 1817\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0012811801861971617\n",
      "Epoch 2500, Loss: 0.0008961942512542009\n",
      "Epoch 3000, Loss: 0.0007372881518676877\n",
      "Epoch 3500, Loss: 0.0004697764234151691\n",
      "Epoch 4000, Loss: 0.00047382793854922056\n",
      "Epoch 4500, Loss: 0.0003959288587793708\n",
      "Epoch 4999, Loss: 0.00032353229471482337\n",
      "--- Experiment 13/20 ---\n",
      "Epoch 0, Loss: 0.6175630688667297\n",
      "Epoch 500, Loss: 0.008485188707709312\n",
      "Epoch 1000, Loss: 0.0035497546195983887\n",
      "Epoch 1500, Loss: 0.0014618760906159878\n",
      "Epoch 2000, Loss: 0.001481527229771018\n",
      "Threshold reach at: 2356\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0008055215585045516\n",
      "Epoch 3000, Loss: 0.0009709495934657753\n",
      "Epoch 3500, Loss: 0.0005297104944474995\n",
      "Epoch 4000, Loss: 0.0005756124155595899\n",
      "Epoch 4500, Loss: 0.000821084133349359\n",
      "Epoch 4999, Loss: 0.00036531774094328284\n",
      "Epoch 0, Loss: 0.38581833243370056\n",
      "Epoch 500, Loss: 0.017332222312688828\n",
      "Epoch 1000, Loss: 0.010824591852724552\n",
      "Epoch 1500, Loss: 0.008095053024590015\n",
      "Epoch 2000, Loss: 0.00562465051189065\n",
      "Epoch 2500, Loss: 0.006362364627420902\n",
      "Epoch 3000, Loss: 0.0038705337792634964\n",
      "Epoch 3500, Loss: 0.00322365784086287\n",
      "Epoch 4000, Loss: 0.0043852985836565495\n",
      "Epoch 4500, Loss: 0.004645404405891895\n",
      "Epoch 4999, Loss: 0.0033261142671108246\n",
      "Epoch 0, Loss: 0.2716728746891022\n",
      "Epoch 500, Loss: 0.01208481378853321\n",
      "Epoch 1000, Loss: 0.0049182274378836155\n",
      "Epoch 1500, Loss: 0.00346379610709846\n",
      "Epoch 2000, Loss: 0.002303136745467782\n",
      "Epoch 2500, Loss: 0.0015568824019283056\n",
      "Epoch 3000, Loss: 0.001237494288943708\n",
      "Epoch 3500, Loss: 0.0011509258765727282\n",
      "Epoch 4000, Loss: 0.0009308329317718744\n",
      "Epoch 4500, Loss: 0.0011543981963768601\n",
      "Epoch 4999, Loss: 0.0007571938913315535\n",
      "Epoch 0, Loss: 0.8636260628700256\n",
      "Epoch 500, Loss: 0.012223814614117146\n",
      "Epoch 1000, Loss: 0.0030330074951052666\n",
      "Epoch 1500, Loss: 0.0018716405611485243\n",
      "Threshold reach at: 1878\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0009835682576522231\n",
      "Epoch 2500, Loss: 0.0009434770327061415\n",
      "Epoch 3000, Loss: 0.0009073847904801369\n",
      "Epoch 3500, Loss: 0.000620704609900713\n",
      "Epoch 4000, Loss: 0.0004671616479754448\n",
      "Epoch 4500, Loss: 0.0006025710608810186\n",
      "Epoch 4999, Loss: 0.00048933265497908\n",
      "--- Experiment 14/20 ---\n",
      "Epoch 0, Loss: 0.32184916734695435\n",
      "Epoch 500, Loss: 0.006110275164246559\n",
      "Epoch 1000, Loss: 0.0029471185989677906\n",
      "Epoch 1500, Loss: 0.002473551779985428\n",
      "Epoch 2000, Loss: 0.001388590782880783\n",
      "Epoch 2500, Loss: 0.0011421132367104292\n",
      "Threshold reach at: 2758\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.001376829226501286\n",
      "Epoch 3500, Loss: 0.0007361930911429226\n",
      "Epoch 4000, Loss: 0.0006547740194946527\n",
      "Epoch 4500, Loss: 0.0011518863029778004\n",
      "Epoch 4999, Loss: 0.000569775584153831\n",
      "Epoch 0, Loss: 0.536806046962738\n",
      "Epoch 500, Loss: 0.02335641346871853\n",
      "Epoch 1000, Loss: 0.014303022995591164\n",
      "Epoch 1500, Loss: 0.00895754061639309\n",
      "Epoch 2000, Loss: 0.00676322914659977\n",
      "Epoch 2500, Loss: 0.00698638753965497\n",
      "Epoch 3000, Loss: 0.005458491388708353\n",
      "Epoch 3500, Loss: 0.004339056089520454\n",
      "Epoch 4000, Loss: 0.004726719576865435\n",
      "Epoch 4500, Loss: 0.003459253814071417\n",
      "Epoch 4999, Loss: 0.0025206345599144697\n",
      "Epoch 0, Loss: 0.5343137383460999\n",
      "Epoch 500, Loss: 0.011942720040678978\n",
      "Epoch 1000, Loss: 0.003486926667392254\n",
      "Epoch 1500, Loss: 0.0019244389841333032\n",
      "Epoch 2000, Loss: 0.0011389018036425114\n",
      "Epoch 2500, Loss: 0.0008872913895174861\n",
      "Epoch 3000, Loss: 0.0006767013110220432\n",
      "Epoch 3500, Loss: 0.0005011826870031655\n",
      "Epoch 4000, Loss: 0.00041000975761562586\n",
      "Epoch 4500, Loss: 0.00035362079506739974\n",
      "Epoch 4999, Loss: 0.000324615859426558\n",
      "Epoch 0, Loss: 0.7602986097335815\n",
      "Epoch 500, Loss: 0.010220246389508247\n",
      "Epoch 1000, Loss: 0.004505452699959278\n",
      "Epoch 1500, Loss: 0.0028009950183331966\n",
      "Epoch 2000, Loss: 0.001953521743416786\n",
      "Threshold reach at: 2448\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0012840874260291457\n",
      "Epoch 3000, Loss: 0.0007578812073916197\n",
      "Epoch 3500, Loss: 0.0006605108501389623\n",
      "Epoch 4000, Loss: 0.0005580683937296271\n",
      "Epoch 4500, Loss: 0.00042751760338433087\n",
      "Epoch 4999, Loss: 0.00035580992698669434\n",
      "--- Experiment 15/20 ---\n",
      "Epoch 0, Loss: 0.5245989561080933\n",
      "Epoch 500, Loss: 0.00998933520168066\n",
      "Epoch 1000, Loss: 0.0035157320089638233\n",
      "Epoch 1500, Loss: 0.0021457925904542208\n",
      "Epoch 2000, Loss: 0.0014165452448651195\n",
      "Threshold reach at: 2273\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.001163192791864276\n",
      "Epoch 3000, Loss: 0.0008353596786037087\n",
      "Epoch 3500, Loss: 0.0009372038184665143\n",
      "Epoch 4000, Loss: 0.0004596458456944674\n",
      "Epoch 4500, Loss: 0.000495752552524209\n",
      "Epoch 4999, Loss: 0.0007853733841329813\n",
      "Epoch 0, Loss: 0.39484235644340515\n",
      "Epoch 500, Loss: 0.016193319112062454\n",
      "Epoch 1000, Loss: 0.008926957845687866\n",
      "Epoch 1500, Loss: 0.0036887959577143192\n",
      "Epoch 2000, Loss: 0.004031241871416569\n",
      "Epoch 2500, Loss: 0.0033645317889750004\n",
      "Epoch 3000, Loss: 0.0016044853255152702\n",
      "Threshold reach at: 3337\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.005157371051609516\n",
      "Epoch 4000, Loss: 0.0019016775768250227\n",
      "Epoch 4500, Loss: 0.0009588485118001699\n",
      "Epoch 4999, Loss: 0.0010504454839974642\n",
      "Epoch 0, Loss: 0.6169748902320862\n",
      "Epoch 500, Loss: 0.014935491606593132\n",
      "Epoch 1000, Loss: 0.0038511669263243675\n",
      "Epoch 1500, Loss: 0.002262354362756014\n",
      "Epoch 2000, Loss: 0.0018162132473662496\n",
      "Epoch 2500, Loss: 0.0012390643823891878\n",
      "Epoch 3000, Loss: 0.0011308426037430763\n",
      "Epoch 3500, Loss: 0.0009299895609728992\n",
      "Epoch 4000, Loss: 0.0006427990738302469\n",
      "Epoch 4500, Loss: 0.0007186630391515791\n",
      "Epoch 4999, Loss: 0.0005420663510449231\n",
      "Epoch 0, Loss: 0.44264456629753113\n",
      "Epoch 500, Loss: 0.00572587363421917\n",
      "Epoch 1000, Loss: 0.0021427490282803774\n",
      "Epoch 1500, Loss: 0.000984302954748273\n",
      "Threshold reach at: 1607\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0011154336389154196\n",
      "Epoch 2500, Loss: 0.0009109488455578685\n",
      "Epoch 3000, Loss: 0.0006388576584868133\n",
      "Epoch 3500, Loss: 0.0006715050549246371\n",
      "Epoch 4000, Loss: 0.0005628665094263852\n",
      "Epoch 4500, Loss: 0.00043931041727773845\n",
      "Epoch 4999, Loss: 0.0003791487542912364\n",
      "--- Experiment 16/20 ---\n",
      "Epoch 0, Loss: 0.5076905488967896\n",
      "Epoch 500, Loss: 0.00874362699687481\n",
      "Epoch 1000, Loss: 0.004474235232919455\n",
      "Epoch 1500, Loss: 0.0026051467284560204\n",
      "Epoch 2000, Loss: 0.002331896685063839\n",
      "Epoch 2500, Loss: 0.0015907777706161141\n",
      "Epoch 3000, Loss: 0.0012053665705025196\n",
      "Threshold reach at: 3109\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0009395070374011993\n",
      "Epoch 4000, Loss: 0.0006959663005545735\n",
      "Epoch 4500, Loss: 0.0006890707882121205\n",
      "Epoch 4999, Loss: 0.0007383397896774113\n",
      "Epoch 0, Loss: 0.42453908920288086\n",
      "Epoch 500, Loss: 0.019699733704328537\n",
      "Epoch 1000, Loss: 0.010860566981136799\n",
      "Epoch 1500, Loss: 0.0062980991788208485\n",
      "Epoch 2000, Loss: 0.004200166091322899\n",
      "Epoch 2500, Loss: 0.0024187990929931402\n",
      "Threshold reach at: 2858\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0018903373274952173\n",
      "Epoch 3500, Loss: 0.003054696600884199\n",
      "Epoch 4000, Loss: 0.0010811961255967617\n",
      "Epoch 4500, Loss: 0.0014782445505261421\n",
      "Epoch 4999, Loss: 0.0012836542446166277\n",
      "Epoch 0, Loss: 0.5137689709663391\n",
      "Epoch 500, Loss: 0.012093651108443737\n",
      "Epoch 1000, Loss: 0.0040699574165046215\n",
      "Epoch 1500, Loss: 0.0028969929553568363\n",
      "Epoch 2000, Loss: 0.002600352745503187\n",
      "Epoch 2500, Loss: 0.0017977808602154255\n",
      "Epoch 3000, Loss: 0.0014305070508271456\n",
      "Epoch 3500, Loss: 0.0012418823316693306\n",
      "Epoch 4000, Loss: 0.0010609675664454699\n",
      "Epoch 4500, Loss: 0.0009474504622630775\n",
      "Epoch 4999, Loss: 0.0006839116103947163\n",
      "Epoch 0, Loss: 0.7524998188018799\n",
      "Epoch 500, Loss: 0.012348856776952744\n",
      "Epoch 1000, Loss: 0.006698322482407093\n",
      "Epoch 1500, Loss: 0.004097410943359137\n",
      "Epoch 2000, Loss: 0.0023837066255509853\n",
      "Threshold reach at: 2365\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0011784877860918641\n",
      "Epoch 3000, Loss: 0.00045176391722634435\n",
      "Epoch 3500, Loss: 0.0005448872107081115\n",
      "Epoch 4000, Loss: 0.0003860863798763603\n",
      "Epoch 4500, Loss: 0.00040556304156780243\n",
      "Epoch 4999, Loss: 0.00025067070964723825\n",
      "--- Experiment 17/20 ---\n",
      "Epoch 0, Loss: 0.4583745002746582\n",
      "Epoch 500, Loss: 0.009722139686346054\n",
      "Epoch 1000, Loss: 0.0035907565616071224\n",
      "Epoch 1500, Loss: 0.0033282216172665358\n",
      "Epoch 2000, Loss: 0.0009482376626692712\n",
      "Threshold reach at: 2065\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0008068615570664406\n",
      "Epoch 3000, Loss: 0.0006303917616605759\n",
      "Epoch 3500, Loss: 0.0003692307509481907\n",
      "Epoch 4000, Loss: 0.0002506378514226526\n",
      "Epoch 4500, Loss: 0.0005035569192841649\n",
      "Epoch 4999, Loss: 0.000448672566562891\n",
      "Epoch 0, Loss: 0.7377311587333679\n",
      "Epoch 500, Loss: 0.018430406227707863\n",
      "Epoch 1000, Loss: 0.010192357003688812\n",
      "Epoch 1500, Loss: 0.005326928570866585\n",
      "Epoch 2000, Loss: 0.00482333404943347\n",
      "Epoch 2500, Loss: 0.003781550098210573\n",
      "Epoch 3000, Loss: 0.0025952542200684547\n",
      "Epoch 3500, Loss: 0.0028619528748095036\n",
      "Epoch 4000, Loss: 0.002861295361071825\n",
      "Epoch 4500, Loss: 0.002333896467462182\n",
      "Epoch 4999, Loss: 0.0018646522657945752\n",
      "Epoch 0, Loss: 0.6011283993721008\n",
      "Epoch 500, Loss: 0.010247034020721912\n",
      "Epoch 1000, Loss: 0.003393966006115079\n",
      "Epoch 1500, Loss: 0.0022771013900637627\n",
      "Epoch 2000, Loss: 0.0016086173709481955\n",
      "Epoch 2500, Loss: 0.001467280788347125\n",
      "Epoch 3000, Loss: 0.00122596207074821\n",
      "Epoch 3500, Loss: 0.0010490790009498596\n",
      "Epoch 4000, Loss: 0.0007620069663971663\n",
      "Epoch 4500, Loss: 0.0008020936511456966\n",
      "Epoch 4999, Loss: 0.00073357077781111\n",
      "Epoch 0, Loss: 0.43900978565216064\n",
      "Epoch 500, Loss: 0.008847186341881752\n",
      "Epoch 1000, Loss: 0.002613405231386423\n",
      "Threshold reach at: 1388\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 1500, Loss: 0.001455157296732068\n",
      "Epoch 2000, Loss: 0.001020045019686222\n",
      "Epoch 2500, Loss: 0.0007799769518896937\n",
      "Epoch 3000, Loss: 0.0007755962433293462\n",
      "Epoch 3500, Loss: 0.0006153209251351655\n",
      "Epoch 4000, Loss: 0.0002225404023192823\n",
      "Epoch 4500, Loss: 0.00048034830251708627\n",
      "Epoch 4999, Loss: 0.000368926499504596\n",
      "--- Experiment 18/20 ---\n",
      "Epoch 0, Loss: 0.5330742597579956\n",
      "Epoch 500, Loss: 0.009125502780079842\n",
      "Epoch 1000, Loss: 0.004140873439610004\n",
      "Epoch 1500, Loss: 0.002787679899483919\n",
      "Epoch 2000, Loss: 0.0022666091099381447\n",
      "Epoch 2500, Loss: 0.0016187974251806736\n",
      "Epoch 3000, Loss: 0.0009337910450994968\n",
      "Threshold reach at: 3116\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0007660341216251254\n",
      "Epoch 4000, Loss: 0.0007372155087068677\n",
      "Epoch 4500, Loss: 0.0006044097826816142\n",
      "Epoch 4999, Loss: 0.0007821780163794756\n",
      "Epoch 0, Loss: 0.5574414134025574\n",
      "Epoch 500, Loss: 0.015215441584587097\n",
      "Epoch 1000, Loss: 0.00783731509000063\n",
      "Epoch 1500, Loss: 0.005003667902201414\n",
      "Epoch 2000, Loss: 0.0039666760712862015\n",
      "Epoch 2500, Loss: 0.00379740446805954\n",
      "Epoch 3000, Loss: 0.0027826668228954077\n",
      "Epoch 3500, Loss: 0.002375083975493908\n",
      "Epoch 4000, Loss: 0.002957714721560478\n",
      "Threshold reach at: 4472\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.002176280366256833\n",
      "Epoch 4999, Loss: 0.0017235276754945517\n",
      "Epoch 0, Loss: 0.30707094073295593\n",
      "Epoch 500, Loss: 0.00811460055410862\n",
      "Epoch 1000, Loss: 0.002298845909535885\n",
      "Epoch 1500, Loss: 0.0015273154713213444\n",
      "Epoch 2000, Loss: 0.0011103007709607482\n",
      "Epoch 2500, Loss: 0.000764346681535244\n",
      "Epoch 3000, Loss: 0.0006573837599717081\n",
      "Epoch 3500, Loss: 0.0005723618669435382\n",
      "Epoch 4000, Loss: 0.0005178367136977613\n",
      "Epoch 4500, Loss: 0.00048118611448444426\n",
      "Epoch 4999, Loss: 0.00035679616848938167\n",
      "Epoch 0, Loss: 0.4296545088291168\n",
      "Epoch 500, Loss: 0.010035006329417229\n",
      "Epoch 1000, Loss: 0.003626725170761347\n",
      "Epoch 1500, Loss: 0.002960043027997017\n",
      "Epoch 2000, Loss: 0.0021506547927856445\n",
      "Epoch 2500, Loss: 0.0014787856489419937\n",
      "Threshold reach at: 2507\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0007531761657446623\n",
      "Epoch 3500, Loss: 0.0006086016073822975\n",
      "Epoch 4000, Loss: 0.0007283957675099373\n",
      "Epoch 4500, Loss: 0.0006577993044629693\n",
      "Epoch 4999, Loss: 0.0004775257548317313\n",
      "--- Experiment 19/20 ---\n",
      "Epoch 0, Loss: 0.2954304814338684\n",
      "Epoch 500, Loss: 0.005701052490621805\n",
      "Epoch 1000, Loss: 0.0035543048288673162\n",
      "Epoch 1500, Loss: 0.0020045763812959194\n",
      "Epoch 2000, Loss: 0.0016115312464535236\n",
      "Threshold reach at: 2335\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0012675587786361575\n",
      "Epoch 3000, Loss: 0.0007679757545702159\n",
      "Epoch 3500, Loss: 0.0007262872532010078\n",
      "Epoch 4000, Loss: 0.0004215272201690823\n",
      "Epoch 4500, Loss: 0.000526831136085093\n",
      "Epoch 4999, Loss: 0.0005789151764474809\n",
      "Epoch 0, Loss: 0.5672519207000732\n",
      "Epoch 500, Loss: 0.02295326068997383\n",
      "Epoch 1000, Loss: 0.015347868204116821\n",
      "Epoch 1500, Loss: 0.010275593027472496\n",
      "Epoch 2000, Loss: 0.005677029490470886\n",
      "Epoch 2500, Loss: 0.004637787584215403\n",
      "Epoch 3000, Loss: 0.003072015242651105\n",
      "Epoch 3500, Loss: 0.002207887824624777\n",
      "Epoch 4000, Loss: 0.0016414341516792774\n",
      "Threshold reach at: 4367\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.008704694919288158\n",
      "Epoch 4999, Loss: 0.0014536952367052436\n",
      "Epoch 0, Loss: 0.4970768392086029\n",
      "Epoch 500, Loss: 0.013859866186976433\n",
      "Epoch 1000, Loss: 0.003924453631043434\n",
      "Epoch 1500, Loss: 0.0017865553963929415\n",
      "Epoch 2000, Loss: 0.0010016676969826221\n",
      "Epoch 2500, Loss: 0.0009690135484561324\n",
      "Epoch 3000, Loss: 0.0007612541085109115\n",
      "Epoch 3500, Loss: 0.0006080578896217048\n",
      "Epoch 4000, Loss: 0.0005364743410609663\n",
      "Epoch 4500, Loss: 0.0004683345905505121\n",
      "Epoch 4999, Loss: 0.0004457724280655384\n",
      "Epoch 0, Loss: 0.5845149755477905\n",
      "Epoch 500, Loss: 0.011661180295050144\n",
      "Epoch 1000, Loss: 0.0033676670864224434\n",
      "Epoch 1500, Loss: 0.0015611432027071714\n",
      "Threshold reach at: 1750\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0012405796442180872\n",
      "Epoch 2500, Loss: 0.0008252974366769195\n",
      "Epoch 3000, Loss: 0.0006580323679372668\n",
      "Epoch 3500, Loss: 0.0007164403796195984\n",
      "Epoch 4000, Loss: 0.0006027906783856452\n",
      "Epoch 4500, Loss: 0.00047763262409716845\n",
      "Epoch 4999, Loss: 0.0004025892703793943\n",
      "--- Experiment 20/20 ---\n",
      "Epoch 0, Loss: 0.27294638752937317\n",
      "Epoch 500, Loss: 0.005151507444679737\n",
      "Epoch 1000, Loss: 0.0032280352897942066\n",
      "Epoch 1500, Loss: 0.0022990053985267878\n",
      "Epoch 2000, Loss: 0.0016558864153921604\n",
      "Epoch 2500, Loss: 0.0011525871232151985\n",
      "Threshold reach at: 2731\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.001173796015791595\n",
      "Epoch 3500, Loss: 0.000569910800550133\n",
      "Epoch 4000, Loss: 0.001359152258373797\n",
      "Epoch 4500, Loss: 0.00047335110139101744\n",
      "Epoch 4999, Loss: 0.0005280136247165501\n",
      "Epoch 0, Loss: 0.32054099440574646\n",
      "Epoch 500, Loss: 0.01849553920328617\n",
      "Epoch 1000, Loss: 0.009945837780833244\n",
      "Epoch 1500, Loss: 0.008409183472394943\n",
      "Epoch 2000, Loss: 0.004015614744275808\n",
      "Epoch 2500, Loss: 0.00496409647166729\n",
      "Epoch 3000, Loss: 0.004122043028473854\n",
      "Epoch 3500, Loss: 0.0028037982992827892\n",
      "Epoch 4000, Loss: 0.002479227026924491\n",
      "Epoch 4500, Loss: 0.002827845513820648\n",
      "Epoch 4999, Loss: 0.0031830642838031054\n",
      "Epoch 0, Loss: 0.6052601337432861\n",
      "Epoch 500, Loss: 0.006759546231478453\n",
      "Epoch 1000, Loss: 0.002689544577151537\n",
      "Epoch 1500, Loss: 0.0015092587564140558\n",
      "Epoch 2000, Loss: 0.0011195095721632242\n",
      "Epoch 2500, Loss: 0.0007531681912951171\n",
      "Epoch 3000, Loss: 0.0007967951241880655\n",
      "Epoch 3500, Loss: 0.0007108068093657494\n",
      "Epoch 4000, Loss: 0.0005623921169899404\n",
      "Epoch 4500, Loss: 0.0004846793017350137\n",
      "Epoch 4999, Loss: 0.0005148444324731827\n",
      "Epoch 0, Loss: 0.4682295620441437\n",
      "Epoch 500, Loss: 0.009672743268311024\n",
      "Epoch 1000, Loss: 0.0035483227111399174\n",
      "Epoch 1500, Loss: 0.0024336944334208965\n",
      "Epoch 2000, Loss: 0.001809823326766491\n",
      "Threshold reach at: 2413\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0010052276775240898\n",
      "Epoch 3000, Loss: 0.0010820869356393814\n",
      "Epoch 3500, Loss: 0.0010406888322904706\n",
      "Epoch 4000, Loss: 0.0006467590574175119\n",
      "Epoch 4500, Loss: 0.000549952732399106\n",
      "Epoch 4999, Loss: 0.0006210776627995074\n",
      "Average Validation Loss (Base):   0.000650 ± 0.000215\n",
      "Average Validation Loss (Import): 0.001443 ± 0.000445\n",
      "Average Validation Loss (GP Out): 0.019498 ± 0.014795\n",
      "Average Validation Loss (GP Res): 0.000367 ± 0.000089\n",
      "\n",
      "==== Running experiments for IC: sin, BC: neumann ====\n",
      "--- Experiment 1/20 ---\n",
      "Epoch 0, Loss: 0.49163368344306946\n",
      "Epoch 500, Loss: 0.010813913308084011\n",
      "Epoch 1000, Loss: 0.004770186729729176\n",
      "Epoch 1500, Loss: 0.0026388196274638176\n",
      "Epoch 2000, Loss: 0.0020163538865745068\n",
      "Epoch 2500, Loss: 0.0016306694597005844\n",
      "Epoch 3000, Loss: 0.0015695149777457118\n",
      "Epoch 3500, Loss: 0.0012965636560693383\n",
      "Threshold reach at: 3864\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.0013398174196481705\n",
      "Epoch 4500, Loss: 0.0010257234098389745\n",
      "Epoch 4999, Loss: 0.0009001127909868956\n",
      "Epoch 0, Loss: 0.7326376438140869\n",
      "Epoch 500, Loss: 0.02101186290383339\n",
      "Epoch 1000, Loss: 0.011929629370570183\n",
      "Epoch 1500, Loss: 0.006256860680878162\n",
      "Epoch 2000, Loss: 0.0033588819205760956\n",
      "Epoch 2500, Loss: 0.0020248237997293472\n",
      "Epoch 3000, Loss: 0.001780129037797451\n",
      "Threshold reach at: 3233\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0018623711075633764\n",
      "Epoch 4000, Loss: 0.0016898401081562042\n",
      "Epoch 4500, Loss: 0.002129402942955494\n",
      "Epoch 4999, Loss: 0.0007451962446793914\n",
      "Epoch 0, Loss: 0.8973878026008606\n",
      "Epoch 500, Loss: 0.014544021338224411\n",
      "Epoch 1000, Loss: 0.004015276208519936\n",
      "Epoch 1500, Loss: 0.002078054938465357\n",
      "Epoch 2000, Loss: 0.0012087689246982336\n",
      "Epoch 2500, Loss: 0.0008032697369344532\n",
      "Epoch 3000, Loss: 0.0008297312306240201\n",
      "Epoch 3500, Loss: 0.0006762408884242177\n",
      "Epoch 4000, Loss: 0.0005640640156343579\n",
      "Epoch 4500, Loss: 0.000419307267293334\n",
      "Epoch 4999, Loss: 0.00047439741319976747\n",
      "Epoch 0, Loss: 0.3163755536079407\n",
      "Epoch 500, Loss: 0.009545971639454365\n",
      "Epoch 1000, Loss: 0.002557446714490652\n",
      "Epoch 1500, Loss: 0.0017767685931175947\n",
      "Threshold reach at: 1805\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.001107116462662816\n",
      "Epoch 2500, Loss: 0.0010015912121161819\n",
      "Epoch 3000, Loss: 0.0009102737531065941\n",
      "Epoch 3500, Loss: 0.0007835649885237217\n",
      "Epoch 4000, Loss: 0.0005716957384720445\n",
      "Epoch 4500, Loss: 0.0005149761564098299\n",
      "Epoch 4999, Loss: 0.00037684591370634735\n",
      "--- Experiment 2/20 ---\n",
      "Epoch 0, Loss: 0.5628025531768799\n",
      "Epoch 500, Loss: 0.008604345843195915\n",
      "Epoch 1000, Loss: 0.00320846913382411\n",
      "Epoch 1500, Loss: 0.002584532368928194\n",
      "Epoch 2000, Loss: 0.0018483990570530295\n",
      "Epoch 2500, Loss: 0.0016219483222812414\n",
      "Epoch 3000, Loss: 0.0010668332688510418\n",
      "Threshold reach at: 3141\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0007548837456852198\n",
      "Epoch 4000, Loss: 0.0007945082616060972\n",
      "Epoch 4500, Loss: 0.0008570380741730332\n",
      "Epoch 4999, Loss: 0.000788322533480823\n",
      "Epoch 0, Loss: 0.5081704258918762\n",
      "Epoch 500, Loss: 0.019336936995387077\n",
      "Epoch 1000, Loss: 0.01138297002762556\n",
      "Epoch 1500, Loss: 0.006816852372139692\n",
      "Epoch 2000, Loss: 0.004145827144384384\n",
      "Epoch 2500, Loss: 0.00319906254298985\n",
      "Epoch 3000, Loss: 0.00208873744122684\n",
      "Epoch 3500, Loss: 0.0027698553167283535\n",
      "Epoch 4000, Loss: 0.0022851594258099794\n",
      "Threshold reach at: 4281\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.00162147032096982\n",
      "Epoch 4999, Loss: 0.0015966559294611216\n",
      "Epoch 0, Loss: 0.5375750660896301\n",
      "Epoch 500, Loss: 0.011257655918598175\n",
      "Epoch 1000, Loss: 0.0038587618619203568\n",
      "Epoch 1500, Loss: 0.002330998657271266\n",
      "Epoch 2000, Loss: 0.0016941241919994354\n",
      "Epoch 2500, Loss: 0.0012426890898495913\n",
      "Epoch 3000, Loss: 0.0010802759788930416\n",
      "Epoch 3500, Loss: 0.000831295270472765\n",
      "Epoch 4000, Loss: 0.0009136738954111934\n",
      "Epoch 4500, Loss: 0.0006938494043424726\n",
      "Epoch 4999, Loss: 0.0006159400218166411\n",
      "Epoch 0, Loss: 0.44669899344444275\n",
      "Epoch 500, Loss: 0.008112098090350628\n",
      "Epoch 1000, Loss: 0.002818235196173191\n",
      "Threshold reach at: 1434\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 1500, Loss: 0.001077515073120594\n",
      "Epoch 2000, Loss: 0.0007407426601275802\n",
      "Epoch 2500, Loss: 0.00044837832683697343\n",
      "Epoch 3000, Loss: 0.0003406900505069643\n",
      "Epoch 3500, Loss: 0.00021072113304398954\n",
      "Epoch 4000, Loss: 0.00024848312023095787\n",
      "Epoch 4500, Loss: 0.00020253336697351187\n",
      "Epoch 4999, Loss: 7.056391768855974e-05\n",
      "--- Experiment 3/20 ---\n",
      "Epoch 0, Loss: 0.2976807951927185\n",
      "Epoch 500, Loss: 0.008963942527770996\n",
      "Epoch 1000, Loss: 0.004634789656847715\n",
      "Epoch 1500, Loss: 0.004441657569259405\n",
      "Epoch 2000, Loss: 0.002211506012827158\n",
      "Epoch 2500, Loss: 0.0021809875033795834\n",
      "Epoch 3000, Loss: 0.0016627190634608269\n",
      "Epoch 3500, Loss: 0.0013859225437045097\n",
      "Epoch 4000, Loss: 0.0013382085599005222\n",
      "Epoch 4500, Loss: 0.001013999106362462\n",
      "Threshold reach at: 4697\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.0012900748988613486\n",
      "Epoch 0, Loss: 0.2713838517665863\n",
      "Epoch 500, Loss: 0.012436380609869957\n",
      "Epoch 1000, Loss: 0.007342689670622349\n",
      "Epoch 1500, Loss: 0.008590808138251305\n",
      "Epoch 2000, Loss: 0.0036653312854468822\n",
      "Epoch 2500, Loss: 0.0026200073771178722\n",
      "Epoch 3000, Loss: 0.002190063241869211\n",
      "Epoch 3500, Loss: 0.001988727133721113\n",
      "Threshold reach at: 3995\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.001891118474304676\n",
      "Epoch 4500, Loss: 0.0017858792562037706\n",
      "Epoch 4999, Loss: 0.001745961606502533\n",
      "Epoch 0, Loss: 0.5016086101531982\n",
      "Epoch 500, Loss: 0.010076593607664108\n",
      "Epoch 1000, Loss: 0.002826444339007139\n",
      "Epoch 1500, Loss: 0.0018671343568712473\n",
      "Epoch 2000, Loss: 0.0013491049176082015\n",
      "Epoch 2500, Loss: 0.0009854256641119719\n",
      "Epoch 3000, Loss: 0.0008719633333384991\n",
      "Epoch 3500, Loss: 0.0007448631804436445\n",
      "Epoch 4000, Loss: 0.0005523558938875794\n",
      "Epoch 4500, Loss: 0.0005079121910966933\n",
      "Epoch 4999, Loss: 0.0005050798645243049\n",
      "Epoch 0, Loss: 0.6611620187759399\n",
      "Epoch 500, Loss: 0.010406899265944958\n",
      "Epoch 1000, Loss: 0.002962547354400158\n",
      "Epoch 1500, Loss: 0.0016892553539946675\n",
      "Threshold reach at: 1715\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.001038138521835208\n",
      "Epoch 2500, Loss: 0.0009392019128426909\n",
      "Epoch 3000, Loss: 0.000721054500900209\n",
      "Epoch 3500, Loss: 0.0004976973286829889\n",
      "Epoch 4000, Loss: 0.00018035550601780415\n",
      "Epoch 4500, Loss: 0.00012017283734166995\n",
      "Epoch 4999, Loss: 0.0002061827981378883\n",
      "--- Experiment 4/20 ---\n",
      "Epoch 0, Loss: 0.3582295775413513\n",
      "Epoch 500, Loss: 0.006840611808001995\n",
      "Epoch 1000, Loss: 0.0031340448185801506\n",
      "Epoch 1500, Loss: 0.0021929419599473476\n",
      "Epoch 2000, Loss: 0.001984095200896263\n",
      "Epoch 2500, Loss: 0.0015080503653734922\n",
      "Epoch 3000, Loss: 0.001242566155269742\n",
      "Threshold reach at: 3396\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0008642782922834158\n",
      "Epoch 4000, Loss: 0.0008230191888287663\n",
      "Epoch 4500, Loss: 0.00047114226617850363\n",
      "Epoch 4999, Loss: 0.0005229993257671595\n",
      "Epoch 0, Loss: 0.3303951919078827\n",
      "Epoch 500, Loss: 0.015623170882463455\n",
      "Epoch 1000, Loss: 0.008580649271607399\n",
      "Epoch 1500, Loss: 0.007631621323525906\n",
      "Epoch 2000, Loss: 0.005357678048312664\n",
      "Epoch 2500, Loss: 0.0036746342666447163\n",
      "Epoch 3000, Loss: 0.0027499126736074686\n",
      "Epoch 3500, Loss: 0.002339209895581007\n",
      "Epoch 4000, Loss: 0.0016207029111683369\n",
      "Epoch 4500, Loss: 0.0020997365936636925\n",
      "Epoch 4999, Loss: 0.0024882371071726084\n",
      "Epoch 0, Loss: 0.6440451145172119\n",
      "Epoch 500, Loss: 0.00884186290204525\n",
      "Epoch 1000, Loss: 0.0034295590594410896\n",
      "Epoch 1500, Loss: 0.0019920808263123035\n",
      "Epoch 2000, Loss: 0.0014925271971151233\n",
      "Epoch 2500, Loss: 0.0010090977884829044\n",
      "Epoch 3000, Loss: 0.000881553569342941\n",
      "Epoch 3500, Loss: 0.0007566089043393731\n",
      "Epoch 4000, Loss: 0.0007544885156676173\n",
      "Epoch 4500, Loss: 0.0006567780510522425\n",
      "Epoch 4999, Loss: 0.0005016820505261421\n",
      "Epoch 0, Loss: 0.45644745230674744\n",
      "Epoch 500, Loss: 0.010286349803209305\n",
      "Epoch 1000, Loss: 0.003912351094186306\n",
      "Epoch 1500, Loss: 0.0021198559552431107\n",
      "Epoch 2000, Loss: 0.001451093703508377\n",
      "Threshold reach at: 2401\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0010505953105166554\n",
      "Epoch 3000, Loss: 0.001008931314572692\n",
      "Epoch 3500, Loss: 0.0007860648911446333\n",
      "Epoch 4000, Loss: 0.0006830784259364009\n",
      "Epoch 4500, Loss: 0.0005615449044853449\n",
      "Epoch 4999, Loss: 0.00045583833707496524\n",
      "--- Experiment 5/20 ---\n",
      "Epoch 0, Loss: 0.7449504137039185\n",
      "Epoch 500, Loss: 0.009379595518112183\n",
      "Epoch 1000, Loss: 0.005342553835362196\n",
      "Epoch 1500, Loss: 0.003445754759013653\n",
      "Epoch 2000, Loss: 0.002884784247726202\n",
      "Epoch 2500, Loss: 0.0021658530458807945\n",
      "Epoch 3000, Loss: 0.0018371539190411568\n",
      "Epoch 3500, Loss: 0.0014664421323686838\n",
      "Epoch 4000, Loss: 0.0013494244776666164\n",
      "Threshold reach at: 4239\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.000867962371557951\n",
      "Epoch 4999, Loss: 0.0006691035814583302\n",
      "Epoch 0, Loss: 0.5268353223800659\n",
      "Epoch 500, Loss: 0.018773097544908524\n",
      "Epoch 1000, Loss: 0.013574366457760334\n",
      "Epoch 1500, Loss: 0.008986438624560833\n",
      "Epoch 2000, Loss: 0.00609883526340127\n",
      "Epoch 2500, Loss: 0.005206044297665358\n",
      "Epoch 3000, Loss: 0.00496907252818346\n",
      "Epoch 3500, Loss: 0.003988184966146946\n",
      "Epoch 4000, Loss: 0.007783184293657541\n",
      "Epoch 4500, Loss: 0.0033679786138236523\n",
      "Epoch 4999, Loss: 0.0035436805337667465\n",
      "Epoch 0, Loss: 0.7049918174743652\n",
      "Epoch 500, Loss: 0.0110963499173522\n",
      "Epoch 1000, Loss: 0.00427404697984457\n",
      "Epoch 1500, Loss: 0.0022557037882506847\n",
      "Epoch 2000, Loss: 0.001817675307393074\n",
      "Epoch 2500, Loss: 0.001635994529351592\n",
      "Epoch 3000, Loss: 0.0014704063069075346\n",
      "Epoch 3500, Loss: 0.00123787228949368\n",
      "Epoch 4000, Loss: 0.0009004924213513732\n",
      "Epoch 4500, Loss: 0.0009448234923183918\n",
      "Epoch 4999, Loss: 0.0008302522473968565\n",
      "Epoch 0, Loss: 0.7307538986206055\n",
      "Epoch 500, Loss: 0.011016422882676125\n",
      "Epoch 1000, Loss: 0.004685761872678995\n",
      "Epoch 1500, Loss: 0.0029839305207133293\n",
      "Epoch 2000, Loss: 0.0017588475020602345\n",
      "Threshold reach at: 2400\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.001176424091681838\n",
      "Epoch 3000, Loss: 0.000945555861108005\n",
      "Epoch 3500, Loss: 0.00051340606296435\n",
      "Epoch 4000, Loss: 0.0005016358918510377\n",
      "Epoch 4500, Loss: 0.0004848530516028404\n",
      "Epoch 4999, Loss: 0.0004735125694423914\n",
      "--- Experiment 6/20 ---\n",
      "Epoch 0, Loss: 0.41711482405662537\n",
      "Epoch 500, Loss: 0.0073499842546880245\n",
      "Epoch 1000, Loss: 0.0035802610218524933\n",
      "Epoch 1500, Loss: 0.002391776069998741\n",
      "Epoch 2000, Loss: 0.0015841765562072396\n",
      "Epoch 2500, Loss: 0.0011915750801563263\n",
      "Threshold reach at: 2577\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0007905204547569156\n",
      "Epoch 3500, Loss: 0.000907199690118432\n",
      "Epoch 4000, Loss: 0.0006372960633598268\n",
      "Epoch 4500, Loss: 0.000563804293051362\n",
      "Epoch 4999, Loss: 0.0006506798090413213\n",
      "Epoch 0, Loss: 0.6209548711776733\n",
      "Epoch 500, Loss: 0.020069193094968796\n",
      "Epoch 1000, Loss: 0.01617865078151226\n",
      "Epoch 1500, Loss: 0.008051076903939247\n",
      "Epoch 2000, Loss: 0.004979908466339111\n",
      "Epoch 2500, Loss: 0.005152569152414799\n",
      "Epoch 3000, Loss: 0.0038363952189683914\n",
      "Epoch 3500, Loss: 0.0029186662286520004\n",
      "Epoch 4000, Loss: 0.0032856883481144905\n",
      "Epoch 4500, Loss: 0.0024918171111494303\n",
      "Epoch 4999, Loss: 0.001943499082699418\n",
      "Epoch 0, Loss: 0.9302924871444702\n",
      "Epoch 500, Loss: 0.014773454517126083\n",
      "Epoch 1000, Loss: 0.00570518895983696\n",
      "Epoch 1500, Loss: 0.00263663986697793\n",
      "Epoch 2000, Loss: 0.002466765232384205\n",
      "Epoch 2500, Loss: 0.0019682832062244415\n",
      "Epoch 3000, Loss: 0.0014213195536285639\n",
      "Epoch 3500, Loss: 0.0012420000275596976\n",
      "Epoch 4000, Loss: 0.0010931941214948893\n",
      "Epoch 4500, Loss: 0.0009553753188811243\n",
      "Epoch 4999, Loss: 0.0005422576796263456\n",
      "Epoch 0, Loss: 0.5129963755607605\n",
      "Epoch 500, Loss: 0.010679656639695168\n",
      "Epoch 1000, Loss: 0.004036513157188892\n",
      "Epoch 1500, Loss: 0.002355915494263172\n",
      "Epoch 2000, Loss: 0.0013104709796607494\n",
      "Threshold reach at: 2403\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0011575971730053425\n",
      "Epoch 3000, Loss: 0.001015480374917388\n",
      "Epoch 3500, Loss: 0.0008307649986818433\n",
      "Epoch 4000, Loss: 0.0005832553724758327\n",
      "Epoch 4500, Loss: 0.0005751483840867877\n",
      "Epoch 4999, Loss: 0.0005574679234996438\n",
      "--- Experiment 7/20 ---\n",
      "Epoch 0, Loss: 0.3685787618160248\n",
      "Epoch 500, Loss: 0.008786924183368683\n",
      "Epoch 1000, Loss: 0.00626433826982975\n",
      "Epoch 1500, Loss: 0.0031552575528621674\n",
      "Epoch 2000, Loss: 0.00214660563506186\n",
      "Epoch 2500, Loss: 0.0015794382197782397\n",
      "Epoch 3000, Loss: 0.0012790132313966751\n",
      "Threshold reach at: 3164\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0010021219495683908\n",
      "Epoch 4000, Loss: 0.0005988850607536733\n",
      "Epoch 4500, Loss: 0.000346764805726707\n",
      "Epoch 4999, Loss: 0.00044365483336150646\n",
      "Epoch 0, Loss: 0.4548410177230835\n",
      "Epoch 500, Loss: 0.019011743366718292\n",
      "Epoch 1000, Loss: 0.01072655338793993\n",
      "Epoch 1500, Loss: 0.00643004197627306\n",
      "Epoch 2000, Loss: 0.0054118819534778595\n",
      "Epoch 2500, Loss: 0.005835860501974821\n",
      "Epoch 3000, Loss: 0.0031251360196620226\n",
      "Epoch 3500, Loss: 0.0038195615634322166\n",
      "Epoch 4000, Loss: 0.0030032589565962553\n",
      "Epoch 4500, Loss: 0.009807683527469635\n",
      "Epoch 4999, Loss: 0.001861494965851307\n",
      "Epoch 0, Loss: 0.24774065613746643\n",
      "Epoch 500, Loss: 0.009709905833005905\n",
      "Epoch 1000, Loss: 0.003875074675306678\n",
      "Epoch 1500, Loss: 0.0027235429733991623\n",
      "Epoch 2000, Loss: 0.0018600960029289126\n",
      "Epoch 2500, Loss: 0.0013497181935235858\n",
      "Epoch 3000, Loss: 0.0011629461077973247\n",
      "Epoch 3500, Loss: 0.0008604699396528304\n",
      "Epoch 4000, Loss: 0.0008514753426425159\n",
      "Epoch 4500, Loss: 0.0007123963441699743\n",
      "Epoch 4999, Loss: 0.0005209680530242622\n",
      "Epoch 0, Loss: 0.6500455737113953\n",
      "Epoch 500, Loss: 0.009936557151377201\n",
      "Epoch 1000, Loss: 0.003862727666273713\n",
      "Epoch 1500, Loss: 0.002669328823685646\n",
      "Epoch 2000, Loss: 0.0017811513971537352\n",
      "Threshold reach at: 2342\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0011334293521940708\n",
      "Epoch 3000, Loss: 0.0005360745708458126\n",
      "Epoch 3500, Loss: 0.0009521509055048227\n",
      "Epoch 4000, Loss: 0.0007762253517284989\n",
      "Epoch 4500, Loss: 0.0006245396798476577\n",
      "Epoch 4999, Loss: 0.0005503081483766437\n",
      "--- Experiment 8/20 ---\n",
      "Epoch 0, Loss: 0.5746760964393616\n",
      "Epoch 500, Loss: 0.007200012449175119\n",
      "Epoch 1000, Loss: 0.0036566234193742275\n",
      "Epoch 1500, Loss: 0.0026660540606826544\n",
      "Epoch 2000, Loss: 0.002251911908388138\n",
      "Epoch 2500, Loss: 0.0019208568846806884\n",
      "Epoch 3000, Loss: 0.0011198280844837427\n",
      "Epoch 3500, Loss: 0.0013706344179809093\n",
      "Threshold reach at: 3591\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.0006141288904473186\n",
      "Epoch 4500, Loss: 0.0006826297612860799\n",
      "Epoch 4999, Loss: 0.0007994057959876955\n",
      "Epoch 0, Loss: 0.656158983707428\n",
      "Epoch 500, Loss: 0.021672967821359634\n",
      "Epoch 1000, Loss: 0.01613728702068329\n",
      "Epoch 1500, Loss: 0.011961346492171288\n",
      "Epoch 2000, Loss: 0.010790856555104256\n",
      "Epoch 2500, Loss: 0.006112912204116583\n",
      "Epoch 3000, Loss: 0.005079380236566067\n",
      "Epoch 3500, Loss: 0.003705920884385705\n",
      "Epoch 4000, Loss: 0.003021166194230318\n",
      "Epoch 4500, Loss: 0.0029339888133108616\n",
      "Epoch 4999, Loss: 0.003511151298880577\n",
      "Epoch 0, Loss: 0.3674685060977936\n",
      "Epoch 500, Loss: 0.010027018375694752\n",
      "Epoch 1000, Loss: 0.00277047511190176\n",
      "Epoch 1500, Loss: 0.0017181446310132742\n",
      "Epoch 2000, Loss: 0.0012595831649377942\n",
      "Epoch 2500, Loss: 0.0010014560539275408\n",
      "Epoch 3000, Loss: 0.000886922178324312\n",
      "Epoch 3500, Loss: 0.0008822591044008732\n",
      "Epoch 4000, Loss: 0.0007497583283111453\n",
      "Epoch 4500, Loss: 0.0006339164101518691\n",
      "Epoch 4999, Loss: 0.0006117034936323762\n",
      "Epoch 0, Loss: 0.6077330112457275\n",
      "Epoch 500, Loss: 0.012906179763376713\n",
      "Epoch 1000, Loss: 0.00414311233907938\n",
      "Epoch 1500, Loss: 0.003229429479688406\n",
      "Epoch 2000, Loss: 0.002608821727335453\n",
      "Epoch 2500, Loss: 0.002126641571521759\n",
      "Epoch 3000, Loss: 0.0018909068312495947\n",
      "Epoch 3500, Loss: 0.001579532166942954\n",
      "Threshold reach at: 3911\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.0011010885937139392\n",
      "Epoch 4500, Loss: 0.0011410802835598588\n",
      "Epoch 4999, Loss: 0.0008843338582664728\n",
      "--- Experiment 9/20 ---\n",
      "Epoch 0, Loss: 0.5661846399307251\n",
      "Epoch 500, Loss: 0.008920754306018353\n",
      "Epoch 1000, Loss: 0.00492078298702836\n",
      "Epoch 1500, Loss: 0.0037325723096728325\n",
      "Epoch 2000, Loss: 0.00304966839030385\n",
      "Epoch 2500, Loss: 0.002474171807989478\n",
      "Epoch 3000, Loss: 0.002287367358803749\n",
      "Epoch 3500, Loss: 0.0018386795418336987\n",
      "Epoch 4000, Loss: 0.0012379724066704512\n",
      "Threshold reach at: 4330\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.0011225028429180384\n",
      "Epoch 4999, Loss: 0.0010466721141710877\n",
      "Epoch 0, Loss: 0.31725654006004333\n",
      "Epoch 500, Loss: 0.017334479838609695\n",
      "Epoch 1000, Loss: 0.009909942746162415\n",
      "Epoch 1500, Loss: 0.007249440066516399\n",
      "Epoch 2000, Loss: 0.005144872702658176\n",
      "Epoch 2500, Loss: 0.004996206145733595\n",
      "Epoch 3000, Loss: 0.003931385464966297\n",
      "Epoch 3500, Loss: 0.0029665427282452583\n",
      "Epoch 4000, Loss: 0.0025394074618816376\n",
      "Epoch 4500, Loss: 0.0022320214193314314\n",
      "Threshold reach at: 4710\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.0019142716191709042\n",
      "Epoch 0, Loss: 0.4265446066856384\n",
      "Epoch 500, Loss: 0.005854380317032337\n",
      "Epoch 1000, Loss: 0.0030925143510103226\n",
      "Epoch 1500, Loss: 0.0019436553120613098\n",
      "Epoch 2000, Loss: 0.002396239433437586\n",
      "Epoch 2500, Loss: 0.0011596929980441928\n",
      "Epoch 3000, Loss: 0.0009447101037949324\n",
      "Epoch 3500, Loss: 0.0005697847809642553\n",
      "Epoch 4000, Loss: 0.0006527117802761495\n",
      "Epoch 4500, Loss: 0.0005748810945078731\n",
      "Epoch 4999, Loss: 0.000530663353856653\n",
      "Epoch 0, Loss: 0.5020848512649536\n",
      "Epoch 500, Loss: 0.013497846201062202\n",
      "Epoch 1000, Loss: 0.0030482907313853502\n",
      "Epoch 1500, Loss: 0.0017789488192647696\n",
      "Threshold reach at: 1890\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0010735642863437533\n",
      "Epoch 2500, Loss: 0.0008156861877068877\n",
      "Epoch 3000, Loss: 0.0007332540117204189\n",
      "Epoch 3500, Loss: 0.0007968851132318377\n",
      "Epoch 4000, Loss: 0.0005210127565078437\n",
      "Epoch 4500, Loss: 0.0005671554245054722\n",
      "Epoch 4999, Loss: 0.00047717796405777335\n",
      "--- Experiment 10/20 ---\n",
      "Epoch 0, Loss: 0.4058983623981476\n",
      "Epoch 500, Loss: 0.006722759455442429\n",
      "Epoch 1000, Loss: 0.003719066735357046\n",
      "Epoch 1500, Loss: 0.0019853641279041767\n",
      "Epoch 2000, Loss: 0.001167301437817514\n",
      "Threshold reach at: 2407\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0009587639942765236\n",
      "Epoch 3000, Loss: 0.0005686847725883126\n",
      "Epoch 3500, Loss: 0.0004824927309527993\n",
      "Epoch 4000, Loss: 0.0005451786564663053\n",
      "Epoch 4500, Loss: 0.0004939150530844927\n",
      "Epoch 4999, Loss: 0.0005830181762576103\n",
      "Epoch 0, Loss: 0.5622074604034424\n",
      "Epoch 500, Loss: 0.018801311030983925\n",
      "Epoch 1000, Loss: 0.00983402319252491\n",
      "Epoch 1500, Loss: 0.0062589324079453945\n",
      "Epoch 2000, Loss: 0.00438128923997283\n",
      "Epoch 2500, Loss: 0.003572528250515461\n",
      "Epoch 3000, Loss: 0.003224749118089676\n",
      "Epoch 3500, Loss: 0.00195973995141685\n",
      "Epoch 4000, Loss: 0.0031815434340387583\n",
      "Threshold reach at: 4401\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.0013564276741817594\n",
      "Epoch 4999, Loss: 0.0019731330685317516\n",
      "Epoch 0, Loss: 0.7735119462013245\n",
      "Epoch 500, Loss: 0.011707675643265247\n",
      "Epoch 1000, Loss: 0.003793725511059165\n",
      "Epoch 1500, Loss: 0.0019456105073913932\n",
      "Epoch 2000, Loss: 0.001481531304307282\n",
      "Epoch 2500, Loss: 0.0010896011954173446\n",
      "Epoch 3000, Loss: 0.0010282429866492748\n",
      "Epoch 3500, Loss: 0.0009381248964928091\n",
      "Epoch 4000, Loss: 0.0007741297595202923\n",
      "Epoch 4500, Loss: 0.0006174417212605476\n",
      "Epoch 4999, Loss: 0.0004983044345863163\n",
      "Epoch 0, Loss: 0.651357889175415\n",
      "Epoch 500, Loss: 0.012752793729305267\n",
      "Epoch 1000, Loss: 0.004140655975788832\n",
      "Epoch 1500, Loss: 0.002957104006782174\n",
      "Epoch 2000, Loss: 0.0022306572645902634\n",
      "Epoch 2500, Loss: 0.0015512874815613031\n",
      "Threshold reach at: 2504\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0010188187006860971\n",
      "Epoch 3500, Loss: 0.0008961611893028021\n",
      "Epoch 4000, Loss: 0.0006954127456992865\n",
      "Epoch 4500, Loss: 0.0005857725627720356\n",
      "Epoch 4999, Loss: 0.0005707053351216018\n",
      "--- Experiment 11/20 ---\n",
      "Epoch 0, Loss: 0.861850380897522\n",
      "Epoch 500, Loss: 0.009873843751847744\n",
      "Epoch 1000, Loss: 0.005543069913983345\n",
      "Epoch 1500, Loss: 0.0035068809520453215\n",
      "Epoch 2000, Loss: 0.00171648059040308\n",
      "Epoch 2500, Loss: 0.0013631449546664953\n",
      "Threshold reach at: 2883\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0008583845337852836\n",
      "Epoch 3500, Loss: 0.0006917671416886151\n",
      "Epoch 4000, Loss: 0.0005839260993525386\n",
      "Epoch 4500, Loss: 0.000616842822637409\n",
      "Epoch 4999, Loss: 0.0004862589994445443\n",
      "Epoch 0, Loss: 0.5217949151992798\n",
      "Epoch 500, Loss: 0.015134146437048912\n",
      "Epoch 1000, Loss: 0.007966317236423492\n",
      "Epoch 1500, Loss: 0.006448950618505478\n",
      "Epoch 2000, Loss: 0.005240641068667173\n",
      "Epoch 2500, Loss: 0.003151240758597851\n",
      "Epoch 3000, Loss: 0.0030947255436331034\n",
      "Epoch 3500, Loss: 0.002470944542437792\n",
      "Epoch 4000, Loss: 0.0021904942113906145\n",
      "Epoch 4500, Loss: 0.002087476197630167\n",
      "Epoch 4999, Loss: 0.0038358159363269806\n",
      "Epoch 0, Loss: 0.462248295545578\n",
      "Epoch 500, Loss: 0.009394405409693718\n",
      "Epoch 1000, Loss: 0.0036613265983760357\n",
      "Epoch 1500, Loss: 0.002598394174128771\n",
      "Epoch 2000, Loss: 0.002194142434746027\n",
      "Epoch 2500, Loss: 0.001880205818451941\n",
      "Epoch 3000, Loss: 0.0018473180243745446\n",
      "Epoch 3500, Loss: 0.001514864037744701\n",
      "Epoch 4000, Loss: 0.001159856328740716\n",
      "Epoch 4500, Loss: 0.0010776034323498607\n",
      "Epoch 4999, Loss: 0.0008575969259254634\n",
      "Epoch 0, Loss: 0.29426151514053345\n",
      "Epoch 500, Loss: 0.007523426786065102\n",
      "Epoch 1000, Loss: 0.003858538344502449\n",
      "Epoch 1500, Loss: 0.0016494726296514273\n",
      "Epoch 2000, Loss: 0.0012780444230884314\n",
      "Threshold reach at: 2203\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0010878359898924828\n",
      "Epoch 3000, Loss: 0.0010836736764758825\n",
      "Epoch 3500, Loss: 0.001003367593511939\n",
      "Epoch 4000, Loss: 0.000757105415686965\n",
      "Epoch 4500, Loss: 0.0007903814548626542\n",
      "Epoch 4999, Loss: 0.0006956772413104773\n",
      "--- Experiment 12/20 ---\n",
      "Epoch 0, Loss: 0.6617090702056885\n",
      "Epoch 500, Loss: 0.011115845292806625\n",
      "Epoch 1000, Loss: 0.003928272984921932\n",
      "Epoch 1500, Loss: 0.0036314725875854492\n",
      "Epoch 2000, Loss: 0.001628541387617588\n",
      "Epoch 2500, Loss: 0.0013380803866311908\n",
      "Epoch 3000, Loss: 0.0013446103548631072\n",
      "Threshold reach at: 3132\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0011071114568039775\n",
      "Epoch 4000, Loss: 0.0007678365218453109\n",
      "Epoch 4500, Loss: 0.001065530115738511\n",
      "Epoch 4999, Loss: 0.0007397813606075943\n",
      "Epoch 0, Loss: 0.5404735803604126\n",
      "Epoch 500, Loss: 0.01842440478503704\n",
      "Epoch 1000, Loss: 0.00787508673965931\n",
      "Epoch 1500, Loss: 0.005167433060705662\n",
      "Epoch 2000, Loss: 0.004760405048727989\n",
      "Epoch 2500, Loss: 0.003995478618890047\n",
      "Epoch 3000, Loss: 0.0030098408460617065\n",
      "Epoch 3500, Loss: 0.0028935279697179794\n",
      "Epoch 4000, Loss: 0.003228082787245512\n",
      "Epoch 4500, Loss: 0.002886869478970766\n",
      "Epoch 4999, Loss: 0.002091065514832735\n",
      "Epoch 0, Loss: 0.551684558391571\n",
      "Epoch 500, Loss: 0.014800619333982468\n",
      "Epoch 1000, Loss: 0.006255039945244789\n",
      "Epoch 1500, Loss: 0.0037866695784032345\n",
      "Epoch 2000, Loss: 0.001947001088410616\n",
      "Epoch 2500, Loss: 0.001485923770815134\n",
      "Epoch 3000, Loss: 0.000984054058790207\n",
      "Epoch 3500, Loss: 0.000882505439221859\n",
      "Epoch 4000, Loss: 0.0007208660244941711\n",
      "Epoch 4500, Loss: 0.0007102895178832114\n",
      "Epoch 4999, Loss: 0.0004719682619906962\n",
      "Epoch 0, Loss: 0.5830543041229248\n",
      "Epoch 500, Loss: 0.010845168493688107\n",
      "Epoch 1000, Loss: 0.004912577569484711\n",
      "Epoch 1500, Loss: 0.0029197281692177057\n",
      "Epoch 2000, Loss: 0.0017987806349992752\n",
      "Threshold reach at: 2287\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0010676723904907703\n",
      "Epoch 3000, Loss: 0.0007674972293898463\n",
      "Epoch 3500, Loss: 0.0007046316750347614\n",
      "Epoch 4000, Loss: 0.0006780672702006996\n",
      "Epoch 4500, Loss: 0.0006418934790417552\n",
      "Epoch 4999, Loss: 0.0005148295895196497\n",
      "--- Experiment 13/20 ---\n",
      "Epoch 0, Loss: 0.3771759271621704\n",
      "Epoch 500, Loss: 0.00918438471853733\n",
      "Epoch 1000, Loss: 0.0035736269783228636\n",
      "Epoch 1500, Loss: 0.0023043924011290073\n",
      "Epoch 2000, Loss: 0.0023114997893571854\n",
      "Epoch 2500, Loss: 0.0017815511673688889\n",
      "Epoch 3000, Loss: 0.0017399991629645228\n",
      "Epoch 3500, Loss: 0.0010896502062678337\n",
      "Threshold reach at: 3621\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.0011289081303402781\n",
      "Epoch 4500, Loss: 0.0009777508676052094\n",
      "Epoch 4999, Loss: 0.0009659690549597144\n",
      "Epoch 0, Loss: 0.4026722013950348\n",
      "Epoch 500, Loss: 0.021980002522468567\n",
      "Epoch 1000, Loss: 0.009267397224903107\n",
      "Epoch 1500, Loss: 0.006060291081666946\n",
      "Epoch 2000, Loss: 0.005507241934537888\n",
      "Epoch 2500, Loss: 0.0029801498167216778\n",
      "Epoch 3000, Loss: 0.0032516547944396734\n",
      "Epoch 3500, Loss: 0.002871898002922535\n",
      "Threshold reach at: 3683\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.0015881299041211605\n",
      "Epoch 4500, Loss: 0.002534886822104454\n",
      "Epoch 4999, Loss: 0.0015752591425552964\n",
      "Epoch 0, Loss: 0.5580095052719116\n",
      "Epoch 500, Loss: 0.01260565035045147\n",
      "Epoch 1000, Loss: 0.0038385954685509205\n",
      "Epoch 1500, Loss: 0.0020643717143684626\n",
      "Epoch 2000, Loss: 0.001528317341580987\n",
      "Epoch 2500, Loss: 0.0011655762791633606\n",
      "Epoch 3000, Loss: 0.000833955011330545\n",
      "Epoch 3500, Loss: 0.0008835028856992722\n",
      "Epoch 4000, Loss: 0.0006413516239263117\n",
      "Epoch 4500, Loss: 0.0006905475165694952\n",
      "Epoch 4999, Loss: 0.0007594908820465207\n",
      "Epoch 0, Loss: 0.28529343008995056\n",
      "Epoch 500, Loss: 0.00978836789727211\n",
      "Epoch 1000, Loss: 0.00321872066706419\n",
      "Epoch 1500, Loss: 0.0021665736567229033\n",
      "Epoch 2000, Loss: 0.0017919255187734962\n",
      "Threshold reach at: 2386\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0013822322944179177\n",
      "Epoch 3000, Loss: 0.0012638544430956244\n",
      "Epoch 3500, Loss: 0.000977060291916132\n",
      "Epoch 4000, Loss: 0.0006249543512240052\n",
      "Epoch 4500, Loss: 0.000656561111100018\n",
      "Epoch 4999, Loss: 0.0005218539154157043\n",
      "--- Experiment 14/20 ---\n",
      "Epoch 0, Loss: 0.5328444242477417\n",
      "Epoch 500, Loss: 0.007681026589125395\n",
      "Epoch 1000, Loss: 0.0028316020034253597\n",
      "Epoch 1500, Loss: 0.002276619430631399\n",
      "Epoch 2000, Loss: 0.0010959776118397713\n",
      "Threshold reach at: 2085\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0010213768109679222\n",
      "Epoch 3000, Loss: 0.0007828131783753633\n",
      "Epoch 3500, Loss: 0.0004732334055006504\n",
      "Epoch 4000, Loss: 0.0006991785485297441\n",
      "Epoch 4500, Loss: 0.0009897544514387846\n",
      "Epoch 4999, Loss: 0.00038218346890062094\n",
      "Epoch 0, Loss: 0.5477323532104492\n",
      "Epoch 500, Loss: 0.017845572903752327\n",
      "Epoch 1000, Loss: 0.007897024042904377\n",
      "Epoch 1500, Loss: 0.004879914224147797\n",
      "Epoch 2000, Loss: 0.004972977563738823\n",
      "Epoch 2500, Loss: 0.003455854719504714\n",
      "Epoch 3000, Loss: 0.0024946690537035465\n",
      "Epoch 3500, Loss: 0.0022716366220265627\n",
      "Epoch 4000, Loss: 0.0016420395113527775\n",
      "Threshold reach at: 4190\n",
      "Val loss: tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.0023759366013109684\n",
      "Epoch 4999, Loss: 0.0012956028804183006\n",
      "Epoch 0, Loss: 0.3413712978363037\n",
      "Epoch 500, Loss: 0.010175484232604504\n",
      "Epoch 1000, Loss: 0.005248366389423609\n",
      "Epoch 1500, Loss: 0.0030944151803851128\n",
      "Epoch 2000, Loss: 0.0027062741573899984\n",
      "Epoch 2500, Loss: 0.002077685669064522\n",
      "Epoch 3000, Loss: 0.001968089025467634\n",
      "Epoch 3500, Loss: 0.0016804490005597472\n",
      "Epoch 4000, Loss: 0.0012425838503986597\n",
      "Epoch 4500, Loss: 0.0010702441213652492\n",
      "Epoch 4999, Loss: 0.0008965797023847699\n",
      "Epoch 0, Loss: 0.2864280343055725\n",
      "Epoch 500, Loss: 0.0065701911225914955\n",
      "Epoch 1000, Loss: 0.0016162917017936707\n",
      "Threshold reach at: 1208\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 1500, Loss: 0.0007816449506208301\n",
      "Epoch 2000, Loss: 0.0005457388469949365\n",
      "Epoch 2500, Loss: 0.000380846846383065\n",
      "Epoch 3000, Loss: 0.0002726586244534701\n",
      "Epoch 3500, Loss: 0.00026652682572603226\n",
      "Epoch 4000, Loss: 0.0002481529663782567\n",
      "Epoch 4500, Loss: 0.00013677631795872003\n",
      "Epoch 4999, Loss: 0.00011811697913799435\n",
      "--- Experiment 15/20 ---\n",
      "Epoch 0, Loss: 0.643340528011322\n",
      "Epoch 500, Loss: 0.007983386516571045\n",
      "Epoch 1000, Loss: 0.004155176691710949\n",
      "Epoch 1500, Loss: 0.0019494660664349794\n",
      "Epoch 2000, Loss: 0.0019421327160671353\n",
      "Epoch 2500, Loss: 0.0009511306998319924\n",
      "Threshold reach at: 2560\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.001226949505507946\n",
      "Epoch 3500, Loss: 0.0006923938053660095\n",
      "Epoch 4000, Loss: 0.0009681779774837196\n",
      "Epoch 4500, Loss: 0.0010732031660154462\n",
      "Epoch 4999, Loss: 0.000494907726533711\n",
      "Epoch 0, Loss: 0.24467965960502625\n",
      "Epoch 500, Loss: 0.016658298671245575\n",
      "Epoch 1000, Loss: 0.010826123878359795\n",
      "Epoch 1500, Loss: 0.0077168261632323265\n",
      "Epoch 2000, Loss: 0.006509862840175629\n",
      "Epoch 2500, Loss: 0.0034945672377943993\n",
      "Epoch 3000, Loss: 0.004118524491786957\n",
      "Epoch 3500, Loss: 0.00399559922516346\n",
      "Epoch 4000, Loss: 0.002020037965849042\n",
      "Epoch 4500, Loss: 0.0023654948454350233\n",
      "Threshold reach at: 4687\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.003645456861704588\n",
      "Epoch 0, Loss: 0.5756840705871582\n",
      "Epoch 500, Loss: 0.011575899086892605\n",
      "Epoch 1000, Loss: 0.004429999273270369\n",
      "Epoch 1500, Loss: 0.002501291688531637\n",
      "Epoch 2000, Loss: 0.002132824622094631\n",
      "Epoch 2500, Loss: 0.0016330934595316648\n",
      "Epoch 3000, Loss: 0.0014313440769910812\n",
      "Epoch 3500, Loss: 0.0011989128543063998\n",
      "Epoch 4000, Loss: 0.0010253549553453922\n",
      "Epoch 4500, Loss: 0.0007235294906422496\n",
      "Epoch 4999, Loss: 0.00078209163621068\n",
      "Epoch 0, Loss: 0.4079608917236328\n",
      "Epoch 500, Loss: 0.009646977297961712\n",
      "Epoch 1000, Loss: 0.004658750724047422\n",
      "Epoch 1500, Loss: 0.0029953401535749435\n",
      "Epoch 2000, Loss: 0.0023772362619638443\n",
      "Epoch 2500, Loss: 0.0018686732510104775\n",
      "Epoch 3000, Loss: 0.0016709542833268642\n",
      "Threshold reach at: 3009\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.001221479382365942\n",
      "Epoch 4000, Loss: 0.0005955884698778391\n",
      "Epoch 4500, Loss: 0.0008691339171491563\n",
      "Epoch 4999, Loss: 0.0008723362116143107\n",
      "--- Experiment 16/20 ---\n",
      "Epoch 0, Loss: 0.44992709159851074\n",
      "Epoch 500, Loss: 0.0063047390431165695\n",
      "Epoch 1000, Loss: 0.00334879569709301\n",
      "Epoch 1500, Loss: 0.0020973128266632557\n",
      "Epoch 2000, Loss: 0.001725983340293169\n",
      "Epoch 2500, Loss: 0.001486050896346569\n",
      "Threshold reach at: 2512\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0009889501379802823\n",
      "Epoch 3500, Loss: 0.000771304068621248\n",
      "Epoch 4000, Loss: 0.0008704278152436018\n",
      "Epoch 4500, Loss: 0.0005047890590503812\n",
      "Epoch 4999, Loss: 0.0005179846193641424\n",
      "Epoch 0, Loss: 0.41470709443092346\n",
      "Epoch 500, Loss: 0.017083244398236275\n",
      "Epoch 1000, Loss: 0.010206203907728195\n",
      "Epoch 1500, Loss: 0.007745510898530483\n",
      "Epoch 2000, Loss: 0.004589021671563387\n",
      "Epoch 2500, Loss: 0.0031252584885805845\n",
      "Epoch 3000, Loss: 0.002255306113511324\n",
      "Epoch 3500, Loss: 0.0022208611480891705\n",
      "Epoch 4000, Loss: 0.0020361619535833597\n",
      "Threshold reach at: 4210\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.002331515308469534\n",
      "Epoch 4999, Loss: 0.0027168288361281157\n",
      "Epoch 0, Loss: 0.4685569405555725\n",
      "Epoch 500, Loss: 0.010234979912638664\n",
      "Epoch 1000, Loss: 0.0022051334381103516\n",
      "Epoch 1500, Loss: 0.0017215099651366472\n",
      "Epoch 2000, Loss: 0.0011433942709118128\n",
      "Epoch 2500, Loss: 0.0008153206435963511\n",
      "Epoch 3000, Loss: 0.0007248058100230992\n",
      "Epoch 3500, Loss: 0.0005502752610482275\n",
      "Epoch 4000, Loss: 0.00047008279943838716\n",
      "Epoch 4500, Loss: 0.0005032800836488605\n",
      "Epoch 4999, Loss: 0.0003779881517402828\n",
      "Epoch 0, Loss: 0.5024545788764954\n",
      "Epoch 500, Loss: 0.007512509357184172\n",
      "Epoch 1000, Loss: 0.003173741977661848\n",
      "Epoch 1500, Loss: 0.0019958331249654293\n",
      "Epoch 2000, Loss: 0.001531891175545752\n",
      "Threshold reach at: 2209\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0011262634070590138\n",
      "Epoch 3000, Loss: 0.0010067381663247943\n",
      "Epoch 3500, Loss: 0.0009017573902383447\n",
      "Epoch 4000, Loss: 0.0007189695606939495\n",
      "Epoch 4500, Loss: 0.0006426546606235206\n",
      "Epoch 4999, Loss: 0.0004714315291494131\n",
      "--- Experiment 17/20 ---\n",
      "Epoch 0, Loss: 0.8019047975540161\n",
      "Epoch 500, Loss: 0.01106978952884674\n",
      "Epoch 1000, Loss: 0.005780001170933247\n",
      "Epoch 1500, Loss: 0.0035010450519621372\n",
      "Epoch 2000, Loss: 0.002491319552063942\n",
      "Epoch 2500, Loss: 0.0011539639672264457\n",
      "Threshold reach at: 2917\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0009912947425618768\n",
      "Epoch 3500, Loss: 0.0006167002720758319\n",
      "Epoch 4000, Loss: 0.0006351268384605646\n",
      "Epoch 4500, Loss: 0.0004267677431926131\n",
      "Epoch 4999, Loss: 0.0006162996869534254\n",
      "Epoch 0, Loss: 0.564544677734375\n",
      "Epoch 500, Loss: 0.01737319305539131\n",
      "Epoch 1000, Loss: 0.008400837890803814\n",
      "Epoch 1500, Loss: 0.005146288778632879\n",
      "Epoch 2000, Loss: 0.008444087579846382\n",
      "Epoch 2500, Loss: 0.0035021500661969185\n",
      "Epoch 3000, Loss: 0.0023821680806577206\n",
      "Threshold reach at: 3457\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.001200142316520214\n",
      "Epoch 4000, Loss: 0.0019968287087976933\n",
      "Epoch 4500, Loss: 0.0017665054183453321\n",
      "Epoch 4999, Loss: 0.0016265453305095434\n",
      "Epoch 0, Loss: 0.2665261924266815\n",
      "Epoch 500, Loss: 0.00926245003938675\n",
      "Epoch 1000, Loss: 0.003898650174960494\n",
      "Epoch 1500, Loss: 0.002802775241434574\n",
      "Epoch 2000, Loss: 0.001721167005598545\n",
      "Epoch 2500, Loss: 0.0010067153489217162\n",
      "Epoch 3000, Loss: 0.0010457707103341818\n",
      "Epoch 3500, Loss: 0.0007923337398096919\n",
      "Epoch 4000, Loss: 0.0007342424942180514\n",
      "Epoch 4500, Loss: 0.0006826502503827214\n",
      "Epoch 4999, Loss: 0.0005013735499233007\n",
      "Epoch 0, Loss: 0.3459417521953583\n",
      "Epoch 500, Loss: 0.008180765435099602\n",
      "Epoch 1000, Loss: 0.0034054676070809364\n",
      "Epoch 1500, Loss: 0.0024331307504326105\n",
      "Epoch 2000, Loss: 0.001469119219109416\n",
      "Threshold reach at: 2118\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0009594387374818325\n",
      "Epoch 3000, Loss: 0.0008623446919955313\n",
      "Epoch 3500, Loss: 0.0006027869530953467\n",
      "Epoch 4000, Loss: 0.0006841116119176149\n",
      "Epoch 4500, Loss: 0.0005408771103248\n",
      "Epoch 4999, Loss: 0.0005726419622078538\n",
      "--- Experiment 18/20 ---\n",
      "Epoch 0, Loss: 0.9310463666915894\n",
      "Epoch 500, Loss: 0.012209152802824974\n",
      "Epoch 1000, Loss: 0.004757468588650227\n",
      "Epoch 1500, Loss: 0.0027252421714365482\n",
      "Epoch 2000, Loss: 0.0024028208572417498\n",
      "Epoch 2500, Loss: 0.0013548769056797028\n",
      "Threshold reach at: 2865\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.000887411180883646\n",
      "Epoch 3500, Loss: 0.0010981466621160507\n",
      "Epoch 4000, Loss: 0.00067884725285694\n",
      "Epoch 4500, Loss: 0.0004935144097544253\n",
      "Epoch 4999, Loss: 0.00044890260323882103\n",
      "Epoch 0, Loss: 0.36439335346221924\n",
      "Epoch 500, Loss: 0.014912113547325134\n",
      "Epoch 1000, Loss: 0.010413723066449165\n",
      "Epoch 1500, Loss: 0.006271044723689556\n",
      "Epoch 2000, Loss: 0.004348871298134327\n",
      "Epoch 2500, Loss: 0.004367549438029528\n",
      "Epoch 3000, Loss: 0.004943266045302153\n",
      "Epoch 3500, Loss: 0.0036380356177687645\n",
      "Epoch 4000, Loss: 0.0025876478757709265\n",
      "Epoch 4500, Loss: 0.0021774526685476303\n",
      "Epoch 4999, Loss: 0.002057841280475259\n",
      "Epoch 0, Loss: 0.7117060422897339\n",
      "Epoch 500, Loss: 0.010115716606378555\n",
      "Epoch 1000, Loss: 0.0033491256181150675\n",
      "Epoch 1500, Loss: 0.0019800602458417416\n",
      "Epoch 2000, Loss: 0.001465263543650508\n",
      "Epoch 2500, Loss: 0.0011035884963348508\n",
      "Epoch 3000, Loss: 0.0009962937328964472\n",
      "Epoch 3500, Loss: 0.0008404773543588817\n",
      "Epoch 4000, Loss: 0.0007188674062490463\n",
      "Epoch 4500, Loss: 0.0006234645843505859\n",
      "Epoch 4999, Loss: 0.0006008123746141791\n",
      "Epoch 0, Loss: 0.5787554383277893\n",
      "Epoch 500, Loss: 0.009853682480752468\n",
      "Epoch 1000, Loss: 0.0031836898997426033\n",
      "Epoch 1500, Loss: 0.0018027287442237139\n",
      "Threshold reach at: 1907\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0011085770092904568\n",
      "Epoch 2500, Loss: 0.0007792129181325436\n",
      "Epoch 3000, Loss: 0.0007471207645721734\n",
      "Epoch 3500, Loss: 0.0007003780920058489\n",
      "Epoch 4000, Loss: 0.0005841085221618414\n",
      "Epoch 4500, Loss: 0.00044705922482535243\n",
      "Epoch 4999, Loss: 0.0004049886192660779\n",
      "--- Experiment 19/20 ---\n",
      "Epoch 0, Loss: 0.6031531691551208\n",
      "Epoch 500, Loss: 0.00811925157904625\n",
      "Epoch 1000, Loss: 0.003603540826588869\n",
      "Epoch 1500, Loss: 0.0026820700149983168\n",
      "Epoch 2000, Loss: 0.0024519769940525293\n",
      "Epoch 2500, Loss: 0.0016060497146099806\n",
      "Epoch 3000, Loss: 0.0012115377467125654\n",
      "Epoch 3500, Loss: 0.001645286101847887\n",
      "Threshold reach at: 3896\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.000982574187219143\n",
      "Epoch 4500, Loss: 0.0007247100584208965\n",
      "Epoch 4999, Loss: 0.0011348016560077667\n",
      "Epoch 0, Loss: 0.5144667625427246\n",
      "Epoch 500, Loss: 0.018830090761184692\n",
      "Epoch 1000, Loss: 0.013172556646168232\n",
      "Epoch 1500, Loss: 0.006449948064982891\n",
      "Epoch 2000, Loss: 0.005252770613878965\n",
      "Epoch 2500, Loss: 0.005988303571939468\n",
      "Epoch 3000, Loss: 0.003541515674442053\n",
      "Epoch 3500, Loss: 0.003908946644514799\n",
      "Epoch 4000, Loss: 0.0037819768767803907\n",
      "Epoch 4500, Loss: 0.003313837107270956\n",
      "Epoch 4999, Loss: 0.0025220911484211683\n",
      "Epoch 0, Loss: 0.6241034269332886\n",
      "Epoch 500, Loss: 0.015355617739260197\n",
      "Epoch 1000, Loss: 0.005597332492470741\n",
      "Epoch 1500, Loss: 0.0038532556500285864\n",
      "Epoch 2000, Loss: 0.0030294577591121197\n",
      "Epoch 2500, Loss: 0.002631818875670433\n",
      "Epoch 3000, Loss: 0.001893168780952692\n",
      "Epoch 3500, Loss: 0.0014648397918790579\n",
      "Epoch 4000, Loss: 0.001264299382455647\n",
      "Epoch 4500, Loss: 0.0011978080729022622\n",
      "Epoch 4999, Loss: 0.001030519139021635\n",
      "Epoch 0, Loss: 0.6927798986434937\n",
      "Epoch 500, Loss: 0.010690165683627129\n",
      "Epoch 1000, Loss: 0.0027707433328032494\n",
      "Epoch 1500, Loss: 0.0013905504019930959\n",
      "Threshold reach at: 1643\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0009954673005267978\n",
      "Epoch 2500, Loss: 0.0008679558522999287\n",
      "Epoch 3000, Loss: 0.0006774923531338573\n",
      "Epoch 3500, Loss: 0.0006191361462697387\n",
      "Epoch 4000, Loss: 0.0005211826064623892\n",
      "Epoch 4500, Loss: 0.00048182543832808733\n",
      "Epoch 4999, Loss: 0.00041059323120862246\n",
      "--- Experiment 20/20 ---\n",
      "Epoch 0, Loss: 0.3125292956829071\n",
      "Epoch 500, Loss: 0.005124516319483519\n",
      "Epoch 1000, Loss: 0.002837494248524308\n",
      "Epoch 1500, Loss: 0.001976434141397476\n",
      "Epoch 2000, Loss: 0.001383911119773984\n",
      "Threshold reach at: 2079\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0009194679441861808\n",
      "Epoch 3000, Loss: 0.0011490237666293979\n",
      "Epoch 3500, Loss: 0.0006193773588165641\n",
      "Epoch 4000, Loss: 0.0005988572956994176\n",
      "Epoch 4500, Loss: 0.0004910466377623379\n",
      "Epoch 4999, Loss: 0.00020326855883467942\n",
      "Epoch 0, Loss: 0.23867738246917725\n",
      "Epoch 500, Loss: 0.0143263665959239\n",
      "Epoch 1000, Loss: 0.007225331384688616\n",
      "Epoch 1500, Loss: 0.0056004333309829235\n",
      "Epoch 2000, Loss: 0.003039176343008876\n",
      "Epoch 2500, Loss: 0.0026049846783280373\n",
      "Epoch 3000, Loss: 0.002728540450334549\n",
      "Epoch 3500, Loss: 0.004534395877271891\n",
      "Epoch 4000, Loss: 0.0023675537668168545\n",
      "Threshold reach at: 4077\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.0015215786406770349\n",
      "Epoch 4999, Loss: 0.0011894284980371594\n",
      "Epoch 0, Loss: 0.27531519532203674\n",
      "Epoch 500, Loss: 0.0061050220392644405\n",
      "Epoch 1000, Loss: 0.0033057122491300106\n",
      "Epoch 1500, Loss: 0.0022880209144204855\n",
      "Epoch 2000, Loss: 0.0012588310055434704\n",
      "Epoch 2500, Loss: 0.0012386026792228222\n",
      "Epoch 3000, Loss: 0.0008242832846008241\n",
      "Epoch 3500, Loss: 0.000713781570084393\n",
      "Epoch 4000, Loss: 0.0007034320151433349\n",
      "Epoch 4500, Loss: 0.0006506157806143165\n",
      "Epoch 4999, Loss: 0.0006327642477117479\n",
      "Epoch 0, Loss: 0.42675817012786865\n",
      "Epoch 500, Loss: 0.008241133764386177\n",
      "Epoch 1000, Loss: 0.003385018790140748\n",
      "Epoch 1500, Loss: 0.0020723678171634674\n",
      "Threshold reach at: 1903\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0008410277077928185\n",
      "Epoch 2500, Loss: 0.0009448567871004343\n",
      "Epoch 3000, Loss: 0.0007910183630883694\n",
      "Epoch 3500, Loss: 0.0007782999891787767\n",
      "Epoch 4000, Loss: 0.0006186537793837488\n",
      "Epoch 4500, Loss: 0.0005283226491883397\n",
      "Epoch 4999, Loss: 0.0005330255371518433\n",
      "Average Validation Loss (Base):   0.000644 ± 0.000199\n",
      "Average Validation Loss (Import): 0.001462 ± 0.000426\n",
      "Average Validation Loss (GP Out): 0.016104 ± 0.010140\n",
      "Average Validation Loss (GP Res): 0.000405 ± 0.000145\n",
      "\n",
      "==== Running experiments for IC: sin, BC: periodic ====\n",
      "--- Experiment 1/20 ---\n",
      "Epoch 0, Loss: 0.5272701382637024\n",
      "Epoch 500, Loss: 0.007445591501891613\n",
      "Epoch 1000, Loss: 0.0039929961785674095\n",
      "Epoch 1500, Loss: 0.0017526124138385057\n",
      "Epoch 2000, Loss: 0.0018803826533257961\n",
      "Epoch 2500, Loss: 0.0011810617288574576\n",
      "Threshold reach at: 2757\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.000749436963815242\n",
      "Epoch 3500, Loss: 0.0005951750790700316\n",
      "Epoch 4000, Loss: 0.0005537572433240712\n",
      "Epoch 4500, Loss: 0.00041261687874794006\n",
      "Epoch 4999, Loss: 0.0006728266016580164\n",
      "Epoch 0, Loss: 0.393004834651947\n",
      "Epoch 500, Loss: 0.011977776885032654\n",
      "Epoch 1000, Loss: 0.007254758384078741\n",
      "Epoch 1500, Loss: 0.00453790882602334\n",
      "Epoch 2000, Loss: 0.0032212939113378525\n",
      "Epoch 2500, Loss: 0.004243954084813595\n",
      "Epoch 3000, Loss: 0.007021669298410416\n",
      "Epoch 3500, Loss: 0.002542195376008749\n",
      "Epoch 4000, Loss: 0.00207603070884943\n",
      "Epoch 4500, Loss: 0.0016139335930347443\n",
      "Epoch 4999, Loss: 0.0019094678573310375\n",
      "Epoch 0, Loss: 0.6354315876960754\n",
      "Epoch 500, Loss: 0.012279234826564789\n",
      "Epoch 1000, Loss: 0.003343121614307165\n",
      "Epoch 1500, Loss: 0.0017862061504274607\n",
      "Epoch 2000, Loss: 0.0014242035103961825\n",
      "Epoch 2500, Loss: 0.0018412364879623055\n",
      "Epoch 3000, Loss: 0.0008306941017508507\n",
      "Epoch 3500, Loss: 0.0007707334589213133\n",
      "Epoch 4000, Loss: 0.0006129858084022999\n",
      "Epoch 4500, Loss: 0.00048026509466581047\n",
      "Epoch 4999, Loss: 0.0005759595660492778\n",
      "Epoch 0, Loss: 0.37434282898902893\n",
      "Epoch 500, Loss: 0.008623034693300724\n",
      "Epoch 1000, Loss: 0.0033315569162368774\n",
      "Epoch 1500, Loss: 0.0015628272667527199\n",
      "Threshold reach at: 1769\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0009294183691963553\n",
      "Epoch 2500, Loss: 0.0007919117342680693\n",
      "Epoch 3000, Loss: 0.0006585191003978252\n",
      "Epoch 3500, Loss: 0.00047290828661061823\n",
      "Epoch 4000, Loss: 0.0005786860128864646\n",
      "Epoch 4500, Loss: 0.00046715643838979304\n",
      "Epoch 4999, Loss: 0.0005284722428768873\n",
      "--- Experiment 2/20 ---\n",
      "Epoch 0, Loss: 0.46964049339294434\n",
      "Epoch 500, Loss: 0.008655838668346405\n",
      "Epoch 1000, Loss: 0.004041649401187897\n",
      "Epoch 1500, Loss: 0.0024003060534596443\n",
      "Epoch 2000, Loss: 0.0017368313856422901\n",
      "Epoch 2500, Loss: 0.0015046305488795042\n",
      "Epoch 3000, Loss: 0.0015433031367138028\n",
      "Epoch 3500, Loss: 0.001466149347834289\n",
      "Threshold reach at: 3865\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.000929858535528183\n",
      "Epoch 4500, Loss: 0.0006741865654475987\n",
      "Epoch 4999, Loss: 0.0005965175805613399\n",
      "Epoch 0, Loss: 0.5430576205253601\n",
      "Epoch 500, Loss: 0.02044690027832985\n",
      "Epoch 1000, Loss: 0.011620649136602879\n",
      "Epoch 1500, Loss: 0.0066716019064188\n",
      "Epoch 2000, Loss: 0.0065216622315347195\n",
      "Epoch 2500, Loss: 0.0043257917277514935\n",
      "Epoch 3000, Loss: 0.004021584056317806\n",
      "Epoch 3500, Loss: 0.0028477658051997423\n",
      "Epoch 4000, Loss: 0.002529899124056101\n",
      "Epoch 4500, Loss: 0.001918242545798421\n",
      "Epoch 4999, Loss: 0.0016846387879922986\n",
      "Epoch 0, Loss: 0.8040552139282227\n",
      "Epoch 500, Loss: 0.014422466047108173\n",
      "Epoch 1000, Loss: 0.005108680576086044\n",
      "Epoch 1500, Loss: 0.0038532745093107224\n",
      "Epoch 2000, Loss: 0.0031137082260102034\n",
      "Epoch 2500, Loss: 0.0027288286946713924\n",
      "Epoch 3000, Loss: 0.0024546468630433083\n",
      "Epoch 3500, Loss: 0.0021086421329528093\n",
      "Epoch 4000, Loss: 0.00154592574108392\n",
      "Epoch 4500, Loss: 0.0013951134169474244\n",
      "Epoch 4999, Loss: 0.0009111502440646291\n",
      "Epoch 0, Loss: 0.3428060710430145\n",
      "Epoch 500, Loss: 0.010630873963236809\n",
      "Epoch 1000, Loss: 0.0028111168649047613\n",
      "Epoch 1500, Loss: 0.0012343269772827625\n",
      "Threshold reach at: 1703\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0012100804597139359\n",
      "Epoch 2500, Loss: 0.0009778360836207867\n",
      "Epoch 3000, Loss: 0.0007655465742573142\n",
      "Epoch 3500, Loss: 0.0005832043825648725\n",
      "Epoch 4000, Loss: 0.0005163951427675784\n",
      "Epoch 4500, Loss: 0.0005425020353868604\n",
      "Epoch 4999, Loss: 0.00043959252070635557\n",
      "--- Experiment 3/20 ---\n",
      "Epoch 0, Loss: 0.46696698665618896\n",
      "Epoch 500, Loss: 0.006985703017562628\n",
      "Epoch 1000, Loss: 0.004631048999726772\n",
      "Epoch 1500, Loss: 0.0030976259149610996\n",
      "Epoch 2000, Loss: 0.0021027298644185066\n",
      "Epoch 2500, Loss: 0.00135480472818017\n",
      "Epoch 3000, Loss: 0.0009044174803420901\n",
      "Threshold reach at: 3263\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0013185972347855568\n",
      "Epoch 4000, Loss: 0.0010445346124470234\n",
      "Epoch 4500, Loss: 0.0009481654269620776\n",
      "Epoch 4999, Loss: 0.0007361171301454306\n",
      "Epoch 0, Loss: 0.36896511912345886\n",
      "Epoch 500, Loss: 0.016410812735557556\n",
      "Epoch 1000, Loss: 0.01115136407315731\n",
      "Epoch 1500, Loss: 0.005568822845816612\n",
      "Epoch 2000, Loss: 0.004062484949827194\n",
      "Epoch 2500, Loss: 0.0029494825284928083\n",
      "Epoch 3000, Loss: 0.0031541315838694572\n",
      "Epoch 3500, Loss: 0.0023460397496819496\n",
      "Epoch 4000, Loss: 0.0018058784771710634\n",
      "Epoch 4500, Loss: 0.003804266918450594\n",
      "Threshold reach at: 4565\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.001550756860524416\n",
      "Epoch 0, Loss: 0.5564795136451721\n",
      "Epoch 500, Loss: 0.010965661145746708\n",
      "Epoch 1000, Loss: 0.004065321292728186\n",
      "Epoch 1500, Loss: 0.0024445373564958572\n",
      "Epoch 2000, Loss: 0.0012677699560299516\n",
      "Epoch 2500, Loss: 0.0007617920637130737\n",
      "Epoch 3000, Loss: 0.0008747527026571333\n",
      "Epoch 3500, Loss: 0.0007970888400450349\n",
      "Epoch 4000, Loss: 0.0005325203528627753\n",
      "Epoch 4500, Loss: 0.000592667143791914\n",
      "Epoch 4999, Loss: 0.0005722921341657639\n",
      "Epoch 0, Loss: 0.6568191051483154\n",
      "Epoch 500, Loss: 0.012517070397734642\n",
      "Epoch 1000, Loss: 0.004154953174293041\n",
      "Epoch 1500, Loss: 0.0028612525202333927\n",
      "Epoch 2000, Loss: 0.001497725141234696\n",
      "Threshold reach at: 2106\n",
      "Val loss: tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0007715786341577768\n",
      "Epoch 3000, Loss: 0.000820113462395966\n",
      "Epoch 3500, Loss: 0.0006815026281401515\n",
      "Epoch 4000, Loss: 0.0007293528178706765\n",
      "Epoch 4500, Loss: 0.0005710463155992329\n",
      "Epoch 4999, Loss: 0.00041223777225241065\n",
      "--- Experiment 4/20 ---\n",
      "Epoch 0, Loss: 0.5674044489860535\n",
      "Epoch 500, Loss: 0.011487007141113281\n",
      "Epoch 1000, Loss: 0.005963119678199291\n",
      "Epoch 1500, Loss: 0.004844574723392725\n",
      "Epoch 2000, Loss: 0.0031410790979862213\n",
      "Epoch 2500, Loss: 0.0024889123160392046\n",
      "Epoch 3000, Loss: 0.0021891798824071884\n",
      "Epoch 3500, Loss: 0.0018901382572948933\n",
      "Epoch 4000, Loss: 0.0010620737448334694\n",
      "Epoch 4500, Loss: 0.0012904993491247296\n",
      "Threshold reach at: 4650\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.0008525236044079065\n",
      "Epoch 0, Loss: 0.3896615207195282\n",
      "Epoch 500, Loss: 0.01744021102786064\n",
      "Epoch 1000, Loss: 0.008579722605645657\n",
      "Epoch 1500, Loss: 0.006769406609237194\n",
      "Epoch 2000, Loss: 0.004063996020704508\n",
      "Epoch 2500, Loss: 0.004159979056566954\n",
      "Epoch 3000, Loss: 0.0034387134946882725\n",
      "Epoch 3500, Loss: 0.002692588372156024\n",
      "Epoch 4000, Loss: 0.0033877301029860973\n",
      "Threshold reach at: 4378\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.001742699882015586\n",
      "Epoch 4999, Loss: 0.0012863891897723079\n",
      "Epoch 0, Loss: 0.29092633724212646\n",
      "Epoch 500, Loss: 0.009705794043838978\n",
      "Epoch 1000, Loss: 0.0028941878117620945\n",
      "Epoch 1500, Loss: 0.0014202401507645845\n",
      "Epoch 2000, Loss: 0.0011169682256877422\n",
      "Epoch 2500, Loss: 0.0009817376267164946\n",
      "Epoch 3000, Loss: 0.0007934412569738925\n",
      "Epoch 3500, Loss: 0.0006406076718121767\n",
      "Epoch 4000, Loss: 0.0005386590491980314\n",
      "Epoch 4500, Loss: 0.00042706570820882916\n",
      "Epoch 4999, Loss: 0.0003608399420045316\n",
      "Epoch 0, Loss: 0.3831653892993927\n",
      "Epoch 500, Loss: 0.009099336341023445\n",
      "Epoch 1000, Loss: 0.004933948628604412\n",
      "Epoch 1500, Loss: 0.0031755384989082813\n",
      "Epoch 2000, Loss: 0.0021905084140598774\n",
      "Epoch 2500, Loss: 0.0014263298362493515\n",
      "Threshold reach at: 2909\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0011580167338252068\n",
      "Epoch 3500, Loss: 0.001214481657370925\n",
      "Epoch 4000, Loss: 0.0011517300736159086\n",
      "Epoch 4500, Loss: 0.0009494074620306492\n",
      "Epoch 4999, Loss: 0.0005653313128277659\n",
      "--- Experiment 5/20 ---\n",
      "Epoch 0, Loss: 0.5110414028167725\n",
      "Epoch 500, Loss: 0.008723128587007523\n",
      "Epoch 1000, Loss: 0.0040646689012646675\n",
      "Epoch 1500, Loss: 0.00231563881970942\n",
      "Epoch 2000, Loss: 0.0020468642469495535\n",
      "Epoch 2500, Loss: 0.0013642502017319202\n",
      "Epoch 3000, Loss: 0.0017203574534505606\n",
      "Epoch 3500, Loss: 0.001186446868814528\n",
      "Threshold reach at: 3684\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.0010458810720592737\n",
      "Epoch 4500, Loss: 0.0008399463258683681\n",
      "Epoch 4999, Loss: 0.000850404379889369\n",
      "Epoch 0, Loss: 0.6604910492897034\n",
      "Epoch 500, Loss: 0.020134277641773224\n",
      "Epoch 1000, Loss: 0.013758084736764431\n",
      "Epoch 1500, Loss: 0.007594419177621603\n",
      "Epoch 2000, Loss: 0.005904661025851965\n",
      "Epoch 2500, Loss: 0.004778875038027763\n",
      "Epoch 3000, Loss: 0.003154184902086854\n",
      "Epoch 3500, Loss: 0.0025481441989541054\n",
      "Epoch 4000, Loss: 0.0023303180932998657\n",
      "Epoch 4500, Loss: 0.0017522303387522697\n",
      "Threshold reach at: 4614\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.0035998173989355564\n",
      "Epoch 0, Loss: 0.385442316532135\n",
      "Epoch 500, Loss: 0.008433802053332329\n",
      "Epoch 1000, Loss: 0.004284287802875042\n",
      "Epoch 1500, Loss: 0.0031921053305268288\n",
      "Epoch 2000, Loss: 0.0022659292444586754\n",
      "Epoch 2500, Loss: 0.0016173062613233924\n",
      "Epoch 3000, Loss: 0.0009677328635007143\n",
      "Epoch 3500, Loss: 0.0009717220673337579\n",
      "Epoch 4000, Loss: 0.0007975368062034249\n",
      "Epoch 4500, Loss: 0.0005390881560742855\n",
      "Epoch 4999, Loss: 0.0004488229169510305\n",
      "Epoch 0, Loss: 0.33966538310050964\n",
      "Epoch 500, Loss: 0.009078447706997395\n",
      "Epoch 1000, Loss: 0.002494480460882187\n",
      "Epoch 1500, Loss: 0.0018133767880499363\n",
      "Threshold reach at: 1937\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0011465479619801044\n",
      "Epoch 2500, Loss: 0.000867239898070693\n",
      "Epoch 3000, Loss: 0.0010378531878814101\n",
      "Epoch 3500, Loss: 0.0006683623651042581\n",
      "Epoch 4000, Loss: 0.00044094209442846477\n",
      "Epoch 4500, Loss: 0.0005095417145639658\n",
      "Epoch 4999, Loss: 0.0005659173475578427\n",
      "--- Experiment 6/20 ---\n",
      "Epoch 0, Loss: 0.5183663368225098\n",
      "Epoch 500, Loss: 0.008259490132331848\n",
      "Epoch 1000, Loss: 0.003369526006281376\n",
      "Epoch 1500, Loss: 0.002043271204456687\n",
      "Epoch 2000, Loss: 0.002372459275647998\n",
      "Epoch 2500, Loss: 0.002375731710344553\n",
      "Epoch 3000, Loss: 0.0011632067617028952\n",
      "Threshold reach at: 3421\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0008695875876583159\n",
      "Epoch 4000, Loss: 0.0006331349723041058\n",
      "Epoch 4500, Loss: 0.0006956587312743068\n",
      "Epoch 4999, Loss: 0.0009922394528985023\n",
      "Epoch 0, Loss: 0.7280367016792297\n",
      "Epoch 500, Loss: 0.01598038151860237\n",
      "Epoch 1000, Loss: 0.007103431038558483\n",
      "Epoch 1500, Loss: 0.005151432938873768\n",
      "Epoch 2000, Loss: 0.004568149335682392\n",
      "Epoch 2500, Loss: 0.002767150988802314\n",
      "Epoch 3000, Loss: 0.003559758421033621\n",
      "Epoch 3500, Loss: 0.002032105578109622\n",
      "Epoch 4000, Loss: 0.0028210380114614964\n",
      "Epoch 4500, Loss: 0.0017577228136360645\n",
      "Threshold reach at: 4554\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.0016707532340660691\n",
      "Epoch 0, Loss: 0.2739744186401367\n",
      "Epoch 500, Loss: 0.011058383621275425\n",
      "Epoch 1000, Loss: 0.004067122936248779\n",
      "Epoch 1500, Loss: 0.002504912670701742\n",
      "Epoch 2000, Loss: 0.001930523314513266\n",
      "Epoch 2500, Loss: 0.0014554003719240427\n",
      "Epoch 3000, Loss: 0.0012585645308718085\n",
      "Epoch 3500, Loss: 0.0009152333368547261\n",
      "Epoch 4000, Loss: 0.0007969553698785603\n",
      "Epoch 4500, Loss: 0.0006023600581102073\n",
      "Epoch 4999, Loss: 0.0006126195657998323\n",
      "Epoch 0, Loss: 0.45590847730636597\n",
      "Epoch 500, Loss: 0.01133019756525755\n",
      "Epoch 1000, Loss: 0.004242556169629097\n",
      "Epoch 1500, Loss: 0.0026062950491905212\n",
      "Epoch 2000, Loss: 0.001583147794008255\n",
      "Threshold reach at: 2188\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.000937970238737762\n",
      "Epoch 3000, Loss: 0.000646864646114409\n",
      "Epoch 3500, Loss: 0.0007167156436480582\n",
      "Epoch 4000, Loss: 0.0004936483455821872\n",
      "Epoch 4500, Loss: 0.00046534251305274665\n",
      "Epoch 4999, Loss: 0.0004332625831011683\n",
      "--- Experiment 7/20 ---\n",
      "Epoch 0, Loss: 0.8123263716697693\n",
      "Epoch 500, Loss: 0.010192455723881721\n",
      "Epoch 1000, Loss: 0.004268838092684746\n",
      "Epoch 1500, Loss: 0.002646744716912508\n",
      "Epoch 2000, Loss: 0.0031627737917006016\n",
      "Epoch 2500, Loss: 0.001969375880435109\n",
      "Epoch 3000, Loss: 0.0017975291702896357\n",
      "Epoch 3500, Loss: 0.0016641439869999886\n",
      "Epoch 4000, Loss: 0.0011532711796462536\n",
      "Epoch 4500, Loss: 0.0012068272335454822\n",
      "Threshold reach at: 4811\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.0009284759871661663\n",
      "Epoch 0, Loss: 0.47833889722824097\n",
      "Epoch 500, Loss: 0.01909814029932022\n",
      "Epoch 1000, Loss: 0.013548392802476883\n",
      "Epoch 1500, Loss: 0.007262096740305424\n",
      "Epoch 2000, Loss: 0.004815124906599522\n",
      "Epoch 2500, Loss: 0.0036592530086636543\n",
      "Epoch 3000, Loss: 0.0027256424073129892\n",
      "Epoch 3500, Loss: 0.002335103927180171\n",
      "Epoch 4000, Loss: 0.0014841355150565505\n",
      "Threshold reach at: 4482\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.002659635152667761\n",
      "Epoch 4999, Loss: 0.0014677323633804917\n",
      "Epoch 0, Loss: 0.6147934198379517\n",
      "Epoch 500, Loss: 0.018796686083078384\n",
      "Epoch 1000, Loss: 0.006189078092575073\n",
      "Epoch 1500, Loss: 0.003904924029484391\n",
      "Epoch 2000, Loss: 0.0034949013497680426\n",
      "Epoch 2500, Loss: 0.00269276462495327\n",
      "Epoch 3000, Loss: 0.0022417991422116756\n",
      "Epoch 3500, Loss: 0.0018927964847534895\n",
      "Epoch 4000, Loss: 0.0015193196013569832\n",
      "Epoch 4500, Loss: 0.0011623578611761332\n",
      "Epoch 4999, Loss: 0.0011264588683843613\n",
      "Epoch 0, Loss: 0.35598182678222656\n",
      "Epoch 500, Loss: 0.007819442078471184\n",
      "Epoch 1000, Loss: 0.00351513409987092\n",
      "Epoch 1500, Loss: 0.0024690681602805853\n",
      "Epoch 2000, Loss: 0.001262066187337041\n",
      "Threshold reach at: 2068\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0009789966279640794\n",
      "Epoch 3000, Loss: 0.000951158523093909\n",
      "Epoch 3500, Loss: 0.0008124882588163018\n",
      "Epoch 4000, Loss: 0.0006307841977104545\n",
      "Epoch 4500, Loss: 0.0006318758241832256\n",
      "Epoch 4999, Loss: 0.0004257357504684478\n",
      "--- Experiment 8/20 ---\n",
      "Epoch 0, Loss: 0.32679253816604614\n",
      "Epoch 500, Loss: 0.007422302849590778\n",
      "Epoch 1000, Loss: 0.002530149184167385\n",
      "Epoch 1500, Loss: 0.0015795885119587183\n",
      "Threshold reach at: 1839\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0007457489846274257\n",
      "Epoch 2500, Loss: 0.0005564498715102673\n",
      "Epoch 3000, Loss: 0.00043058564187958837\n",
      "Epoch 3500, Loss: 0.0004970222944393754\n",
      "Epoch 4000, Loss: 0.0007302802987396717\n",
      "Epoch 4500, Loss: 0.0003227149718441069\n",
      "Epoch 4999, Loss: 0.00043946848018094897\n",
      "Epoch 0, Loss: 0.5683256983757019\n",
      "Epoch 500, Loss: 0.019344350323081017\n",
      "Epoch 1000, Loss: 0.009859857149422169\n",
      "Epoch 1500, Loss: 0.007350778207182884\n",
      "Epoch 2000, Loss: 0.0065979547798633575\n",
      "Epoch 2500, Loss: 0.00496439915150404\n",
      "Epoch 3000, Loss: 0.00327800284139812\n",
      "Epoch 3500, Loss: 0.003832075744867325\n",
      "Epoch 4000, Loss: 0.003057817928493023\n",
      "Epoch 4500, Loss: 0.002254971768707037\n",
      "Epoch 4999, Loss: 0.0016727903857827187\n",
      "Epoch 0, Loss: 0.3182032108306885\n",
      "Epoch 500, Loss: 0.01653001457452774\n",
      "Epoch 1000, Loss: 0.009776471182703972\n",
      "Epoch 1500, Loss: 0.005960110574960709\n",
      "Epoch 2000, Loss: 0.0040248059667646885\n",
      "Epoch 2500, Loss: 0.003226347267627716\n",
      "Epoch 3000, Loss: 0.0024479953572154045\n",
      "Epoch 3500, Loss: 0.001967786345630884\n",
      "Epoch 4000, Loss: 0.0017338378820568323\n",
      "Epoch 4500, Loss: 0.0013276825193315744\n",
      "Epoch 4999, Loss: 0.0013550414005294442\n",
      "Epoch 0, Loss: 0.47920089960098267\n",
      "Epoch 500, Loss: 0.00938744843006134\n",
      "Epoch 1000, Loss: 0.0025505463127046824\n",
      "Epoch 1500, Loss: 0.0014231835957616568\n",
      "Threshold reach at: 1550\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0009839260019361973\n",
      "Epoch 2500, Loss: 0.0008512053173035383\n",
      "Epoch 3000, Loss: 0.0006146393716335297\n",
      "Epoch 3500, Loss: 0.0007488191477023065\n",
      "Epoch 4000, Loss: 0.0004680856945924461\n",
      "Epoch 4500, Loss: 0.0004889629781246185\n",
      "Epoch 4999, Loss: 0.0005074064247310162\n",
      "--- Experiment 9/20 ---\n",
      "Epoch 0, Loss: 0.4565171003341675\n",
      "Epoch 500, Loss: 0.009096174500882626\n",
      "Epoch 1000, Loss: 0.00397122697904706\n",
      "Epoch 1500, Loss: 0.001671075588092208\n",
      "Epoch 2000, Loss: 0.001313940854743123\n",
      "Epoch 2500, Loss: 0.0011235987767577171\n",
      "Threshold reach at: 2553\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0006812489591538906\n",
      "Epoch 3500, Loss: 0.0005129590281285346\n",
      "Epoch 4000, Loss: 0.0006221132352948189\n",
      "Epoch 4500, Loss: 0.0005385565455071628\n",
      "Epoch 4999, Loss: 0.0004806466167792678\n",
      "Epoch 0, Loss: 0.40373072028160095\n",
      "Epoch 500, Loss: 0.0190581027418375\n",
      "Epoch 1000, Loss: 0.01229360792785883\n",
      "Epoch 1500, Loss: 0.008096934296190739\n",
      "Epoch 2000, Loss: 0.0064151473343372345\n",
      "Epoch 2500, Loss: 0.006431346759200096\n",
      "Epoch 3000, Loss: 0.0036708395928144455\n",
      "Epoch 3500, Loss: 0.005606393795460463\n",
      "Epoch 4000, Loss: 0.002732302062213421\n",
      "Epoch 4500, Loss: 0.0028919423930346966\n",
      "Epoch 4999, Loss: 0.0038179014809429646\n",
      "Epoch 0, Loss: 0.5913498997688293\n",
      "Epoch 500, Loss: 0.007973785512149334\n",
      "Epoch 1000, Loss: 0.0034548561088740826\n",
      "Epoch 1500, Loss: 0.001988978125154972\n",
      "Epoch 2000, Loss: 0.0013754856772720814\n",
      "Epoch 2500, Loss: 0.001177568337880075\n",
      "Epoch 3000, Loss: 0.0009285802370868623\n",
      "Epoch 3500, Loss: 0.0008595826802775264\n",
      "Epoch 4000, Loss: 0.0007421846967190504\n",
      "Epoch 4500, Loss: 0.0006950111710466444\n",
      "Epoch 4999, Loss: 0.00036739843199029565\n",
      "Epoch 0, Loss: 0.2451794445514679\n",
      "Epoch 500, Loss: 0.00690546864643693\n",
      "Epoch 1000, Loss: 0.001929898513481021\n",
      "Threshold reach at: 1307\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 1500, Loss: 0.0010911051649600267\n",
      "Epoch 2000, Loss: 0.0006035338155925274\n",
      "Epoch 2500, Loss: 0.00051155686378479\n",
      "Epoch 3000, Loss: 0.0003917888388969004\n",
      "Epoch 3500, Loss: 0.00032054021721705794\n",
      "Epoch 4000, Loss: 0.0002536090323701501\n",
      "Epoch 4500, Loss: 0.0003161498752888292\n",
      "Epoch 4999, Loss: 0.00024221761850640178\n",
      "--- Experiment 10/20 ---\n",
      "Epoch 0, Loss: 0.3390822112560272\n",
      "Epoch 500, Loss: 0.007149128708988428\n",
      "Epoch 1000, Loss: 0.0026722713373601437\n",
      "Epoch 1500, Loss: 0.0021056809928268194\n",
      "Epoch 2000, Loss: 0.001564845908433199\n",
      "Epoch 2500, Loss: 0.0015811766497790813\n",
      "Threshold reach at: 2766\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0012734897900372744\n",
      "Epoch 3500, Loss: 0.0005500847473740578\n",
      "Epoch 4000, Loss: 0.00034681454417295754\n",
      "Epoch 4500, Loss: 0.001226134249009192\n",
      "Epoch 4999, Loss: 0.000394258473534137\n",
      "Epoch 0, Loss: 0.7449713349342346\n",
      "Epoch 500, Loss: 0.01860681176185608\n",
      "Epoch 1000, Loss: 0.009280135855078697\n",
      "Epoch 1500, Loss: 0.00557028129696846\n",
      "Epoch 2000, Loss: 0.005139292683452368\n",
      "Epoch 2500, Loss: 0.004763864912092686\n",
      "Epoch 3000, Loss: 0.0029884502291679382\n",
      "Epoch 3500, Loss: 0.003055084962397814\n",
      "Epoch 4000, Loss: 0.0019738769624382257\n",
      "Epoch 4500, Loss: 0.001448195893317461\n",
      "Threshold reach at: 4829\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.0017531292978674173\n",
      "Epoch 0, Loss: 0.6309967637062073\n",
      "Epoch 500, Loss: 0.010840767994523048\n",
      "Epoch 1000, Loss: 0.003699966473504901\n",
      "Epoch 1500, Loss: 0.001838937634602189\n",
      "Epoch 2000, Loss: 0.0010795904090628028\n",
      "Epoch 2500, Loss: 0.000938303885050118\n",
      "Epoch 3000, Loss: 0.0008895037462934852\n",
      "Epoch 3500, Loss: 0.00071896449662745\n",
      "Epoch 4000, Loss: 0.0008414120529778302\n",
      "Epoch 4500, Loss: 0.0005236130673438311\n",
      "Epoch 4999, Loss: 0.00048347533447667956\n",
      "Epoch 0, Loss: 0.2793569266796112\n",
      "Epoch 500, Loss: 0.006902862805873156\n",
      "Epoch 1000, Loss: 0.0019305269233882427\n",
      "Epoch 1500, Loss: 0.0014972733333706856\n",
      "Threshold reach at: 1809\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0010183571139350533\n",
      "Epoch 2500, Loss: 0.0009847956243902445\n",
      "Epoch 3000, Loss: 0.0008898096857592463\n",
      "Epoch 3500, Loss: 0.0005421707173809409\n",
      "Epoch 4000, Loss: 0.0005689761019311845\n",
      "Epoch 4500, Loss: 0.0005596018163487315\n",
      "Epoch 4999, Loss: 0.0005918674869462848\n",
      "--- Experiment 11/20 ---\n",
      "Epoch 0, Loss: 0.6612511873245239\n",
      "Epoch 500, Loss: 0.008829059079289436\n",
      "Epoch 1000, Loss: 0.003294514026492834\n",
      "Epoch 1500, Loss: 0.003142275847494602\n",
      "Epoch 2000, Loss: 0.0013113899622112513\n",
      "Epoch 2500, Loss: 0.0009009519126266241\n",
      "Threshold reach at: 2786\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0009515035199001431\n",
      "Epoch 3500, Loss: 0.0008966100867837667\n",
      "Epoch 4000, Loss: 0.0007977276109158993\n",
      "Epoch 4500, Loss: 0.0009341074619442225\n",
      "Epoch 4999, Loss: 0.0009893822716549039\n",
      "Epoch 0, Loss: 0.75417560338974\n",
      "Epoch 500, Loss: 0.023327942937612534\n",
      "Epoch 1000, Loss: 0.01526080071926117\n",
      "Epoch 1500, Loss: 0.013882922008633614\n",
      "Epoch 2000, Loss: 0.007438618689775467\n",
      "Epoch 2500, Loss: 0.007165238261222839\n",
      "Epoch 3000, Loss: 0.008543725125491619\n",
      "Epoch 3500, Loss: 0.005139593034982681\n",
      "Epoch 4000, Loss: 0.005449960939586163\n",
      "Epoch 4500, Loss: 0.00423617335036397\n",
      "Epoch 4999, Loss: 0.004764317534863949\n",
      "Epoch 0, Loss: 0.25656676292419434\n",
      "Epoch 500, Loss: 0.006504539866000414\n",
      "Epoch 1000, Loss: 0.0037028337828814983\n",
      "Epoch 1500, Loss: 0.0021475304383784533\n",
      "Epoch 2000, Loss: 0.0015468322671949863\n",
      "Epoch 2500, Loss: 0.001315644127316773\n",
      "Epoch 3000, Loss: 0.00107199780177325\n",
      "Epoch 3500, Loss: 0.0009714618790894747\n",
      "Epoch 4000, Loss: 0.0008403104729950428\n",
      "Epoch 4500, Loss: 0.0006744078709743917\n",
      "Epoch 4999, Loss: 0.0005919758696109056\n",
      "Epoch 0, Loss: 0.24878013134002686\n",
      "Epoch 500, Loss: 0.00510851526632905\n",
      "Epoch 1000, Loss: 0.002920401282608509\n",
      "Epoch 1500, Loss: 0.0016543569508939981\n",
      "Threshold reach at: 1797\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0011932181660085917\n",
      "Epoch 2500, Loss: 0.0009249826543964446\n",
      "Epoch 3000, Loss: 0.0006583570502698421\n",
      "Epoch 3500, Loss: 0.0006387352477759123\n",
      "Epoch 4000, Loss: 0.0004272980149835348\n",
      "Epoch 4500, Loss: 0.0003911004459951073\n",
      "Epoch 4999, Loss: 0.0003669837024062872\n",
      "--- Experiment 12/20 ---\n",
      "Epoch 0, Loss: 0.596327543258667\n",
      "Epoch 500, Loss: 0.009175917133688927\n",
      "Epoch 1000, Loss: 0.0048601641319692135\n",
      "Epoch 1500, Loss: 0.0018382362322881818\n",
      "Epoch 2000, Loss: 0.0010805479250848293\n",
      "Threshold reach at: 2124\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0006543862400576472\n",
      "Epoch 3000, Loss: 0.0008801575750112534\n",
      "Epoch 3500, Loss: 0.0004901924403384328\n",
      "Epoch 4000, Loss: 0.0003221275983378291\n",
      "Epoch 4500, Loss: 0.00027055860846303403\n",
      "Epoch 4999, Loss: 0.0002879667154047638\n",
      "Epoch 0, Loss: 0.5154791474342346\n",
      "Epoch 500, Loss: 0.01662912406027317\n",
      "Epoch 1000, Loss: 0.00975155457854271\n",
      "Epoch 1500, Loss: 0.006677600089460611\n",
      "Epoch 2000, Loss: 0.005807003006339073\n",
      "Epoch 2500, Loss: 0.0026633054949343204\n",
      "Epoch 3000, Loss: 0.008017215877771378\n",
      "Epoch 3500, Loss: 0.002778578782454133\n",
      "Epoch 4000, Loss: 0.0019687130115926266\n",
      "Epoch 4500, Loss: 0.0016269873594865203\n",
      "Threshold reach at: 4620\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4999, Loss: 0.0023076175712049007\n",
      "Epoch 0, Loss: 0.5132015347480774\n",
      "Epoch 500, Loss: 0.013128611259162426\n",
      "Epoch 1000, Loss: 0.0036117960698902607\n",
      "Epoch 1500, Loss: 0.0020356830209493637\n",
      "Epoch 2000, Loss: 0.001637103734537959\n",
      "Epoch 2500, Loss: 0.002056131139397621\n",
      "Epoch 3000, Loss: 0.0009667453123256564\n",
      "Epoch 3500, Loss: 0.0007959823124110699\n",
      "Epoch 4000, Loss: 0.0007449926342815161\n",
      "Epoch 4500, Loss: 0.0005873306654393673\n",
      "Epoch 4999, Loss: 0.0007331200176849961\n",
      "Epoch 0, Loss: 0.7065185308456421\n",
      "Epoch 500, Loss: 0.011419516056776047\n",
      "Epoch 1000, Loss: 0.00412595784291625\n",
      "Epoch 1500, Loss: 0.0017996912356466055\n",
      "Threshold reach at: 1910\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0009831039933487773\n",
      "Epoch 2500, Loss: 0.0010548534337431192\n",
      "Epoch 3000, Loss: 0.0007554279291070998\n",
      "Epoch 3500, Loss: 0.0006684379768557847\n",
      "Epoch 4000, Loss: 0.0005346108810044825\n",
      "Epoch 4500, Loss: 0.0004971171729266644\n",
      "Epoch 4999, Loss: 0.00041198599501512945\n",
      "--- Experiment 13/20 ---\n",
      "Epoch 0, Loss: 0.5563651919364929\n",
      "Epoch 500, Loss: 0.01053519919514656\n",
      "Epoch 1000, Loss: 0.004080210346728563\n",
      "Epoch 1500, Loss: 0.00335525325499475\n",
      "Epoch 2000, Loss: 0.002787170000374317\n",
      "Epoch 2500, Loss: 0.0016276618698611856\n",
      "Epoch 3000, Loss: 0.0015053139068186283\n",
      "Epoch 3500, Loss: 0.0015434707747772336\n",
      "Threshold reach at: 3954\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.0007197917439043522\n",
      "Epoch 4500, Loss: 0.0007119706133380532\n",
      "Epoch 4999, Loss: 0.0008654632838442922\n",
      "Epoch 0, Loss: 0.422074556350708\n",
      "Epoch 500, Loss: 0.011793501675128937\n",
      "Epoch 1000, Loss: 0.008664884604513645\n",
      "Epoch 1500, Loss: 0.0070276460610330105\n",
      "Epoch 2000, Loss: 0.006132656708359718\n",
      "Epoch 2500, Loss: 0.004482459742575884\n",
      "Epoch 3000, Loss: 0.003726276569068432\n",
      "Epoch 3500, Loss: 0.0034746122546494007\n",
      "Epoch 4000, Loss: 0.0027234102599322796\n",
      "Epoch 4500, Loss: 0.0029395835008472204\n",
      "Epoch 4999, Loss: 0.0068231625482439995\n",
      "Epoch 0, Loss: 0.7990017533302307\n",
      "Epoch 500, Loss: 0.012678279541432858\n",
      "Epoch 1000, Loss: 0.004138096701353788\n",
      "Epoch 1500, Loss: 0.0029520131647586823\n",
      "Epoch 2000, Loss: 0.001929119462147355\n",
      "Epoch 2500, Loss: 0.001494702184572816\n",
      "Epoch 3000, Loss: 0.0011214460246264935\n",
      "Epoch 3500, Loss: 0.0009434686508029699\n",
      "Epoch 4000, Loss: 0.0008721818448975682\n",
      "Epoch 4500, Loss: 0.0007352853426709771\n",
      "Epoch 4999, Loss: 0.0005568363703787327\n",
      "Epoch 0, Loss: 0.3433036506175995\n",
      "Epoch 500, Loss: 0.008124063722789288\n",
      "Epoch 1000, Loss: 0.003966336604207754\n",
      "Epoch 1500, Loss: 0.002977907657623291\n",
      "Epoch 2000, Loss: 0.0017567190807312727\n",
      "Epoch 2500, Loss: 0.0014985103625804186\n",
      "Threshold reach at: 2961\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0012597767636179924\n",
      "Epoch 3500, Loss: 0.0009305624989792705\n",
      "Epoch 4000, Loss: 0.0010690289782360196\n",
      "Epoch 4500, Loss: 0.0008683246560394764\n",
      "Epoch 4999, Loss: 0.000284398440271616\n",
      "--- Experiment 14/20 ---\n",
      "Epoch 0, Loss: 0.6937570571899414\n",
      "Epoch 500, Loss: 0.009718816727399826\n",
      "Epoch 1000, Loss: 0.005267411004751921\n",
      "Epoch 1500, Loss: 0.0025554150342941284\n",
      "Epoch 2000, Loss: 0.0016720800194889307\n",
      "Epoch 2500, Loss: 0.0012851818464696407\n",
      "Threshold reach at: 2798\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3000, Loss: 0.0007069503189995885\n",
      "Epoch 3500, Loss: 0.0006343995337374508\n",
      "Epoch 4000, Loss: 0.0007038076873868704\n",
      "Epoch 4500, Loss: 0.0004266622709110379\n",
      "Epoch 4999, Loss: 0.0005300066550262272\n",
      "Epoch 0, Loss: 0.6087229251861572\n",
      "Epoch 500, Loss: 0.021274464204907417\n",
      "Epoch 1000, Loss: 0.013797057792544365\n",
      "Epoch 1500, Loss: 0.009183636866509914\n",
      "Epoch 2000, Loss: 0.006590480916202068\n",
      "Epoch 2500, Loss: 0.006490974687039852\n",
      "Epoch 3000, Loss: 0.0049540274776518345\n",
      "Epoch 3500, Loss: 0.006599266082048416\n",
      "Epoch 4000, Loss: 0.0029395094607025385\n",
      "Epoch 4500, Loss: 0.003987750969827175\n",
      "Epoch 4999, Loss: 0.0033210283145308495\n",
      "Epoch 0, Loss: 0.4822975695133209\n",
      "Epoch 500, Loss: 0.012669185176491737\n",
      "Epoch 1000, Loss: 0.006357562728226185\n",
      "Epoch 1500, Loss: 0.0039666639640927315\n",
      "Epoch 2000, Loss: 0.0036095648538321257\n",
      "Epoch 2500, Loss: 0.002812790684401989\n",
      "Epoch 3000, Loss: 0.0022294679656624794\n",
      "Epoch 3500, Loss: 0.002254708670079708\n",
      "Epoch 4000, Loss: 0.001913284184411168\n",
      "Epoch 4500, Loss: 0.0017822308000177145\n",
      "Epoch 4999, Loss: 0.0013495670864358544\n",
      "Epoch 0, Loss: 0.34439003467559814\n",
      "Epoch 500, Loss: 0.006200002506375313\n",
      "Epoch 1000, Loss: 0.003853204194456339\n",
      "Epoch 1500, Loss: 0.002319671679288149\n",
      "Epoch 2000, Loss: 0.0015466728946194053\n",
      "Threshold reach at: 2121\n",
      "Val loss: tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0009176484309136868\n",
      "Epoch 3000, Loss: 0.0008498568786308169\n",
      "Epoch 3500, Loss: 0.000632873794529587\n",
      "Epoch 4000, Loss: 0.0005077722016721964\n",
      "Epoch 4500, Loss: 0.00029318491579033434\n",
      "Epoch 4999, Loss: 0.0004464492085389793\n",
      "--- Experiment 15/20 ---\n",
      "Epoch 0, Loss: 0.2804163098335266\n",
      "Epoch 500, Loss: 0.009704303927719593\n",
      "Epoch 1000, Loss: 0.00454192329198122\n",
      "Epoch 1500, Loss: 0.003069227794185281\n",
      "Epoch 2000, Loss: 0.0021325345151126385\n",
      "Epoch 2500, Loss: 0.00161750300321728\n",
      "Epoch 3000, Loss: 0.0019440397154539824\n",
      "Epoch 3500, Loss: 0.001150147756561637\n",
      "Epoch 4000, Loss: 0.0007441064808517694\n",
      "Threshold reach at: 4036\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.0011410661973059177\n",
      "Epoch 4999, Loss: 0.0009460346191190183\n",
      "Epoch 0, Loss: 0.4135012924671173\n",
      "Epoch 500, Loss: 0.01556447334587574\n",
      "Epoch 1000, Loss: 0.01076576765626669\n",
      "Epoch 1500, Loss: 0.0068201711401343346\n",
      "Epoch 2000, Loss: 0.007751150988042355\n",
      "Epoch 2500, Loss: 0.005931330844759941\n",
      "Epoch 3000, Loss: 0.003958245739340782\n",
      "Epoch 3500, Loss: 0.002964672399684787\n",
      "Epoch 4000, Loss: 0.0026036060880869627\n",
      "Epoch 4500, Loss: 0.0031305949669331312\n",
      "Epoch 4999, Loss: 0.0027050632052123547\n",
      "Epoch 0, Loss: 0.5566606521606445\n",
      "Epoch 500, Loss: 0.011723088100552559\n",
      "Epoch 1000, Loss: 0.003907819278538227\n",
      "Epoch 1500, Loss: 0.001301830168813467\n",
      "Epoch 2000, Loss: 0.0007942371885292232\n",
      "Epoch 2500, Loss: 0.0005623394390568137\n",
      "Epoch 3000, Loss: 0.0004935495089739561\n",
      "Epoch 3500, Loss: 0.00040780677227303386\n",
      "Epoch 4000, Loss: 0.0003047700156457722\n",
      "Epoch 4500, Loss: 0.00030283385422080755\n",
      "Epoch 4999, Loss: 0.00031254751957021654\n",
      "Epoch 0, Loss: 0.5034363269805908\n",
      "Epoch 500, Loss: 0.011023858562111855\n",
      "Epoch 1000, Loss: 0.0038140802644193172\n",
      "Epoch 1500, Loss: 0.0023038696963340044\n",
      "Epoch 2000, Loss: 0.0017448719590902328\n",
      "Threshold reach at: 2414\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0012019556015729904\n",
      "Epoch 3000, Loss: 0.001127699390053749\n",
      "Epoch 3500, Loss: 0.0006760888500139117\n",
      "Epoch 4000, Loss: 0.0006184802623465657\n",
      "Epoch 4500, Loss: 0.0005480939289554954\n",
      "Epoch 4999, Loss: 0.0004787354846484959\n",
      "--- Experiment 16/20 ---\n",
      "Epoch 0, Loss: 0.6310513019561768\n",
      "Epoch 500, Loss: 0.009142402559518814\n",
      "Epoch 1000, Loss: 0.0036128144711256027\n",
      "Epoch 1500, Loss: 0.002257533138617873\n",
      "Epoch 2000, Loss: 0.0019078012555837631\n",
      "Epoch 2500, Loss: 0.0010506708640605211\n",
      "Epoch 3000, Loss: 0.0012175504816696048\n",
      "Threshold reach at: 3185\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0007855640724301338\n",
      "Epoch 4000, Loss: 0.0008288405369967222\n",
      "Epoch 4500, Loss: 0.0005305837257765234\n",
      "Epoch 4999, Loss: 0.0007686652825213969\n",
      "Epoch 0, Loss: 0.4928104281425476\n",
      "Epoch 500, Loss: 0.020297186449170113\n",
      "Epoch 1000, Loss: 0.009193622507154942\n",
      "Epoch 1500, Loss: 0.005419726017862558\n",
      "Epoch 2000, Loss: 0.0036311112344264984\n",
      "Epoch 2500, Loss: 0.002280943561345339\n",
      "Epoch 3000, Loss: 0.0038039805367588997\n",
      "Epoch 3500, Loss: 0.0026256153360009193\n",
      "Threshold reach at: 3941\n",
      "Val loss: tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.002028925344347954\n",
      "Epoch 4500, Loss: 0.001258254749700427\n",
      "Epoch 4999, Loss: 0.0012682470260187984\n",
      "Epoch 0, Loss: 0.6025598645210266\n",
      "Epoch 500, Loss: 0.012709630653262138\n",
      "Epoch 1000, Loss: 0.004641525447368622\n",
      "Epoch 1500, Loss: 0.0023186756297945976\n",
      "Epoch 2000, Loss: 0.0015035744290798903\n",
      "Epoch 2500, Loss: 0.0012022266164422035\n",
      "Epoch 3000, Loss: 0.0010290222708135843\n",
      "Epoch 3500, Loss: 0.000808678800240159\n",
      "Epoch 4000, Loss: 0.0007398112211376429\n",
      "Epoch 4500, Loss: 0.00048126294859685004\n",
      "Epoch 4999, Loss: 0.0005995745304971933\n",
      "Epoch 0, Loss: 0.7823348045349121\n",
      "Epoch 500, Loss: 0.01571674272418022\n",
      "Epoch 1000, Loss: 0.005844587925821543\n",
      "Epoch 1500, Loss: 0.0026902901008725166\n",
      "Epoch 2000, Loss: 0.0025063734501600266\n",
      "Epoch 2500, Loss: 0.001693749101832509\n",
      "Epoch 3000, Loss: 0.0013951800065115094\n",
      "Threshold reach at: 3087\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0009408831829205155\n",
      "Epoch 4000, Loss: 0.0009385168086737394\n",
      "Epoch 4500, Loss: 0.0008552317740395665\n",
      "Epoch 4999, Loss: 0.0005302808131091297\n",
      "--- Experiment 17/20 ---\n",
      "Epoch 0, Loss: 0.552113950252533\n",
      "Epoch 500, Loss: 0.010028954595327377\n",
      "Epoch 1000, Loss: 0.005909718107432127\n",
      "Epoch 1500, Loss: 0.0039847856387495995\n",
      "Epoch 2000, Loss: 0.0025195449125021696\n",
      "Epoch 2500, Loss: 0.0022649571765214205\n",
      "Epoch 3000, Loss: 0.0021521300077438354\n",
      "Epoch 3500, Loss: 0.00200820155441761\n",
      "Epoch 4000, Loss: 0.0016757308039814234\n",
      "Threshold reach at: 4420\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4500, Loss: 0.001399168511852622\n",
      "Epoch 4999, Loss: 0.0009485009359195828\n",
      "Epoch 0, Loss: 0.6414960026741028\n",
      "Epoch 500, Loss: 0.020023053511977196\n",
      "Epoch 1000, Loss: 0.011795051395893097\n",
      "Epoch 1500, Loss: 0.007202677428722382\n",
      "Epoch 2000, Loss: 0.005486198700964451\n",
      "Epoch 2500, Loss: 0.0038811201229691505\n",
      "Epoch 3000, Loss: 0.003437418956309557\n",
      "Epoch 3500, Loss: 0.00407838448882103\n",
      "Threshold reach at: 3983\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.0019295710371807218\n",
      "Epoch 4500, Loss: 0.0027664911467581987\n",
      "Epoch 4999, Loss: 0.002398709300905466\n",
      "Epoch 0, Loss: 0.606951892375946\n",
      "Epoch 500, Loss: 0.013898786157369614\n",
      "Epoch 1000, Loss: 0.004982850980013609\n",
      "Epoch 1500, Loss: 0.002013738267123699\n",
      "Epoch 2000, Loss: 0.0013954020105302334\n",
      "Epoch 2500, Loss: 0.0010258586844429374\n",
      "Epoch 3000, Loss: 0.000946291780564934\n",
      "Epoch 3500, Loss: 0.0007067668484523892\n",
      "Epoch 4000, Loss: 0.000574182893615216\n",
      "Epoch 4500, Loss: 0.0004859737819060683\n",
      "Epoch 4999, Loss: 0.0004279852728359401\n",
      "Epoch 0, Loss: 0.2744189500808716\n",
      "Epoch 500, Loss: 0.008343356661498547\n",
      "Epoch 1000, Loss: 0.0023379665799438953\n",
      "Epoch 1500, Loss: 0.001273544505238533\n",
      "Threshold reach at: 1604\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0009327403968200088\n",
      "Epoch 2500, Loss: 0.0009514165576547384\n",
      "Epoch 3000, Loss: 0.0007291153306141496\n",
      "Epoch 3500, Loss: 0.0005461354739964008\n",
      "Epoch 4000, Loss: 0.0004859577165916562\n",
      "Epoch 4500, Loss: 0.0004886926617473364\n",
      "Epoch 4999, Loss: 0.00045744614908471704\n",
      "--- Experiment 18/20 ---\n",
      "Epoch 0, Loss: 0.4177154302597046\n",
      "Epoch 500, Loss: 0.007125113159418106\n",
      "Epoch 1000, Loss: 0.0040370505303144455\n",
      "Epoch 1500, Loss: 0.002726220991462469\n",
      "Epoch 2000, Loss: 0.0022923648357391357\n",
      "Epoch 2500, Loss: 0.0019356488483026624\n",
      "Epoch 3000, Loss: 0.0011630789376795292\n",
      "Epoch 3500, Loss: 0.0015440117567777634\n",
      "Threshold reach at: 3933\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.0012691197916865349\n",
      "Epoch 4500, Loss: 0.0009688433492556214\n",
      "Epoch 4999, Loss: 0.001006805570796132\n",
      "Epoch 0, Loss: 0.3660498261451721\n",
      "Epoch 500, Loss: 0.01945796236395836\n",
      "Epoch 1000, Loss: 0.010548675432801247\n",
      "Epoch 1500, Loss: 0.006177063100039959\n",
      "Epoch 2000, Loss: 0.00512467697262764\n",
      "Epoch 2500, Loss: 0.0036895032972097397\n",
      "Epoch 3000, Loss: 0.002354763215407729\n",
      "Epoch 3500, Loss: 0.0027130157686769962\n",
      "Threshold reach at: 3902\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.0015861227875575423\n",
      "Epoch 4500, Loss: 0.0014216823037713766\n",
      "Epoch 4999, Loss: 0.0017834326718002558\n",
      "Epoch 0, Loss: 0.539580225944519\n",
      "Epoch 500, Loss: 0.016444649547338486\n",
      "Epoch 1000, Loss: 0.009491438046097755\n",
      "Epoch 1500, Loss: 0.0035520726814866066\n",
      "Epoch 2000, Loss: 0.0025337955448776484\n",
      "Epoch 2500, Loss: 0.00192960724234581\n",
      "Epoch 3000, Loss: 0.001249412540346384\n",
      "Epoch 3500, Loss: 0.001479587983340025\n",
      "Epoch 4000, Loss: 0.0013979831710457802\n",
      "Epoch 4500, Loss: 0.001218895660713315\n",
      "Epoch 4999, Loss: 0.0009949100203812122\n",
      "Epoch 0, Loss: 0.41904938220977783\n",
      "Epoch 500, Loss: 0.00782005675137043\n",
      "Epoch 1000, Loss: 0.0021745108533650637\n",
      "Threshold reach at: 1299\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 1500, Loss: 0.0009566650842316449\n",
      "Epoch 2000, Loss: 0.0006516677676700056\n",
      "Epoch 2500, Loss: 0.0005128594348207116\n",
      "Epoch 3000, Loss: 0.0003908192738890648\n",
      "Epoch 3500, Loss: 0.0003415081591811031\n",
      "Epoch 4000, Loss: 0.00025818427093327045\n",
      "Epoch 4500, Loss: 0.0002306100504938513\n",
      "Epoch 4999, Loss: 0.00015412361244671047\n",
      "--- Experiment 19/20 ---\n",
      "Epoch 0, Loss: 0.45184755325317383\n",
      "Epoch 500, Loss: 0.007233792915940285\n",
      "Epoch 1000, Loss: 0.0025224597193300724\n",
      "Epoch 1500, Loss: 0.0013362226309254766\n",
      "Threshold reach at: 1554\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.000611231429502368\n",
      "Epoch 2500, Loss: 0.0005375022301450372\n",
      "Epoch 3000, Loss: 0.000416921975556761\n",
      "Epoch 3500, Loss: 0.00047174771316349506\n",
      "Epoch 4000, Loss: 0.0004635556833818555\n",
      "Epoch 4500, Loss: 0.0003525682259351015\n",
      "Epoch 4999, Loss: 0.0004598839732352644\n",
      "Epoch 0, Loss: 0.44713491201400757\n",
      "Epoch 500, Loss: 0.01973896101117134\n",
      "Epoch 1000, Loss: 0.011009386740624905\n",
      "Epoch 1500, Loss: 0.0069588422775268555\n",
      "Epoch 2000, Loss: 0.00382847897708416\n",
      "Epoch 2500, Loss: 0.00525110075250268\n",
      "Epoch 3000, Loss: 0.002308475784957409\n",
      "Epoch 3500, Loss: 0.007686718367040157\n",
      "Threshold reach at: 3666\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4000, Loss: 0.0017562066204845905\n",
      "Epoch 4500, Loss: 0.002282647415995598\n",
      "Epoch 4999, Loss: 0.0017429536674171686\n",
      "Epoch 0, Loss: 0.35356515645980835\n",
      "Epoch 500, Loss: 0.009467882104218006\n",
      "Epoch 1000, Loss: 0.0034472886472940445\n",
      "Epoch 1500, Loss: 0.001537468982860446\n",
      "Epoch 2000, Loss: 0.001127217197790742\n",
      "Epoch 2500, Loss: 0.000771005405113101\n",
      "Epoch 3000, Loss: 0.0005601541488431394\n",
      "Epoch 3500, Loss: 0.000548525364138186\n",
      "Epoch 4000, Loss: 0.00045778232743032277\n",
      "Epoch 4500, Loss: 0.00036581113818101585\n",
      "Epoch 4999, Loss: 0.00038610337651334703\n",
      "Epoch 0, Loss: 0.4363292455673218\n",
      "Epoch 500, Loss: 0.009332044050097466\n",
      "Epoch 1000, Loss: 0.003381823655217886\n",
      "Epoch 1500, Loss: 0.0020682169124484062\n",
      "Epoch 2000, Loss: 0.001665240852162242\n",
      "Threshold reach at: 2114\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2500, Loss: 0.0012654457241296768\n",
      "Epoch 3000, Loss: 0.0006420499412342906\n",
      "Epoch 3500, Loss: 0.0007657631067559123\n",
      "Epoch 4000, Loss: 0.0005407014396041632\n",
      "Epoch 4500, Loss: 0.0006273937760852277\n",
      "Epoch 4999, Loss: 0.0004654279036913067\n",
      "--- Experiment 20/20 ---\n",
      "Epoch 0, Loss: 0.5796652436256409\n",
      "Epoch 500, Loss: 0.01011127419769764\n",
      "Epoch 1000, Loss: 0.005619797855615616\n",
      "Epoch 1500, Loss: 0.0021260492503643036\n",
      "Epoch 2000, Loss: 0.0016583448741585016\n",
      "Epoch 2500, Loss: 0.0015753970947116613\n",
      "Epoch 3000, Loss: 0.0010994498152285814\n",
      "Threshold reach at: 3061\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3500, Loss: 0.0009835798991844058\n",
      "Epoch 4000, Loss: 0.0007394254789687693\n",
      "Epoch 4500, Loss: 0.0008537641260772943\n",
      "Epoch 4999, Loss: 0.0005462695262394845\n",
      "Epoch 0, Loss: 0.710563063621521\n",
      "Epoch 500, Loss: 0.01906767301261425\n",
      "Epoch 1000, Loss: 0.009467079304158688\n",
      "Epoch 1500, Loss: 0.006391999311745167\n",
      "Epoch 2000, Loss: 0.005008190870285034\n",
      "Epoch 2500, Loss: 0.004524529445916414\n",
      "Epoch 3000, Loss: 0.004114513751119375\n",
      "Epoch 3500, Loss: 0.003598146140575409\n",
      "Epoch 4000, Loss: 0.004402593243867159\n",
      "Epoch 4500, Loss: 0.0035591903142631054\n",
      "Epoch 4999, Loss: 0.0023304393980652094\n",
      "Epoch 0, Loss: 0.516684889793396\n",
      "Epoch 500, Loss: 0.010590581223368645\n",
      "Epoch 1000, Loss: 0.0034647968132048845\n",
      "Epoch 1500, Loss: 0.0023652834352105856\n",
      "Epoch 2000, Loss: 0.0017201376613229513\n",
      "Epoch 2500, Loss: 0.001480713253840804\n",
      "Epoch 3000, Loss: 0.0013558929786086082\n",
      "Epoch 3500, Loss: 0.0010270837228745222\n",
      "Epoch 4000, Loss: 0.0007538814097642899\n",
      "Epoch 4500, Loss: 0.0008141273865476251\n",
      "Epoch 4999, Loss: 0.000799601199105382\n",
      "Epoch 0, Loss: 0.3707886338233948\n",
      "Epoch 500, Loss: 0.009755188599228859\n",
      "Epoch 1000, Loss: 0.0023507399018853903\n",
      "Epoch 1500, Loss: 0.0011334707960486412\n",
      "Threshold reach at: 1811\n",
      "Val loss: tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2000, Loss: 0.0008156037656590343\n",
      "Epoch 2500, Loss: 0.0007225144072435796\n",
      "Epoch 3000, Loss: 0.0005796271725557745\n",
      "Epoch 3500, Loss: 0.0005797803169116378\n",
      "Epoch 4000, Loss: 0.0005324225639924407\n",
      "Epoch 4500, Loss: 0.000373801973182708\n",
      "Epoch 4999, Loss: 0.0003082724870182574\n",
      "Average Validation Loss (Base):   0.000660 ± 0.000197\n",
      "Average Validation Loss (Import): 0.001437 ± 0.000415\n",
      "Average Validation Loss (GP Out): 0.026916 ± 0.020239\n",
      "Average Validation Loss (GP Res): 0.000373 ± 0.000090\n",
      "\n",
      "==== Running experiments for IC: step, BC: dirichlet ====\n",
      "--- Experiment 1/20 ---\n",
      "Epoch 0, Loss: 0.5362976789474487\n",
      "Epoch 500, Loss: 0.06251489371061325\n",
      "Epoch 1000, Loss: 0.054205115884542465\n",
      "Epoch 1500, Loss: 0.0520222932100296\n",
      "Epoch 2000, Loss: 0.05114535242319107\n",
      "Epoch 2500, Loss: 0.0476958304643631\n",
      "Epoch 3000, Loss: 0.052602097392082214\n",
      "Epoch 3500, Loss: 0.04933054745197296\n",
      "Epoch 4000, Loss: 0.0516337975859642\n",
      "Epoch 4500, Loss: 0.05016282945871353\n",
      "Epoch 4999, Loss: 0.048567645251750946\n",
      "Epoch 0, Loss: 0.3855496048927307\n",
      "Epoch 500, Loss: 0.08245746046304703\n",
      "Epoch 1000, Loss: 0.08308254182338715\n",
      "Epoch 1500, Loss: 0.08161766082048416\n",
      "Epoch 2000, Loss: 0.07702663540840149\n",
      "Epoch 2500, Loss: 0.07566972821950912\n",
      "Epoch 3000, Loss: 0.07387208938598633\n",
      "Epoch 3500, Loss: 0.0756172239780426\n",
      "Epoch 4000, Loss: 0.07474907487630844\n",
      "Epoch 4500, Loss: 0.07331512123346329\n",
      "Epoch 4999, Loss: 0.07624226808547974\n",
      "Epoch 0, Loss: 0.4189120829105377\n",
      "Epoch 500, Loss: 0.07828108966350555\n",
      "Epoch 1000, Loss: 0.06774340569972992\n",
      "Epoch 1500, Loss: 0.06535068899393082\n",
      "Epoch 2000, Loss: 0.06891465932130814\n",
      "Epoch 2500, Loss: 0.06632291525602341\n",
      "Epoch 3000, Loss: 0.06392904371023178\n",
      "Epoch 3500, Loss: 0.06215820834040642\n",
      "Epoch 4000, Loss: 0.06470116227865219\n",
      "Epoch 4500, Loss: 0.06213773041963577\n",
      "Epoch 4999, Loss: 0.06417800486087799\n",
      "Epoch 0, Loss: 0.4189631938934326\n",
      "Epoch 500, Loss: 0.0656074732542038\n",
      "Epoch 1000, Loss: 0.06343904882669449\n",
      "Epoch 1500, Loss: 0.060033999383449554\n",
      "Epoch 2000, Loss: 0.05982457101345062\n",
      "Epoch 2500, Loss: 0.059332847595214844\n",
      "Epoch 3000, Loss: 0.06130019575357437\n",
      "Epoch 3500, Loss: 0.05694356560707092\n",
      "Epoch 4000, Loss: 0.056484293192625046\n",
      "Epoch 4500, Loss: 0.05725347250699997\n",
      "Epoch 4999, Loss: 0.05790761113166809\n",
      "--- Experiment 2/20 ---\n",
      "Epoch 0, Loss: 0.5700357556343079\n",
      "Epoch 500, Loss: 0.05791156738996506\n",
      "Epoch 1000, Loss: 0.05496928468346596\n",
      "Epoch 1500, Loss: 0.05256025493144989\n",
      "Epoch 2000, Loss: 0.049963463097810745\n",
      "Epoch 2500, Loss: 0.05151093006134033\n",
      "Epoch 3000, Loss: 0.049692898988723755\n",
      "Epoch 3500, Loss: 0.049810588359832764\n",
      "Epoch 4000, Loss: 0.049416325986385345\n",
      "Epoch 4500, Loss: 0.0468406043946743\n",
      "Epoch 4999, Loss: 0.05216728895902634\n",
      "Epoch 0, Loss: 0.4543840289115906\n",
      "Epoch 500, Loss: 0.0891047790646553\n",
      "Epoch 1000, Loss: 0.07992085069417953\n",
      "Epoch 1500, Loss: 0.07888440787792206\n",
      "Epoch 2000, Loss: 0.07587212324142456\n",
      "Epoch 2500, Loss: 0.07475633919239044\n",
      "Epoch 3000, Loss: 0.07578976452350616\n",
      "Epoch 3500, Loss: 0.07451348006725311\n",
      "Epoch 4000, Loss: 0.0730762779712677\n",
      "Epoch 4500, Loss: 0.0718027651309967\n",
      "Epoch 4999, Loss: 0.07157013565301895\n",
      "Epoch 0, Loss: 0.42363783717155457\n",
      "Epoch 500, Loss: 0.07500427216291428\n",
      "Epoch 1000, Loss: 0.06855980306863785\n",
      "Epoch 1500, Loss: 0.06430178880691528\n",
      "Epoch 2000, Loss: 0.06341706961393356\n",
      "Epoch 2500, Loss: 0.06387712806463242\n",
      "Epoch 3000, Loss: 0.06937635689973831\n",
      "Epoch 3500, Loss: 0.06205218657851219\n",
      "Epoch 4000, Loss: 0.0627635046839714\n",
      "Epoch 4500, Loss: 0.06154820695519447\n",
      "Epoch 4999, Loss: 0.061069127172231674\n",
      "Epoch 0, Loss: 0.5577471256256104\n",
      "Epoch 500, Loss: 0.06421016901731491\n",
      "Epoch 1000, Loss: 0.06280580908060074\n",
      "Epoch 1500, Loss: 0.061749283224344254\n",
      "Epoch 2000, Loss: 0.05947394669055939\n",
      "Epoch 2500, Loss: 0.05869007110595703\n",
      "Epoch 3000, Loss: 0.06255683302879333\n",
      "Epoch 3500, Loss: 0.053604669868946075\n",
      "Epoch 4000, Loss: 0.055339884012937546\n",
      "Epoch 4500, Loss: 0.06564123928546906\n",
      "Epoch 4999, Loss: 0.06076420843601227\n",
      "--- Experiment 3/20 ---\n",
      "Epoch 0, Loss: 0.49645158648490906\n",
      "Epoch 500, Loss: 0.05774485319852829\n",
      "Epoch 1000, Loss: 0.055024653673172\n",
      "Epoch 1500, Loss: 0.049660224467515945\n",
      "Epoch 2000, Loss: 0.04976784437894821\n",
      "Epoch 2500, Loss: 0.05402536317706108\n",
      "Epoch 3000, Loss: 0.05027817189693451\n",
      "Epoch 3500, Loss: 0.04976639896631241\n",
      "Epoch 4000, Loss: 0.04893649369478226\n",
      "Epoch 4500, Loss: 0.04925018548965454\n",
      "Epoch 4999, Loss: 0.05129854381084442\n",
      "Epoch 0, Loss: 0.6396909356117249\n",
      "Epoch 500, Loss: 0.09049491584300995\n",
      "Epoch 1000, Loss: 0.08252522349357605\n",
      "Epoch 1500, Loss: 0.08000035583972931\n",
      "Epoch 2000, Loss: 0.07997673749923706\n",
      "Epoch 2500, Loss: 0.08343755453824997\n",
      "Epoch 3000, Loss: 0.07638470828533173\n",
      "Epoch 3500, Loss: 0.07507269084453583\n",
      "Epoch 4000, Loss: 0.07400166988372803\n",
      "Epoch 4500, Loss: 0.07554058730602264\n",
      "Epoch 4999, Loss: 0.07198828458786011\n",
      "Epoch 0, Loss: 0.5840042233467102\n",
      "Epoch 500, Loss: 0.07637134194374084\n",
      "Epoch 1000, Loss: 0.07040812075138092\n",
      "Epoch 1500, Loss: 0.06688832491636276\n",
      "Epoch 2000, Loss: 0.06667757779359818\n",
      "Epoch 2500, Loss: 0.061358414590358734\n",
      "Epoch 3000, Loss: 0.06352295726537704\n",
      "Epoch 3500, Loss: 0.06681977957487106\n",
      "Epoch 4000, Loss: 0.06281477212905884\n",
      "Epoch 4500, Loss: 0.06242949515581131\n",
      "Epoch 4999, Loss: 0.06413232535123825\n",
      "Epoch 0, Loss: 0.4730093479156494\n",
      "Epoch 500, Loss: 0.06757866591215134\n",
      "Epoch 1000, Loss: 0.060873840004205704\n",
      "Epoch 1500, Loss: 0.057718098163604736\n",
      "Epoch 2000, Loss: 0.062135107815265656\n",
      "Epoch 2500, Loss: 0.05971626937389374\n",
      "Epoch 3000, Loss: 0.05785338580608368\n",
      "Epoch 3500, Loss: 0.05727832391858101\n",
      "Epoch 4000, Loss: 0.055421750992536545\n",
      "Epoch 4500, Loss: 0.06144566088914871\n",
      "Epoch 4999, Loss: 0.05752892047166824\n",
      "--- Experiment 4/20 ---\n",
      "Epoch 0, Loss: 0.511296808719635\n",
      "Epoch 500, Loss: 0.05459681525826454\n",
      "Epoch 1000, Loss: 0.05430011451244354\n",
      "Epoch 1500, Loss: 0.052602291107177734\n",
      "Epoch 2000, Loss: 0.05655597522854805\n",
      "Epoch 2500, Loss: 0.05304362624883652\n",
      "Epoch 3000, Loss: 0.04852462559938431\n",
      "Epoch 3500, Loss: 0.04922173172235489\n",
      "Epoch 4000, Loss: 0.04767134040594101\n",
      "Epoch 4500, Loss: 0.04812592640519142\n",
      "Epoch 4999, Loss: 0.0505714938044548\n",
      "Epoch 0, Loss: 0.5266855955123901\n",
      "Epoch 500, Loss: 0.08905090391635895\n",
      "Epoch 1000, Loss: 0.08565863966941833\n",
      "Epoch 1500, Loss: 0.07775849848985672\n",
      "Epoch 2000, Loss: 0.07700377702713013\n",
      "Epoch 2500, Loss: 0.08211411535739899\n",
      "Epoch 3000, Loss: 0.07507643848657608\n",
      "Epoch 3500, Loss: 0.07279525697231293\n",
      "Epoch 4000, Loss: 0.07383570820093155\n",
      "Epoch 4500, Loss: 0.07296106964349747\n",
      "Epoch 4999, Loss: 0.07209383696317673\n",
      "Epoch 0, Loss: 0.7480672597885132\n",
      "Epoch 500, Loss: 0.0782092809677124\n",
      "Epoch 1000, Loss: 0.06827002018690109\n",
      "Epoch 1500, Loss: 0.06790313124656677\n",
      "Epoch 2000, Loss: 0.06306321918964386\n",
      "Epoch 2500, Loss: 0.06719560921192169\n",
      "Epoch 3000, Loss: 0.06465855985879898\n",
      "Epoch 3500, Loss: 0.06162259727716446\n",
      "Epoch 4000, Loss: 0.06287720054388046\n",
      "Epoch 4500, Loss: 0.0614287331700325\n",
      "Epoch 4999, Loss: 0.06466171890497208\n",
      "Epoch 0, Loss: 0.41579189896583557\n",
      "Epoch 500, Loss: 0.06362167745828629\n",
      "Epoch 1000, Loss: 0.06395982205867767\n",
      "Epoch 1500, Loss: 0.06419452279806137\n",
      "Epoch 2000, Loss: 0.06108377128839493\n",
      "Epoch 2500, Loss: 0.06023489683866501\n",
      "Epoch 3000, Loss: 0.059395402669906616\n",
      "Epoch 3500, Loss: 0.054896511137485504\n",
      "Epoch 4000, Loss: 0.059272103011608124\n",
      "Epoch 4500, Loss: 0.05741283297538757\n",
      "Epoch 4999, Loss: 0.05884146690368652\n",
      "--- Experiment 5/20 ---\n",
      "Epoch 0, Loss: 0.3276650309562683\n",
      "Epoch 500, Loss: 0.057809244841337204\n",
      "Epoch 1000, Loss: 0.05297304689884186\n",
      "Epoch 1500, Loss: 0.05328592658042908\n",
      "Epoch 2000, Loss: 0.05314537137746811\n",
      "Epoch 2500, Loss: 0.050125494599342346\n",
      "Epoch 3000, Loss: 0.04854521527886391\n",
      "Epoch 3500, Loss: 0.05183606594800949\n",
      "Epoch 4000, Loss: 0.04865237697958946\n",
      "Epoch 4500, Loss: 0.05123842507600784\n",
      "Epoch 4999, Loss: 0.04822869598865509\n",
      "Epoch 0, Loss: 0.5299063324928284\n",
      "Epoch 500, Loss: 0.08597654849290848\n",
      "Epoch 1000, Loss: 0.08302271366119385\n",
      "Epoch 1500, Loss: 0.08164546638727188\n",
      "Epoch 2000, Loss: 0.08059526979923248\n",
      "Epoch 2500, Loss: 0.07667423039674759\n",
      "Epoch 3000, Loss: 0.0748126208782196\n",
      "Epoch 3500, Loss: 0.07378759235143661\n",
      "Epoch 4000, Loss: 0.07516005635261536\n",
      "Epoch 4500, Loss: 0.07260425388813019\n",
      "Epoch 4999, Loss: 0.07223011553287506\n",
      "Epoch 0, Loss: 0.6282170414924622\n",
      "Epoch 500, Loss: 0.07844938337802887\n",
      "Epoch 1000, Loss: 0.0693645179271698\n",
      "Epoch 1500, Loss: 0.06705640256404877\n",
      "Epoch 2000, Loss: 0.06385727971792221\n",
      "Epoch 2500, Loss: 0.0631236657500267\n",
      "Epoch 3000, Loss: 0.06353314220905304\n",
      "Epoch 3500, Loss: 0.06278922408819199\n",
      "Epoch 4000, Loss: 0.062299031764268875\n",
      "Epoch 4500, Loss: 0.06401532888412476\n",
      "Epoch 4999, Loss: 0.06195438653230667\n",
      "Epoch 0, Loss: 0.5814831256866455\n",
      "Epoch 500, Loss: 0.06828436255455017\n",
      "Epoch 1000, Loss: 0.0604066401720047\n",
      "Epoch 1500, Loss: 0.06318452209234238\n",
      "Epoch 2000, Loss: 0.055656757205724716\n",
      "Epoch 2500, Loss: 0.050553832203149796\n",
      "Epoch 3000, Loss: 0.060182228684425354\n",
      "Epoch 3500, Loss: 0.055994488298892975\n",
      "Epoch 4000, Loss: 0.05900401622056961\n",
      "Epoch 4500, Loss: 0.05718189850449562\n",
      "Epoch 4999, Loss: 0.06114416569471359\n",
      "--- Experiment 6/20 ---\n",
      "Epoch 0, Loss: 0.32393378019332886\n",
      "Epoch 500, Loss: 0.058909349143505096\n",
      "Epoch 1000, Loss: 0.05287591740489006\n",
      "Epoch 1500, Loss: 0.05309400334954262\n",
      "Epoch 2000, Loss: 0.05202839523553848\n",
      "Epoch 2500, Loss: 0.050847090780735016\n",
      "Epoch 3000, Loss: 0.053119391202926636\n",
      "Epoch 3500, Loss: 0.04971248656511307\n",
      "Epoch 4000, Loss: 0.05020442605018616\n",
      "Epoch 4500, Loss: 0.051431767642498016\n",
      "Epoch 4999, Loss: 0.051464907824993134\n",
      "Epoch 0, Loss: 0.6029435396194458\n",
      "Epoch 500, Loss: 0.08639181405305862\n",
      "Epoch 1000, Loss: 0.08096526563167572\n",
      "Epoch 1500, Loss: 0.07953676581382751\n",
      "Epoch 2000, Loss: 0.07826634496450424\n",
      "Epoch 2500, Loss: 0.07267606258392334\n",
      "Epoch 3000, Loss: 0.07699582725763321\n",
      "Epoch 3500, Loss: 0.07582710683345795\n",
      "Epoch 4000, Loss: 0.07852233946323395\n",
      "Epoch 4500, Loss: 0.07224827259778976\n",
      "Epoch 4999, Loss: 0.07553787529468536\n",
      "Epoch 0, Loss: 0.4237038493156433\n",
      "Epoch 500, Loss: 0.07612155377864838\n",
      "Epoch 1000, Loss: 0.06726706773042679\n",
      "Epoch 1500, Loss: 0.06276968121528625\n",
      "Epoch 2000, Loss: 0.06556543707847595\n",
      "Epoch 2500, Loss: 0.0643668919801712\n",
      "Epoch 3000, Loss: 0.06300900876522064\n",
      "Epoch 3500, Loss: 0.06177989020943642\n",
      "Epoch 4000, Loss: 0.06189741939306259\n",
      "Epoch 4500, Loss: 0.05439804494380951\n",
      "Epoch 4999, Loss: 0.06681811809539795\n",
      "Epoch 0, Loss: 0.4432465136051178\n",
      "Epoch 500, Loss: 0.06584330648183823\n",
      "Epoch 1000, Loss: 0.06235673651099205\n",
      "Epoch 1500, Loss: 0.060532934963703156\n",
      "Epoch 2000, Loss: 0.06528829038143158\n",
      "Epoch 2500, Loss: 0.060909204185009\n",
      "Epoch 3000, Loss: 0.059943705797195435\n",
      "Epoch 3500, Loss: 0.06352139264345169\n",
      "Epoch 4000, Loss: 0.05589994415640831\n",
      "Epoch 4500, Loss: 0.05885908007621765\n",
      "Epoch 4999, Loss: 0.05966293811798096\n",
      "--- Experiment 7/20 ---\n",
      "Epoch 0, Loss: 0.369963675737381\n",
      "Epoch 500, Loss: 0.05622940883040428\n",
      "Epoch 1000, Loss: 0.05483531951904297\n",
      "Epoch 1500, Loss: 0.05167256295681\n",
      "Epoch 2000, Loss: 0.05568621680140495\n",
      "Epoch 2500, Loss: 0.049096252769231796\n",
      "Epoch 3000, Loss: 0.048483580350875854\n",
      "Epoch 3500, Loss: 0.04947289824485779\n",
      "Epoch 4000, Loss: 0.04626162722706795\n",
      "Epoch 4500, Loss: 0.05042462795972824\n",
      "Epoch 4999, Loss: 0.04773528873920441\n",
      "Epoch 0, Loss: 0.43147170543670654\n",
      "Epoch 500, Loss: 0.09432564675807953\n",
      "Epoch 1000, Loss: 0.08577290177345276\n",
      "Epoch 1500, Loss: 0.07828719913959503\n",
      "Epoch 2000, Loss: 0.0758531391620636\n",
      "Epoch 2500, Loss: 0.07801797240972519\n",
      "Epoch 3000, Loss: 0.07737945020198822\n",
      "Epoch 3500, Loss: 0.07299710810184479\n",
      "Epoch 4000, Loss: 0.07158534973859787\n",
      "Epoch 4500, Loss: 0.07098371535539627\n",
      "Epoch 4999, Loss: 0.07216136157512665\n",
      "Epoch 0, Loss: 0.5798835158348083\n",
      "Epoch 500, Loss: 0.07424118369817734\n",
      "Epoch 1000, Loss: 0.0705939382314682\n",
      "Epoch 1500, Loss: 0.06718238443136215\n",
      "Epoch 2000, Loss: 0.06418716162443161\n",
      "Epoch 2500, Loss: 0.0653746947646141\n",
      "Epoch 3000, Loss: 0.06312680244445801\n",
      "Epoch 3500, Loss: 0.06810666620731354\n",
      "Epoch 4000, Loss: 0.0650877058506012\n",
      "Epoch 4500, Loss: 0.06195800378918648\n",
      "Epoch 4999, Loss: 0.06146132946014404\n",
      "Epoch 0, Loss: 0.6821503639221191\n",
      "Epoch 500, Loss: 0.06804019957780838\n",
      "Epoch 1000, Loss: 0.06505057960748672\n",
      "Epoch 1500, Loss: 0.061543628573417664\n",
      "Epoch 2000, Loss: 0.057961318641901016\n",
      "Epoch 2500, Loss: 0.056604281067848206\n",
      "Epoch 3000, Loss: 0.06040739268064499\n",
      "Epoch 3500, Loss: 0.06593985110521317\n",
      "Epoch 4000, Loss: 0.05726395919919014\n",
      "Epoch 4500, Loss: 0.060898758471012115\n",
      "Epoch 4999, Loss: 0.05508515611290932\n",
      "--- Experiment 8/20 ---\n",
      "Epoch 0, Loss: 0.37382179498672485\n",
      "Epoch 500, Loss: 0.05596426501870155\n",
      "Epoch 1000, Loss: 0.05539259687066078\n",
      "Epoch 1500, Loss: 0.05533590167760849\n",
      "Epoch 2000, Loss: 0.05298447236418724\n",
      "Epoch 2500, Loss: 0.05305403470993042\n",
      "Epoch 3000, Loss: 0.04817267134785652\n",
      "Epoch 3500, Loss: 0.05033878982067108\n",
      "Epoch 4000, Loss: 0.04915887862443924\n",
      "Epoch 4500, Loss: 0.04803483188152313\n",
      "Epoch 4999, Loss: 0.04731885343790054\n",
      "Epoch 0, Loss: 0.5523439645767212\n",
      "Epoch 500, Loss: 0.08823277056217194\n",
      "Epoch 1000, Loss: 0.08521299064159393\n",
      "Epoch 1500, Loss: 0.08210301399230957\n",
      "Epoch 2000, Loss: 0.07947377115488052\n",
      "Epoch 2500, Loss: 0.07902324944734573\n",
      "Epoch 3000, Loss: 0.07431715726852417\n",
      "Epoch 3500, Loss: 0.07673560082912445\n",
      "Epoch 4000, Loss: 0.07720710337162018\n",
      "Epoch 4500, Loss: 0.07175426185131073\n",
      "Epoch 4999, Loss: 0.07236050069332123\n",
      "Epoch 0, Loss: 0.5968785881996155\n",
      "Epoch 500, Loss: 0.07991872727870941\n",
      "Epoch 1000, Loss: 0.06786367297172546\n",
      "Epoch 1500, Loss: 0.06013202667236328\n",
      "Epoch 2000, Loss: 0.06928600370883942\n",
      "Epoch 2500, Loss: 0.0640266165137291\n",
      "Epoch 3000, Loss: 0.06735660135746002\n",
      "Epoch 3500, Loss: 0.06169775873422623\n",
      "Epoch 4000, Loss: 0.06113404035568237\n",
      "Epoch 4500, Loss: 0.06451473385095596\n",
      "Epoch 4999, Loss: 0.06295541673898697\n",
      "Epoch 0, Loss: 0.4281991124153137\n",
      "Epoch 500, Loss: 0.06618493795394897\n",
      "Epoch 1000, Loss: 0.06282124668359756\n",
      "Epoch 1500, Loss: 0.06297087669372559\n",
      "Epoch 2000, Loss: 0.057704709470272064\n",
      "Epoch 2500, Loss: 0.05742897093296051\n",
      "Epoch 3000, Loss: 0.06005670130252838\n",
      "Epoch 3500, Loss: 0.05911186337471008\n",
      "Epoch 4000, Loss: 0.052970271557569504\n",
      "Epoch 4500, Loss: 0.05554162710905075\n",
      "Epoch 4999, Loss: 0.05351312831044197\n",
      "--- Experiment 9/20 ---\n",
      "Epoch 0, Loss: 0.4465278387069702\n",
      "Epoch 500, Loss: 0.05998294800519943\n",
      "Epoch 1000, Loss: 0.05424804612994194\n",
      "Epoch 1500, Loss: 0.05205569788813591\n",
      "Epoch 2000, Loss: 0.05281452089548111\n",
      "Epoch 2500, Loss: 0.056001387536525726\n",
      "Epoch 3000, Loss: 0.050232499837875366\n",
      "Epoch 3500, Loss: 0.05145064741373062\n",
      "Epoch 4000, Loss: 0.05174661800265312\n",
      "Epoch 4500, Loss: 0.046783268451690674\n",
      "Epoch 4999, Loss: 0.04981255158782005\n",
      "Epoch 0, Loss: 0.5122994184494019\n",
      "Epoch 500, Loss: 0.08400454372167587\n",
      "Epoch 1000, Loss: 0.07875905930995941\n",
      "Epoch 1500, Loss: 0.07727955281734467\n",
      "Epoch 2000, Loss: 0.07849206030368805\n",
      "Epoch 2500, Loss: 0.0739884078502655\n",
      "Epoch 3000, Loss: 0.07424085587263107\n",
      "Epoch 3500, Loss: 0.0738149955868721\n",
      "Epoch 4000, Loss: 0.07181372493505478\n",
      "Epoch 4500, Loss: 0.0723278820514679\n",
      "Epoch 4999, Loss: 0.0728108361363411\n",
      "Epoch 0, Loss: 0.5609858632087708\n",
      "Epoch 500, Loss: 0.07194402813911438\n",
      "Epoch 1000, Loss: 0.06459605693817139\n",
      "Epoch 1500, Loss: 0.06357060372829437\n",
      "Epoch 2000, Loss: 0.06304167956113815\n",
      "Epoch 2500, Loss: 0.06194305419921875\n",
      "Epoch 3000, Loss: 0.06325099617242813\n",
      "Epoch 3500, Loss: 0.06338269263505936\n",
      "Epoch 4000, Loss: 0.06059659272432327\n",
      "Epoch 4500, Loss: 0.0607982873916626\n",
      "Epoch 4999, Loss: 0.05903293192386627\n",
      "Epoch 0, Loss: 0.619057834148407\n",
      "Epoch 500, Loss: 0.0685889944434166\n",
      "Epoch 1000, Loss: 0.0646408423781395\n",
      "Epoch 1500, Loss: 0.06116985157132149\n",
      "Epoch 2000, Loss: 0.05712632089853287\n",
      "Epoch 2500, Loss: 0.059125714004039764\n",
      "Epoch 3000, Loss: 0.05821772664785385\n",
      "Epoch 3500, Loss: 0.06145505979657173\n",
      "Epoch 4000, Loss: 0.057852596044540405\n",
      "Epoch 4500, Loss: 0.05334451422095299\n",
      "Epoch 4999, Loss: 0.05659593641757965\n",
      "--- Experiment 10/20 ---\n",
      "Epoch 0, Loss: 0.44571560621261597\n",
      "Epoch 500, Loss: 0.056548427790403366\n",
      "Epoch 1000, Loss: 0.05488484725356102\n",
      "Epoch 1500, Loss: 0.05260509252548218\n",
      "Epoch 2000, Loss: 0.052778687328100204\n",
      "Epoch 2500, Loss: 0.050696007907390594\n",
      "Epoch 3000, Loss: 0.05071360617876053\n",
      "Epoch 3500, Loss: 0.050892993807792664\n",
      "Epoch 4000, Loss: 0.0506831593811512\n",
      "Epoch 4500, Loss: 0.04575120657682419\n",
      "Epoch 4999, Loss: 0.048403702676296234\n",
      "Epoch 0, Loss: 0.4110229015350342\n",
      "Epoch 500, Loss: 0.08318613469600677\n",
      "Epoch 1000, Loss: 0.08701340854167938\n",
      "Epoch 1500, Loss: 0.08088689297437668\n",
      "Epoch 2000, Loss: 0.07773254811763763\n",
      "Epoch 2500, Loss: 0.07615846395492554\n",
      "Epoch 3000, Loss: 0.07208537310361862\n",
      "Epoch 3500, Loss: 0.077937051653862\n",
      "Epoch 4000, Loss: 0.07288148254156113\n",
      "Epoch 4500, Loss: 0.0715145617723465\n",
      "Epoch 4999, Loss: 0.07354749739170074\n",
      "Epoch 0, Loss: 0.420346200466156\n",
      "Epoch 500, Loss: 0.08047610521316528\n",
      "Epoch 1000, Loss: 0.06734348833560944\n",
      "Epoch 1500, Loss: 0.06833957880735397\n",
      "Epoch 2000, Loss: 0.06408059597015381\n",
      "Epoch 2500, Loss: 0.06522262096405029\n",
      "Epoch 3000, Loss: 0.06574350595474243\n",
      "Epoch 3500, Loss: 0.0646650642156601\n",
      "Epoch 4000, Loss: 0.06480799615383148\n",
      "Epoch 4500, Loss: 0.06338854134082794\n",
      "Epoch 4999, Loss: 0.06306076049804688\n",
      "Epoch 0, Loss: 0.44863927364349365\n",
      "Epoch 500, Loss: 0.06755729019641876\n",
      "Epoch 1000, Loss: 0.06588723510503769\n",
      "Epoch 1500, Loss: 0.05857505649328232\n",
      "Epoch 2000, Loss: 0.05566082149744034\n",
      "Epoch 2500, Loss: 0.05634751170873642\n",
      "Epoch 3000, Loss: 0.05876397714018822\n",
      "Epoch 3500, Loss: 0.056888338178396225\n",
      "Epoch 4000, Loss: 0.05888917297124863\n",
      "Epoch 4500, Loss: 0.048635222017765045\n",
      "Epoch 4999, Loss: 0.059010714292526245\n",
      "--- Experiment 11/20 ---\n",
      "Epoch 0, Loss: 0.5470902919769287\n",
      "Epoch 500, Loss: 0.05832034721970558\n",
      "Epoch 1000, Loss: 0.05516990274190903\n",
      "Epoch 1500, Loss: 0.05050412937998772\n",
      "Epoch 2000, Loss: 0.048821620643138885\n",
      "Epoch 2500, Loss: 0.04942404106259346\n",
      "Epoch 3000, Loss: 0.050859235227108\n",
      "Epoch 3500, Loss: 0.05142122507095337\n",
      "Epoch 4000, Loss: 0.04827333241701126\n",
      "Epoch 4500, Loss: 0.04875556379556656\n",
      "Epoch 4999, Loss: 0.047108929604291916\n",
      "Epoch 0, Loss: 0.4194880723953247\n",
      "Epoch 500, Loss: 0.08220533281564713\n",
      "Epoch 1000, Loss: 0.0790785551071167\n",
      "Epoch 1500, Loss: 0.08050625026226044\n",
      "Epoch 2000, Loss: 0.07781422883272171\n",
      "Epoch 2500, Loss: 0.07380002737045288\n",
      "Epoch 3000, Loss: 0.07518438994884491\n",
      "Epoch 3500, Loss: 0.07351785153150558\n",
      "Epoch 4000, Loss: 0.07435819506645203\n",
      "Epoch 4500, Loss: 0.07140317559242249\n",
      "Epoch 4999, Loss: 0.07382447272539139\n",
      "Epoch 0, Loss: 0.6411793231964111\n",
      "Epoch 500, Loss: 0.08078020811080933\n",
      "Epoch 1000, Loss: 0.07133252173662186\n",
      "Epoch 1500, Loss: 0.06708584725856781\n",
      "Epoch 2000, Loss: 0.06239539384841919\n",
      "Epoch 2500, Loss: 0.06523186713457108\n",
      "Epoch 3000, Loss: 0.060446351766586304\n",
      "Epoch 3500, Loss: 0.0636262372136116\n",
      "Epoch 4000, Loss: 0.06317821890115738\n",
      "Epoch 4500, Loss: 0.06045100465416908\n",
      "Epoch 4999, Loss: 0.061659619212150574\n",
      "Epoch 0, Loss: 0.795710027217865\n",
      "Epoch 500, Loss: 0.06611669808626175\n",
      "Epoch 1000, Loss: 0.06306640058755875\n",
      "Epoch 1500, Loss: 0.05840275436639786\n",
      "Epoch 2000, Loss: 0.06194999814033508\n",
      "Epoch 2500, Loss: 0.05587796866893768\n",
      "Epoch 3000, Loss: 0.058348774909973145\n",
      "Epoch 3500, Loss: 0.06161562353372574\n",
      "Epoch 4000, Loss: 0.05887482687830925\n",
      "Epoch 4500, Loss: 0.05833681300282478\n",
      "Epoch 4999, Loss: 0.057875994592905045\n",
      "--- Experiment 12/20 ---\n",
      "Epoch 0, Loss: 0.35456278920173645\n",
      "Epoch 500, Loss: 0.05577172338962555\n",
      "Epoch 1000, Loss: 0.05394097417593002\n",
      "Epoch 1500, Loss: 0.05121658742427826\n",
      "Epoch 2000, Loss: 0.04883364588022232\n",
      "Epoch 2500, Loss: 0.0466533899307251\n",
      "Epoch 3000, Loss: 0.05218261852860451\n",
      "Epoch 3500, Loss: 0.04939475283026695\n",
      "Epoch 4000, Loss: 0.04899776354432106\n",
      "Epoch 4500, Loss: 0.050421908497810364\n",
      "Epoch 4999, Loss: 0.049977317452430725\n",
      "Epoch 0, Loss: 0.4029245376586914\n",
      "Epoch 500, Loss: 0.0853562206029892\n",
      "Epoch 1000, Loss: 0.07950307428836823\n",
      "Epoch 1500, Loss: 0.08289135247468948\n",
      "Epoch 2000, Loss: 0.0757318064570427\n",
      "Epoch 2500, Loss: 0.0749296098947525\n",
      "Epoch 3000, Loss: 0.07505518198013306\n",
      "Epoch 3500, Loss: 0.07246801257133484\n",
      "Epoch 4000, Loss: 0.07303204387426376\n",
      "Epoch 4500, Loss: 0.07237638533115387\n",
      "Epoch 4999, Loss: 0.073025643825531\n",
      "Epoch 0, Loss: 0.5569057464599609\n",
      "Epoch 500, Loss: 0.07697293907403946\n",
      "Epoch 1000, Loss: 0.06912783533334732\n",
      "Epoch 1500, Loss: 0.06644223630428314\n",
      "Epoch 2000, Loss: 0.060796499252319336\n",
      "Epoch 2500, Loss: 0.06635385751724243\n",
      "Epoch 3000, Loss: 0.06662477552890778\n",
      "Epoch 3500, Loss: 0.06290415674448013\n",
      "Epoch 4000, Loss: 0.06455330550670624\n",
      "Epoch 4500, Loss: 0.06424833834171295\n",
      "Epoch 4999, Loss: 0.06409504264593124\n",
      "Epoch 0, Loss: 0.5147790908813477\n",
      "Epoch 500, Loss: 0.06562311947345734\n",
      "Epoch 1000, Loss: 0.05536343529820442\n",
      "Epoch 1500, Loss: 0.06076360493898392\n",
      "Epoch 2000, Loss: 0.057973574846982956\n",
      "Epoch 2500, Loss: 0.058744266629219055\n",
      "Epoch 3000, Loss: 0.055810846388339996\n",
      "Epoch 3500, Loss: 0.057377032935619354\n",
      "Epoch 4000, Loss: 0.05717594176530838\n",
      "Epoch 4500, Loss: 0.05876060575246811\n",
      "Epoch 4999, Loss: 0.05740315094590187\n",
      "--- Experiment 13/20 ---\n",
      "Epoch 0, Loss: 0.6364548802375793\n",
      "Epoch 500, Loss: 0.056353457272052765\n",
      "Epoch 1000, Loss: 0.05427145957946777\n",
      "Epoch 1500, Loss: 0.052606478333473206\n",
      "Epoch 2000, Loss: 0.050418950617313385\n",
      "Epoch 2500, Loss: 0.050012871623039246\n",
      "Epoch 3000, Loss: 0.05414398014545441\n",
      "Epoch 3500, Loss: 0.04950757324695587\n",
      "Epoch 4000, Loss: 0.0514027401804924\n",
      "Epoch 4500, Loss: 0.04739411920309067\n",
      "Epoch 4999, Loss: 0.04876710847020149\n",
      "Epoch 0, Loss: 0.5299228429794312\n",
      "Epoch 500, Loss: 0.08653464913368225\n",
      "Epoch 1000, Loss: 0.08090774714946747\n",
      "Epoch 1500, Loss: 0.08143105357885361\n",
      "Epoch 2000, Loss: 0.07904621213674545\n",
      "Epoch 2500, Loss: 0.07685916870832443\n",
      "Epoch 3000, Loss: 0.07563984394073486\n",
      "Epoch 3500, Loss: 0.07414951920509338\n",
      "Epoch 4000, Loss: 0.08061711490154266\n",
      "Epoch 4500, Loss: 0.07161474227905273\n",
      "Epoch 4999, Loss: 0.07094176113605499\n",
      "Epoch 0, Loss: 0.41236212849617004\n",
      "Epoch 500, Loss: 0.07159897685050964\n",
      "Epoch 1000, Loss: 0.06127648800611496\n",
      "Epoch 1500, Loss: 0.062261294573545456\n",
      "Epoch 2000, Loss: 0.06403882801532745\n",
      "Epoch 2500, Loss: 0.06328815221786499\n",
      "Epoch 3000, Loss: 0.07097101956605911\n",
      "Epoch 3500, Loss: 0.06640689820051193\n",
      "Epoch 4000, Loss: 0.06276144087314606\n",
      "Epoch 4500, Loss: 0.06128930300474167\n",
      "Epoch 4999, Loss: 0.06295941025018692\n",
      "Epoch 0, Loss: 0.3387344181537628\n",
      "Epoch 500, Loss: 0.06168483942747116\n",
      "Epoch 1000, Loss: 0.06291726976633072\n",
      "Epoch 1500, Loss: 0.05831220746040344\n",
      "Epoch 2000, Loss: 0.05894201248884201\n",
      "Epoch 2500, Loss: 0.059305667877197266\n",
      "Epoch 3000, Loss: 0.05836647003889084\n",
      "Epoch 3500, Loss: 0.06174743175506592\n",
      "Epoch 4000, Loss: 0.056342560797929764\n",
      "Epoch 4500, Loss: 0.05776151642203331\n",
      "Epoch 4999, Loss: 0.06249046325683594\n",
      "--- Experiment 14/20 ---\n",
      "Epoch 0, Loss: 0.5251981616020203\n",
      "Epoch 500, Loss: 0.059467945247888565\n",
      "Epoch 1000, Loss: 0.05489180609583855\n",
      "Epoch 1500, Loss: 0.054941583424806595\n",
      "Epoch 2000, Loss: 0.0538133829832077\n",
      "Epoch 2500, Loss: 0.050540272146463394\n",
      "Epoch 3000, Loss: 0.050903208553791046\n",
      "Epoch 3500, Loss: 0.0492490716278553\n",
      "Epoch 4000, Loss: 0.048778973519802094\n",
      "Epoch 4500, Loss: 0.04984183609485626\n",
      "Epoch 4999, Loss: 0.047765448689460754\n",
      "Epoch 0, Loss: 0.4675218164920807\n",
      "Epoch 500, Loss: 0.08538693934679031\n",
      "Epoch 1000, Loss: 0.0835181400179863\n",
      "Epoch 1500, Loss: 0.08027131110429764\n",
      "Epoch 2000, Loss: 0.07593595236539841\n",
      "Epoch 2500, Loss: 0.07409156113862991\n",
      "Epoch 3000, Loss: 0.0755714699625969\n",
      "Epoch 3500, Loss: 0.07346423715353012\n",
      "Epoch 4000, Loss: 0.07312995195388794\n",
      "Epoch 4500, Loss: 0.07783455401659012\n",
      "Epoch 4999, Loss: 0.07436731457710266\n",
      "Epoch 0, Loss: 0.347173810005188\n",
      "Epoch 500, Loss: 0.07181621342897415\n",
      "Epoch 1000, Loss: 0.06549195200204849\n",
      "Epoch 1500, Loss: 0.06335229426622391\n",
      "Epoch 2000, Loss: 0.06423456966876984\n",
      "Epoch 2500, Loss: 0.07336965203285217\n",
      "Epoch 3000, Loss: 0.06830231100320816\n",
      "Epoch 3500, Loss: 0.06266339868307114\n",
      "Epoch 4000, Loss: 0.06458806246519089\n",
      "Epoch 4500, Loss: 0.06262730062007904\n",
      "Epoch 4999, Loss: 0.06367256492376328\n",
      "Epoch 0, Loss: 0.40082964301109314\n",
      "Epoch 500, Loss: 0.06572651863098145\n",
      "Epoch 1000, Loss: 0.06008096784353256\n",
      "Epoch 1500, Loss: 0.06083579361438751\n",
      "Epoch 2000, Loss: 0.060555048286914825\n",
      "Epoch 2500, Loss: 0.05793841928243637\n",
      "Epoch 3000, Loss: 0.0572706013917923\n",
      "Epoch 3500, Loss: 0.06229083240032196\n",
      "Epoch 4000, Loss: 0.05648459494113922\n",
      "Epoch 4500, Loss: 0.056852877140045166\n",
      "Epoch 4999, Loss: 0.0554506778717041\n",
      "--- Experiment 15/20 ---\n",
      "Epoch 0, Loss: 0.4632906913757324\n",
      "Epoch 500, Loss: 0.05515970289707184\n",
      "Epoch 1000, Loss: 0.052913375198841095\n",
      "Epoch 1500, Loss: 0.04999849572777748\n",
      "Epoch 2000, Loss: 0.05136334151029587\n",
      "Epoch 2500, Loss: 0.051902011036872864\n",
      "Epoch 3000, Loss: 0.05116182938218117\n",
      "Epoch 3500, Loss: 0.04795025289058685\n",
      "Epoch 4000, Loss: 0.050283610820770264\n",
      "Epoch 4500, Loss: 0.046284765005111694\n",
      "Epoch 4999, Loss: 0.051787927746772766\n",
      "Epoch 0, Loss: 0.32519781589508057\n",
      "Epoch 500, Loss: 0.0882495865225792\n",
      "Epoch 1000, Loss: 0.08102137595415115\n",
      "Epoch 1500, Loss: 0.0808505266904831\n",
      "Epoch 2000, Loss: 0.07934819161891937\n",
      "Epoch 2500, Loss: 0.073964424431324\n",
      "Epoch 3000, Loss: 0.07448069006204605\n",
      "Epoch 3500, Loss: 0.07498043775558472\n",
      "Epoch 4000, Loss: 0.07591239362955093\n",
      "Epoch 4500, Loss: 0.07147239148616791\n",
      "Epoch 4999, Loss: 0.07389543950557709\n",
      "Epoch 0, Loss: 0.5935596227645874\n",
      "Epoch 500, Loss: 0.07229074835777283\n",
      "Epoch 1000, Loss: 0.06776665151119232\n",
      "Epoch 1500, Loss: 0.0637817233800888\n",
      "Epoch 2000, Loss: 0.0638134628534317\n",
      "Epoch 2500, Loss: 0.06120995804667473\n",
      "Epoch 3000, Loss: 0.06478404998779297\n",
      "Epoch 3500, Loss: 0.06288423389196396\n",
      "Epoch 4000, Loss: 0.06405581533908844\n",
      "Epoch 4500, Loss: 0.059497103095054626\n",
      "Epoch 4999, Loss: 0.06104941666126251\n",
      "Epoch 0, Loss: 0.4541669487953186\n",
      "Epoch 500, Loss: 0.06834417581558228\n",
      "Epoch 1000, Loss: 0.06373512744903564\n",
      "Epoch 1500, Loss: 0.056997112929821014\n",
      "Epoch 2000, Loss: 0.058481648564338684\n",
      "Epoch 2500, Loss: 0.05868424475193024\n",
      "Epoch 3000, Loss: 0.053585730493068695\n",
      "Epoch 3500, Loss: 0.05734127759933472\n",
      "Epoch 4000, Loss: 0.05546677112579346\n",
      "Epoch 4500, Loss: 0.06192140281200409\n",
      "Epoch 4999, Loss: 0.060859061777591705\n",
      "--- Experiment 16/20 ---\n",
      "Epoch 0, Loss: 0.38309410214424133\n",
      "Epoch 500, Loss: 0.056804582476615906\n",
      "Epoch 1000, Loss: 0.05061805248260498\n",
      "Epoch 1500, Loss: 0.05031442642211914\n",
      "Epoch 2000, Loss: 0.05248171091079712\n",
      "Epoch 2500, Loss: 0.05186944827437401\n",
      "Epoch 3000, Loss: 0.04985145479440689\n",
      "Epoch 3500, Loss: 0.049395330250263214\n",
      "Epoch 4000, Loss: 0.0485401451587677\n",
      "Epoch 4500, Loss: 0.04952402785420418\n",
      "Epoch 4999, Loss: 0.048091404139995575\n",
      "Epoch 0, Loss: 0.5726173520088196\n",
      "Epoch 500, Loss: 0.08649982511997223\n",
      "Epoch 1000, Loss: 0.08085380494594574\n",
      "Epoch 1500, Loss: 0.08071859925985336\n",
      "Epoch 2000, Loss: 0.07513337582349777\n",
      "Epoch 2500, Loss: 0.07311153411865234\n",
      "Epoch 3000, Loss: 0.07790493220090866\n",
      "Epoch 3500, Loss: 0.07364022731781006\n",
      "Epoch 4000, Loss: 0.07315264642238617\n",
      "Epoch 4500, Loss: 0.07834459096193314\n",
      "Epoch 4999, Loss: 0.07298601418733597\n",
      "Epoch 0, Loss: 0.37356558442115784\n",
      "Epoch 500, Loss: 0.07546943426132202\n",
      "Epoch 1000, Loss: 0.06969541311264038\n",
      "Epoch 1500, Loss: 0.06366443634033203\n",
      "Epoch 2000, Loss: 0.06545649468898773\n",
      "Epoch 2500, Loss: 0.06315933167934418\n",
      "Epoch 3000, Loss: 0.06180373206734657\n",
      "Epoch 3500, Loss: 0.06338901817798615\n",
      "Epoch 4000, Loss: 0.06376104801893234\n",
      "Epoch 4500, Loss: 0.06090507656335831\n",
      "Epoch 4999, Loss: 0.0673050731420517\n",
      "Epoch 0, Loss: 0.329082190990448\n",
      "Epoch 500, Loss: 0.06465727090835571\n",
      "Epoch 1000, Loss: 0.06097052991390228\n",
      "Epoch 1500, Loss: 0.05989072099328041\n",
      "Epoch 2000, Loss: 0.05665932595729828\n",
      "Epoch 2500, Loss: 0.06134148687124252\n",
      "Epoch 3000, Loss: 0.05763731151819229\n",
      "Epoch 3500, Loss: 0.05665842071175575\n",
      "Epoch 4000, Loss: 0.05602092295885086\n",
      "Epoch 4500, Loss: 0.06292407214641571\n",
      "Epoch 4999, Loss: 0.0570668987929821\n",
      "--- Experiment 17/20 ---\n",
      "Epoch 0, Loss: 0.3954352140426636\n",
      "Epoch 500, Loss: 0.058828577399253845\n",
      "Epoch 1000, Loss: 0.056399472057819366\n",
      "Epoch 1500, Loss: 0.05523701012134552\n",
      "Epoch 2000, Loss: 0.05269059166312218\n",
      "Epoch 2500, Loss: 0.04827241972088814\n",
      "Epoch 3000, Loss: 0.05558033287525177\n",
      "Epoch 3500, Loss: 0.05094309523701668\n",
      "Epoch 4000, Loss: 0.05022335797548294\n",
      "Epoch 4500, Loss: 0.05028340965509415\n",
      "Epoch 4999, Loss: 0.04908912628889084\n",
      "Epoch 0, Loss: 0.2681019902229309\n",
      "Epoch 500, Loss: 0.08590231835842133\n",
      "Epoch 1000, Loss: 0.08075645565986633\n",
      "Epoch 1500, Loss: 0.07904638350009918\n",
      "Epoch 2000, Loss: 0.07967005670070648\n",
      "Epoch 2500, Loss: 0.07344703376293182\n",
      "Epoch 3000, Loss: 0.07336105406284332\n",
      "Epoch 3500, Loss: 0.07343439757823944\n",
      "Epoch 4000, Loss: 0.07007457315921783\n",
      "Epoch 4500, Loss: 0.07226593047380447\n",
      "Epoch 4999, Loss: 0.07232261449098587\n",
      "Epoch 0, Loss: 0.5899912118911743\n",
      "Epoch 500, Loss: 0.07781589031219482\n",
      "Epoch 1000, Loss: 0.0654304251074791\n",
      "Epoch 1500, Loss: 0.06687718629837036\n",
      "Epoch 2000, Loss: 0.0649506002664566\n",
      "Epoch 2500, Loss: 0.06273004412651062\n",
      "Epoch 3000, Loss: 0.06205282360315323\n",
      "Epoch 3500, Loss: 0.06395045667886734\n",
      "Epoch 4000, Loss: 0.0643666684627533\n",
      "Epoch 4500, Loss: 0.06287641823291779\n",
      "Epoch 4999, Loss: 0.0666286051273346\n",
      "Epoch 0, Loss: 0.4905252158641815\n",
      "Epoch 500, Loss: 0.06452886760234833\n",
      "Epoch 1000, Loss: 0.06459734588861465\n",
      "Epoch 1500, Loss: 0.05756821110844612\n",
      "Epoch 2000, Loss: 0.05742894858121872\n",
      "Epoch 2500, Loss: 0.05861230939626694\n",
      "Epoch 3000, Loss: 0.059389062225818634\n",
      "Epoch 3500, Loss: 0.054530657827854156\n",
      "Epoch 4000, Loss: 0.05695512145757675\n",
      "Epoch 4500, Loss: 0.0653509795665741\n",
      "Epoch 4999, Loss: 0.05798869952559471\n",
      "--- Experiment 18/20 ---\n",
      "Epoch 0, Loss: 0.6346200108528137\n",
      "Epoch 500, Loss: 0.05819861590862274\n",
      "Epoch 1000, Loss: 0.05623054504394531\n",
      "Epoch 1500, Loss: 0.051021508872509\n",
      "Epoch 2000, Loss: 0.04980652034282684\n",
      "Epoch 2500, Loss: 0.05379234254360199\n",
      "Epoch 3000, Loss: 0.05163188651204109\n",
      "Epoch 3500, Loss: 0.05194057896733284\n",
      "Epoch 4000, Loss: 0.04826384037733078\n",
      "Epoch 4500, Loss: 0.049063168466091156\n",
      "Epoch 4999, Loss: 0.05012650042772293\n",
      "Epoch 0, Loss: 0.35690054297447205\n",
      "Epoch 500, Loss: 0.08818873763084412\n",
      "Epoch 1000, Loss: 0.08484494686126709\n",
      "Epoch 1500, Loss: 0.07956025004386902\n",
      "Epoch 2000, Loss: 0.07884636521339417\n",
      "Epoch 2500, Loss: 0.07666546106338501\n",
      "Epoch 3000, Loss: 0.07784521579742432\n",
      "Epoch 3500, Loss: 0.07648516446352005\n",
      "Epoch 4000, Loss: 0.07452841848134995\n",
      "Epoch 4500, Loss: 0.07812201976776123\n",
      "Epoch 4999, Loss: 0.07404042035341263\n",
      "Epoch 0, Loss: 0.5373334884643555\n",
      "Epoch 500, Loss: 0.07381055504083633\n",
      "Epoch 1000, Loss: 0.06790328025817871\n",
      "Epoch 1500, Loss: 0.06350205093622208\n",
      "Epoch 2000, Loss: 0.06462510675191879\n",
      "Epoch 2500, Loss: 0.06496122479438782\n",
      "Epoch 3000, Loss: 0.06370645761489868\n",
      "Epoch 3500, Loss: 0.06019940599799156\n",
      "Epoch 4000, Loss: 0.06127296760678291\n",
      "Epoch 4500, Loss: 0.06554330885410309\n",
      "Epoch 4999, Loss: 0.05781184881925583\n",
      "Epoch 0, Loss: 0.37838128209114075\n",
      "Epoch 500, Loss: 0.06611932069063187\n",
      "Epoch 1000, Loss: 0.06297622621059418\n",
      "Epoch 1500, Loss: 0.05708068609237671\n",
      "Epoch 2000, Loss: 0.05723290890455246\n",
      "Epoch 2500, Loss: 0.060173891484737396\n",
      "Epoch 3000, Loss: 0.0529027134180069\n",
      "Epoch 3500, Loss: 0.05924541875720024\n",
      "Epoch 4000, Loss: 0.05540554225444794\n",
      "Epoch 4500, Loss: 0.057325780391693115\n",
      "Epoch 4999, Loss: 0.06192455440759659\n",
      "--- Experiment 19/20 ---\n",
      "Epoch 0, Loss: 0.40064138174057007\n",
      "Epoch 500, Loss: 0.05788180232048035\n",
      "Epoch 1000, Loss: 0.05363636091351509\n",
      "Epoch 1500, Loss: 0.05295901745557785\n",
      "Epoch 2000, Loss: 0.05316261202096939\n",
      "Epoch 2500, Loss: 0.048932477831840515\n",
      "Epoch 3000, Loss: 0.054267533123493195\n",
      "Epoch 3500, Loss: 0.04647998511791229\n",
      "Epoch 4000, Loss: 0.04974546283483505\n",
      "Epoch 4500, Loss: 0.04871779680252075\n",
      "Epoch 4999, Loss: 0.04842212051153183\n",
      "Epoch 0, Loss: 0.4771479666233063\n",
      "Epoch 500, Loss: 0.0879281610250473\n",
      "Epoch 1000, Loss: 0.08234182000160217\n",
      "Epoch 1500, Loss: 0.07690031081438065\n",
      "Epoch 2000, Loss: 0.07642985880374908\n",
      "Epoch 2500, Loss: 0.07491806894540787\n",
      "Epoch 3000, Loss: 0.07345247268676758\n",
      "Epoch 3500, Loss: 0.07565594464540482\n",
      "Epoch 4000, Loss: 0.07290389388799667\n",
      "Epoch 4500, Loss: 0.07713599503040314\n",
      "Epoch 4999, Loss: 0.07077028602361679\n",
      "Epoch 0, Loss: 0.34252381324768066\n",
      "Epoch 500, Loss: 0.07270122319459915\n",
      "Epoch 1000, Loss: 0.06563399732112885\n",
      "Epoch 1500, Loss: 0.06299537420272827\n",
      "Epoch 2000, Loss: 0.06109374389052391\n",
      "Epoch 2500, Loss: 0.06527168303728104\n",
      "Epoch 3000, Loss: 0.06113174557685852\n",
      "Epoch 3500, Loss: 0.061191145330667496\n",
      "Epoch 4000, Loss: 0.06255993247032166\n",
      "Epoch 4500, Loss: 0.06276081502437592\n",
      "Epoch 4999, Loss: 0.06559431552886963\n",
      "Epoch 0, Loss: 0.44593942165374756\n",
      "Epoch 500, Loss: 0.0680461972951889\n",
      "Epoch 1000, Loss: 0.06179570406675339\n",
      "Epoch 1500, Loss: 0.05891157314181328\n",
      "Epoch 2000, Loss: 0.0597565695643425\n",
      "Epoch 2500, Loss: 0.05950482189655304\n",
      "Epoch 3000, Loss: 0.059578850865364075\n",
      "Epoch 3500, Loss: 0.05783930420875549\n",
      "Epoch 4000, Loss: 0.05451827123761177\n",
      "Epoch 4500, Loss: 0.055999092757701874\n",
      "Epoch 4999, Loss: 0.057807378470897675\n",
      "--- Experiment 20/20 ---\n",
      "Epoch 0, Loss: 0.4816986620426178\n",
      "Epoch 500, Loss: 0.054109081625938416\n",
      "Epoch 1000, Loss: 0.053264081478118896\n",
      "Epoch 1500, Loss: 0.05302457511425018\n",
      "Epoch 2000, Loss: 0.05177070200443268\n",
      "Epoch 2500, Loss: 0.05250339210033417\n",
      "Epoch 3000, Loss: 0.050057560205459595\n",
      "Epoch 3500, Loss: 0.051010698080062866\n",
      "Epoch 4000, Loss: 0.04855010658502579\n",
      "Epoch 4500, Loss: 0.049195632338523865\n",
      "Epoch 4999, Loss: 0.04833708330988884\n",
      "Epoch 0, Loss: 0.38041621446609497\n",
      "Epoch 500, Loss: 0.08568835258483887\n",
      "Epoch 1000, Loss: 0.0796695128083229\n",
      "Epoch 1500, Loss: 0.07916796952486038\n",
      "Epoch 2000, Loss: 0.07401493191719055\n",
      "Epoch 2500, Loss: 0.07480230927467346\n",
      "Epoch 3000, Loss: 0.07413207739591599\n",
      "Epoch 3500, Loss: 0.07412262260913849\n",
      "Epoch 4000, Loss: 0.0743521898984909\n",
      "Epoch 4500, Loss: 0.07152561098337173\n",
      "Epoch 4999, Loss: 0.08091221004724503\n",
      "Epoch 0, Loss: 0.527318000793457\n",
      "Epoch 500, Loss: 0.07380970567464828\n",
      "Epoch 1000, Loss: 0.06959742307662964\n",
      "Epoch 1500, Loss: 0.0645139142870903\n",
      "Epoch 2000, Loss: 0.06498463451862335\n",
      "Epoch 2500, Loss: 0.06390439718961716\n",
      "Epoch 3000, Loss: 0.062077365815639496\n",
      "Epoch 3500, Loss: 0.07350602746009827\n",
      "Epoch 4000, Loss: 0.06786993145942688\n",
      "Epoch 4500, Loss: 0.06307990849018097\n",
      "Epoch 4999, Loss: 0.06128236651420593\n",
      "Epoch 0, Loss: 0.8702002763748169\n",
      "Epoch 500, Loss: 0.06827202439308167\n",
      "Epoch 1000, Loss: 0.06612353026866913\n",
      "Epoch 1500, Loss: 0.061905186623334885\n",
      "Epoch 2000, Loss: 0.05808962136507034\n",
      "Epoch 2500, Loss: 0.057156194001436234\n",
      "Epoch 3000, Loss: 0.057909734547138214\n",
      "Epoch 3500, Loss: 0.057277850806713104\n",
      "Epoch 4000, Loss: 0.06045977771282196\n",
      "Epoch 4500, Loss: 0.058763839304447174\n",
      "Epoch 4999, Loss: 0.057147316634655\n",
      "Average Validation Loss (Base):   0.047884 ± 0.000758\n",
      "Average Validation Loss (Import): 0.063162 ± 0.001924\n",
      "Average Validation Loss (GP Out): 0.057072 ± 0.005609\n",
      "Average Validation Loss (GP Res): 0.053176 ± 0.002083\n",
      "\n",
      "==== Running experiments for IC: step, BC: neumann ====\n",
      "--- Experiment 1/20 ---\n",
      "Epoch 0, Loss: 0.48504534363746643\n",
      "Epoch 500, Loss: 0.05544818937778473\n",
      "Epoch 1000, Loss: 0.052203238010406494\n",
      "Epoch 1500, Loss: 0.052150361239910126\n",
      "Epoch 2000, Loss: 0.05312251299619675\n",
      "Epoch 2500, Loss: 0.04881361499428749\n",
      "Epoch 3000, Loss: 0.05074487626552582\n",
      "Epoch 3500, Loss: 0.04911816865205765\n",
      "Epoch 4000, Loss: 0.05167189612984657\n",
      "Epoch 4500, Loss: 0.044938553124666214\n",
      "Epoch 4999, Loss: 0.04941844567656517\n",
      "Epoch 0, Loss: 0.4318923354148865\n",
      "Epoch 500, Loss: 0.08688339591026306\n",
      "Epoch 1000, Loss: 0.0820486843585968\n",
      "Epoch 1500, Loss: 0.08089429140090942\n",
      "Epoch 2000, Loss: 0.08431011438369751\n",
      "Epoch 2500, Loss: 0.07965800166130066\n",
      "Epoch 3000, Loss: 0.079682357609272\n",
      "Epoch 3500, Loss: 0.08477842807769775\n",
      "Epoch 4000, Loss: 0.0723615288734436\n",
      "Epoch 4500, Loss: 0.07589564472436905\n",
      "Epoch 4999, Loss: 0.0743604302406311\n",
      "Epoch 0, Loss: 0.42628687620162964\n",
      "Epoch 500, Loss: 0.07597437500953674\n",
      "Epoch 1000, Loss: 0.06702354550361633\n",
      "Epoch 1500, Loss: 0.06372668594121933\n",
      "Epoch 2000, Loss: 0.061760008335113525\n",
      "Epoch 2500, Loss: 0.06346079707145691\n",
      "Epoch 3000, Loss: 0.06351805478334427\n",
      "Epoch 3500, Loss: 0.07019758969545364\n",
      "Epoch 4000, Loss: 0.06507149338722229\n",
      "Epoch 4500, Loss: 0.06337159872055054\n",
      "Epoch 4999, Loss: 0.06373767554759979\n",
      "Epoch 0, Loss: 0.43591243028640747\n",
      "Epoch 500, Loss: 0.06703190505504608\n",
      "Epoch 1000, Loss: 0.06383354216814041\n",
      "Epoch 1500, Loss: 0.06075930595397949\n",
      "Epoch 2000, Loss: 0.06304696202278137\n",
      "Epoch 2500, Loss: 0.05906348675489426\n",
      "Epoch 3000, Loss: 0.05848700553178787\n",
      "Epoch 3500, Loss: 0.05526396632194519\n",
      "Epoch 4000, Loss: 0.05393436551094055\n",
      "Epoch 4500, Loss: 0.06027314439415932\n",
      "Epoch 4999, Loss: 0.06079462170600891\n",
      "--- Experiment 2/20 ---\n",
      "Epoch 0, Loss: 0.41211169958114624\n",
      "Epoch 500, Loss: 0.05688709020614624\n",
      "Epoch 1000, Loss: 0.05404900014400482\n",
      "Epoch 1500, Loss: 0.05326845496892929\n",
      "Epoch 2000, Loss: 0.052384741604328156\n",
      "Epoch 2500, Loss: 0.05246193706989288\n",
      "Epoch 3000, Loss: 0.053164027631282806\n",
      "Epoch 3500, Loss: 0.04874693602323532\n",
      "Epoch 4000, Loss: 0.04826003313064575\n",
      "Epoch 4500, Loss: 0.04800236597657204\n",
      "Epoch 4999, Loss: 0.04821676015853882\n",
      "Epoch 0, Loss: 0.7605263590812683\n",
      "Epoch 500, Loss: 0.08741496503353119\n",
      "Epoch 1000, Loss: 0.08000726252794266\n",
      "Epoch 1500, Loss: 0.07951132953166962\n",
      "Epoch 2000, Loss: 0.0841304361820221\n",
      "Epoch 2500, Loss: 0.07494198530912399\n",
      "Epoch 3000, Loss: 0.07575385272502899\n",
      "Epoch 3500, Loss: 0.07165985554456711\n",
      "Epoch 4000, Loss: 0.07334612309932709\n",
      "Epoch 4500, Loss: 0.07365784794092178\n",
      "Epoch 4999, Loss: 0.07391075044870377\n",
      "Epoch 0, Loss: 0.4879113733768463\n",
      "Epoch 500, Loss: 0.0774381086230278\n",
      "Epoch 1000, Loss: 0.0690789669752121\n",
      "Epoch 1500, Loss: 0.06589058041572571\n",
      "Epoch 2000, Loss: 0.06212296709418297\n",
      "Epoch 2500, Loss: 0.062220510095357895\n",
      "Epoch 3000, Loss: 0.05842646211385727\n",
      "Epoch 3500, Loss: 0.06484484672546387\n",
      "Epoch 4000, Loss: 0.06492699682712555\n",
      "Epoch 4500, Loss: 0.06378896534442902\n",
      "Epoch 4999, Loss: 0.06208427622914314\n",
      "Epoch 0, Loss: 0.36987611651420593\n",
      "Epoch 500, Loss: 0.06788548827171326\n",
      "Epoch 1000, Loss: 0.06457315385341644\n",
      "Epoch 1500, Loss: 0.05820060893893242\n",
      "Epoch 2000, Loss: 0.05929899215698242\n",
      "Epoch 2500, Loss: 0.05555647984147072\n",
      "Epoch 3000, Loss: 0.05437752604484558\n",
      "Epoch 3500, Loss: 0.05972932651638985\n",
      "Epoch 4000, Loss: 0.06290768831968307\n",
      "Epoch 4500, Loss: 0.05762563645839691\n",
      "Epoch 4999, Loss: 0.057272639125585556\n",
      "--- Experiment 3/20 ---\n",
      "Epoch 0, Loss: 0.4245137572288513\n",
      "Epoch 500, Loss: 0.057254478335380554\n",
      "Epoch 1000, Loss: 0.05511391907930374\n",
      "Epoch 1500, Loss: 0.04970899224281311\n",
      "Epoch 2000, Loss: 0.05541641265153885\n",
      "Epoch 2500, Loss: 0.04872886463999748\n",
      "Epoch 3000, Loss: 0.054947324097156525\n",
      "Epoch 3500, Loss: 0.05199602246284485\n",
      "Epoch 4000, Loss: 0.04910670220851898\n",
      "Epoch 4500, Loss: 0.04827018082141876\n",
      "Epoch 4999, Loss: 0.04888458549976349\n",
      "Epoch 0, Loss: 0.41142845153808594\n",
      "Epoch 500, Loss: 0.08737709373235703\n",
      "Epoch 1000, Loss: 0.08336904644966125\n",
      "Epoch 1500, Loss: 0.0831766203045845\n",
      "Epoch 2000, Loss: 0.07834625244140625\n",
      "Epoch 2500, Loss: 0.07510114461183548\n",
      "Epoch 3000, Loss: 0.0755649209022522\n",
      "Epoch 3500, Loss: 0.0786576196551323\n",
      "Epoch 4000, Loss: 0.07469864934682846\n",
      "Epoch 4500, Loss: 0.07508084923028946\n",
      "Epoch 4999, Loss: 0.07477527856826782\n",
      "Epoch 0, Loss: 0.8104441165924072\n",
      "Epoch 500, Loss: 0.08127676695585251\n",
      "Epoch 1000, Loss: 0.07100827246904373\n",
      "Epoch 1500, Loss: 0.06466522067785263\n",
      "Epoch 2000, Loss: 0.06376631557941437\n",
      "Epoch 2500, Loss: 0.06375788152217865\n",
      "Epoch 3000, Loss: 0.06443384289741516\n",
      "Epoch 3500, Loss: 0.06537049263715744\n",
      "Epoch 4000, Loss: 0.06561623513698578\n",
      "Epoch 4500, Loss: 0.06389018893241882\n",
      "Epoch 4999, Loss: 0.06044238433241844\n",
      "Epoch 0, Loss: 0.5268679261207581\n",
      "Epoch 500, Loss: 0.06461671739816666\n",
      "Epoch 1000, Loss: 0.06350888311862946\n",
      "Epoch 1500, Loss: 0.060604579746723175\n",
      "Epoch 2000, Loss: 0.06304868310689926\n",
      "Epoch 2500, Loss: 0.06008921191096306\n",
      "Epoch 3000, Loss: 0.06015612185001373\n",
      "Epoch 3500, Loss: 0.05742832273244858\n",
      "Epoch 4000, Loss: 0.05730421096086502\n",
      "Epoch 4500, Loss: 0.06096905469894409\n",
      "Epoch 4999, Loss: 0.051718853414058685\n",
      "--- Experiment 4/20 ---\n",
      "Epoch 0, Loss: 0.5126857161521912\n",
      "Epoch 500, Loss: 0.05632951855659485\n",
      "Epoch 1000, Loss: 0.055287569761276245\n",
      "Epoch 1500, Loss: 0.05100741237401962\n",
      "Epoch 2000, Loss: 0.05102679133415222\n",
      "Epoch 2500, Loss: 0.0533585399389267\n",
      "Epoch 3000, Loss: 0.049865998327732086\n",
      "Epoch 3500, Loss: 0.046863533556461334\n",
      "Epoch 4000, Loss: 0.04867015779018402\n",
      "Epoch 4500, Loss: 0.0491085946559906\n",
      "Epoch 4999, Loss: 0.047658830881118774\n",
      "Epoch 0, Loss: 0.34314730763435364\n",
      "Epoch 500, Loss: 0.08415710180997849\n",
      "Epoch 1000, Loss: 0.07982629537582397\n",
      "Epoch 1500, Loss: 0.07799763232469559\n",
      "Epoch 2000, Loss: 0.07568080723285675\n",
      "Epoch 2500, Loss: 0.07530170679092407\n",
      "Epoch 3000, Loss: 0.0725976973772049\n",
      "Epoch 3500, Loss: 0.07478352636098862\n",
      "Epoch 4000, Loss: 0.07095493376255035\n",
      "Epoch 4500, Loss: 0.07180698215961456\n",
      "Epoch 4999, Loss: 0.07135965675115585\n",
      "Epoch 0, Loss: 0.8512070775032043\n",
      "Epoch 500, Loss: 0.08105771243572235\n",
      "Epoch 1000, Loss: 0.06728930026292801\n",
      "Epoch 1500, Loss: 0.06382190436124802\n",
      "Epoch 2000, Loss: 0.06817124038934708\n",
      "Epoch 2500, Loss: 0.06410686671733856\n",
      "Epoch 3000, Loss: 0.06548705697059631\n",
      "Epoch 3500, Loss: 0.06032349541783333\n",
      "Epoch 4000, Loss: 0.0627032220363617\n",
      "Epoch 4500, Loss: 0.06739713251590729\n",
      "Epoch 4999, Loss: 0.06528863310813904\n",
      "Epoch 0, Loss: 0.4763740599155426\n",
      "Epoch 500, Loss: 0.06821957975625992\n",
      "Epoch 1000, Loss: 0.06419234722852707\n",
      "Epoch 1500, Loss: 0.05306629091501236\n",
      "Epoch 2000, Loss: 0.06250225752592087\n",
      "Epoch 2500, Loss: 0.057104211300611496\n",
      "Epoch 3000, Loss: 0.06191020458936691\n",
      "Epoch 3500, Loss: 0.05525370314717293\n",
      "Epoch 4000, Loss: 0.05977197736501694\n",
      "Epoch 4500, Loss: 0.05767381191253662\n",
      "Epoch 4999, Loss: 0.054681289941072464\n",
      "--- Experiment 5/20 ---\n",
      "Epoch 0, Loss: 0.4873678982257843\n",
      "Epoch 500, Loss: 0.05959080904722214\n",
      "Epoch 1000, Loss: 0.05449264496564865\n",
      "Epoch 1500, Loss: 0.05164539813995361\n",
      "Epoch 2000, Loss: 0.05309632048010826\n",
      "Epoch 2500, Loss: 0.051033735275268555\n",
      "Epoch 3000, Loss: 0.05326588451862335\n",
      "Epoch 3500, Loss: 0.04980231821537018\n",
      "Epoch 4000, Loss: 0.04893532022833824\n",
      "Epoch 4500, Loss: 0.04629145562648773\n",
      "Epoch 4999, Loss: 0.04731806367635727\n",
      "Epoch 0, Loss: 0.4886183440685272\n",
      "Epoch 500, Loss: 0.08817262202501297\n",
      "Epoch 1000, Loss: 0.08653384447097778\n",
      "Epoch 1500, Loss: 0.07777897268533707\n",
      "Epoch 2000, Loss: 0.07780136168003082\n",
      "Epoch 2500, Loss: 0.07680109143257141\n",
      "Epoch 3000, Loss: 0.0742286741733551\n",
      "Epoch 3500, Loss: 0.07523883879184723\n",
      "Epoch 4000, Loss: 0.07543043047189713\n",
      "Epoch 4500, Loss: 0.08301158249378204\n",
      "Epoch 4999, Loss: 0.0708952397108078\n",
      "Epoch 0, Loss: 0.39470139145851135\n",
      "Epoch 500, Loss: 0.07759223133325577\n",
      "Epoch 1000, Loss: 0.06648547202348709\n",
      "Epoch 1500, Loss: 0.06362949311733246\n",
      "Epoch 2000, Loss: 0.06352731585502625\n",
      "Epoch 2500, Loss: 0.06187110021710396\n",
      "Epoch 3000, Loss: 0.05731944367289543\n",
      "Epoch 3500, Loss: 0.06765226274728775\n",
      "Epoch 4000, Loss: 0.06560081988573074\n",
      "Epoch 4500, Loss: 0.06160800904035568\n",
      "Epoch 4999, Loss: 0.0650649294257164\n",
      "Epoch 0, Loss: 0.5931415557861328\n",
      "Epoch 500, Loss: 0.0671931654214859\n",
      "Epoch 1000, Loss: 0.06283310055732727\n",
      "Epoch 1500, Loss: 0.05962948501110077\n",
      "Epoch 2000, Loss: 0.05945020914077759\n",
      "Epoch 2500, Loss: 0.056335702538490295\n",
      "Epoch 3000, Loss: 0.05930919945240021\n",
      "Epoch 3500, Loss: 0.06000285595655441\n",
      "Epoch 4000, Loss: 0.05810033529996872\n",
      "Epoch 4500, Loss: 0.05217418447136879\n",
      "Epoch 4999, Loss: 0.06175674498081207\n",
      "--- Experiment 6/20 ---\n",
      "Epoch 0, Loss: 0.606484591960907\n",
      "Epoch 500, Loss: 0.05972420424222946\n",
      "Epoch 1000, Loss: 0.05624499171972275\n",
      "Epoch 1500, Loss: 0.05366873741149902\n",
      "Epoch 2000, Loss: 0.05138222873210907\n",
      "Epoch 2500, Loss: 0.050265878438949585\n",
      "Epoch 3000, Loss: 0.0533711276948452\n",
      "Epoch 3500, Loss: 0.05167868733406067\n",
      "Epoch 4000, Loss: 0.0516292080283165\n",
      "Epoch 4500, Loss: 0.04815713316202164\n",
      "Epoch 4999, Loss: 0.05476631224155426\n",
      "Epoch 0, Loss: 0.8026546239852905\n",
      "Epoch 500, Loss: 0.08639924228191376\n",
      "Epoch 1000, Loss: 0.08089864253997803\n",
      "Epoch 1500, Loss: 0.08167794346809387\n",
      "Epoch 2000, Loss: 0.07733706384897232\n",
      "Epoch 2500, Loss: 0.07379021495580673\n",
      "Epoch 3000, Loss: 0.073383629322052\n",
      "Epoch 3500, Loss: 0.07355231791734695\n",
      "Epoch 4000, Loss: 0.0775250494480133\n",
      "Epoch 4500, Loss: 0.07691364735364914\n",
      "Epoch 4999, Loss: 0.07362663745880127\n",
      "Epoch 0, Loss: 0.4443466067314148\n",
      "Epoch 500, Loss: 0.0762987956404686\n",
      "Epoch 1000, Loss: 0.06797519326210022\n",
      "Epoch 1500, Loss: 0.058098822832107544\n",
      "Epoch 2000, Loss: 0.06428644806146622\n",
      "Epoch 2500, Loss: 0.0630909651517868\n",
      "Epoch 3000, Loss: 0.06348783522844315\n",
      "Epoch 3500, Loss: 0.06505835801362991\n",
      "Epoch 4000, Loss: 0.056474313139915466\n",
      "Epoch 4500, Loss: 0.06681355834007263\n",
      "Epoch 4999, Loss: 0.06394480913877487\n",
      "Epoch 0, Loss: 0.6067949533462524\n",
      "Epoch 500, Loss: 0.06406223773956299\n",
      "Epoch 1000, Loss: 0.06577868014574051\n",
      "Epoch 1500, Loss: 0.0632658451795578\n",
      "Epoch 2000, Loss: 0.06242785602807999\n",
      "Epoch 2500, Loss: 0.06073901057243347\n",
      "Epoch 3000, Loss: 0.05952605605125427\n",
      "Epoch 3500, Loss: 0.05827263742685318\n",
      "Epoch 4000, Loss: 0.06102802976965904\n",
      "Epoch 4500, Loss: 0.059684351086616516\n",
      "Epoch 4999, Loss: 0.0574568435549736\n",
      "--- Experiment 7/20 ---\n",
      "Epoch 0, Loss: 0.41740888357162476\n",
      "Epoch 500, Loss: 0.05807076394557953\n",
      "Epoch 1000, Loss: 0.05560072138905525\n",
      "Epoch 1500, Loss: 0.052789315581321716\n",
      "Epoch 2000, Loss: 0.05586639791727066\n",
      "Epoch 2500, Loss: 0.05160421133041382\n",
      "Epoch 3000, Loss: 0.051294438540935516\n",
      "Epoch 3500, Loss: 0.05036520957946777\n",
      "Epoch 4000, Loss: 0.0474984310567379\n",
      "Epoch 4500, Loss: 0.049032602459192276\n",
      "Epoch 4999, Loss: 0.04637761414051056\n",
      "Epoch 0, Loss: 0.4433687627315521\n",
      "Epoch 500, Loss: 0.08333449065685272\n",
      "Epoch 1000, Loss: 0.08114324510097504\n",
      "Epoch 1500, Loss: 0.07620418071746826\n",
      "Epoch 2000, Loss: 0.07586519420146942\n",
      "Epoch 2500, Loss: 0.0799594447016716\n",
      "Epoch 3000, Loss: 0.07348085194826126\n",
      "Epoch 3500, Loss: 0.07272429764270782\n",
      "Epoch 4000, Loss: 0.07326095551252365\n",
      "Epoch 4500, Loss: 0.07445308566093445\n",
      "Epoch 4999, Loss: 0.07198698818683624\n",
      "Epoch 0, Loss: 0.5471655130386353\n",
      "Epoch 500, Loss: 0.0751408040523529\n",
      "Epoch 1000, Loss: 0.06320963054895401\n",
      "Epoch 1500, Loss: 0.0656311884522438\n",
      "Epoch 2000, Loss: 0.061308540403842926\n",
      "Epoch 2500, Loss: 0.05895128846168518\n",
      "Epoch 3000, Loss: 0.0628986805677414\n",
      "Epoch 3500, Loss: 0.06734280288219452\n",
      "Epoch 4000, Loss: 0.06108984723687172\n",
      "Epoch 4500, Loss: 0.06319334357976913\n",
      "Epoch 4999, Loss: 0.06583644449710846\n",
      "Epoch 0, Loss: 0.39439234137535095\n",
      "Epoch 500, Loss: 0.06345442682504654\n",
      "Epoch 1000, Loss: 0.0612705796957016\n",
      "Epoch 1500, Loss: 0.0533330999314785\n",
      "Epoch 2000, Loss: 0.06622921675443649\n",
      "Epoch 2500, Loss: 0.06117919087409973\n",
      "Epoch 3000, Loss: 0.06094319373369217\n",
      "Epoch 3500, Loss: 0.05285021662712097\n",
      "Epoch 4000, Loss: 0.05777654051780701\n",
      "Epoch 4500, Loss: 0.0576946958899498\n",
      "Epoch 4999, Loss: 0.05850761756300926\n",
      "--- Experiment 8/20 ---\n",
      "Epoch 0, Loss: 0.5462583303451538\n",
      "Epoch 500, Loss: 0.05995266139507294\n",
      "Epoch 1000, Loss: 0.05399997532367706\n",
      "Epoch 1500, Loss: 0.052351050078868866\n",
      "Epoch 2000, Loss: 0.05123187601566315\n",
      "Epoch 2500, Loss: 0.051006246358156204\n",
      "Epoch 3000, Loss: 0.05204091593623161\n",
      "Epoch 3500, Loss: 0.05171710252761841\n",
      "Epoch 4000, Loss: 0.04841027408838272\n",
      "Epoch 4500, Loss: 0.04748314246535301\n",
      "Epoch 4999, Loss: 0.04898480325937271\n",
      "Epoch 0, Loss: 0.3692631721496582\n",
      "Epoch 500, Loss: 0.08911696076393127\n",
      "Epoch 1000, Loss: 0.08306056261062622\n",
      "Epoch 1500, Loss: 0.0829300582408905\n",
      "Epoch 2000, Loss: 0.0797799825668335\n",
      "Epoch 2500, Loss: 0.07878953963518143\n",
      "Epoch 3000, Loss: 0.07499177753925323\n",
      "Epoch 3500, Loss: 0.08511674404144287\n",
      "Epoch 4000, Loss: 0.07237066328525543\n",
      "Epoch 4500, Loss: 0.07273690402507782\n",
      "Epoch 4999, Loss: 0.07256210595369339\n",
      "Epoch 0, Loss: 0.5744948983192444\n",
      "Epoch 500, Loss: 0.07391683757305145\n",
      "Epoch 1000, Loss: 0.06789073348045349\n",
      "Epoch 1500, Loss: 0.06428918242454529\n",
      "Epoch 2000, Loss: 0.06235126778483391\n",
      "Epoch 2500, Loss: 0.06394001841545105\n",
      "Epoch 3000, Loss: 0.06324364244937897\n",
      "Epoch 3500, Loss: 0.06271081417798996\n",
      "Epoch 4000, Loss: 0.05999816209077835\n",
      "Epoch 4500, Loss: 0.058322083204984665\n",
      "Epoch 4999, Loss: 0.060180723667144775\n",
      "Epoch 0, Loss: 0.3614533543586731\n",
      "Epoch 500, Loss: 0.06786838173866272\n",
      "Epoch 1000, Loss: 0.0619860514998436\n",
      "Epoch 1500, Loss: 0.06010504812002182\n",
      "Epoch 2000, Loss: 0.05305573344230652\n",
      "Epoch 2500, Loss: 0.05703936889767647\n",
      "Epoch 3000, Loss: 0.06583499163389206\n",
      "Epoch 3500, Loss: 0.05855771154165268\n",
      "Epoch 4000, Loss: 0.057682085782289505\n",
      "Epoch 4500, Loss: 0.056938253343105316\n",
      "Epoch 4999, Loss: 0.05534939467906952\n",
      "--- Experiment 9/20 ---\n",
      "Epoch 0, Loss: 0.3933251202106476\n",
      "Epoch 500, Loss: 0.056187570095062256\n",
      "Epoch 1000, Loss: 0.05540237948298454\n",
      "Epoch 1500, Loss: 0.052992597222328186\n",
      "Epoch 2000, Loss: 0.05485186353325844\n",
      "Epoch 2500, Loss: 0.05335308611392975\n",
      "Epoch 3000, Loss: 0.05240652710199356\n",
      "Epoch 3500, Loss: 0.05360882729291916\n",
      "Epoch 4000, Loss: 0.04882021248340607\n",
      "Epoch 4500, Loss: 0.0494893454015255\n",
      "Epoch 4999, Loss: 0.04741763323545456\n",
      "Epoch 0, Loss: 0.6566084027290344\n",
      "Epoch 500, Loss: 0.08442641794681549\n",
      "Epoch 1000, Loss: 0.08143375813961029\n",
      "Epoch 1500, Loss: 0.07924579083919525\n",
      "Epoch 2000, Loss: 0.07600048929452896\n",
      "Epoch 2500, Loss: 0.07689293473958969\n",
      "Epoch 3000, Loss: 0.08224925398826599\n",
      "Epoch 3500, Loss: 0.07591944187879562\n",
      "Epoch 4000, Loss: 0.07428587228059769\n",
      "Epoch 4500, Loss: 0.0771561786532402\n",
      "Epoch 4999, Loss: 0.07526834309101105\n",
      "Epoch 0, Loss: 0.5421320199966431\n",
      "Epoch 500, Loss: 0.07465705275535583\n",
      "Epoch 1000, Loss: 0.06750503927469254\n",
      "Epoch 1500, Loss: 0.06475996226072311\n",
      "Epoch 2000, Loss: 0.06399056315422058\n",
      "Epoch 2500, Loss: 0.06025530397891998\n",
      "Epoch 3000, Loss: 0.06244693323969841\n",
      "Epoch 3500, Loss: 0.06712031364440918\n",
      "Epoch 4000, Loss: 0.06881687045097351\n",
      "Epoch 4500, Loss: 0.061551306396722794\n",
      "Epoch 4999, Loss: 0.059098318219184875\n",
      "Epoch 0, Loss: 0.5640585422515869\n",
      "Epoch 500, Loss: 0.06935348361730576\n",
      "Epoch 1000, Loss: 0.06432230770587921\n",
      "Epoch 1500, Loss: 0.0613618902862072\n",
      "Epoch 2000, Loss: 0.0604083389043808\n",
      "Epoch 2500, Loss: 0.06026015430688858\n",
      "Epoch 3000, Loss: 0.0566171295940876\n",
      "Epoch 3500, Loss: 0.05867108702659607\n",
      "Epoch 4000, Loss: 0.0592823401093483\n",
      "Epoch 4500, Loss: 0.05837329104542732\n",
      "Epoch 4999, Loss: 0.05883065611124039\n",
      "--- Experiment 10/20 ---\n",
      "Epoch 0, Loss: 0.4097002148628235\n",
      "Epoch 500, Loss: 0.05472454056143761\n",
      "Epoch 1000, Loss: 0.05566399544477463\n",
      "Epoch 1500, Loss: 0.053638752549886703\n",
      "Epoch 2000, Loss: 0.050477661192417145\n",
      "Epoch 2500, Loss: 0.05032765865325928\n",
      "Epoch 3000, Loss: 0.05220535397529602\n",
      "Epoch 3500, Loss: 0.04754989966750145\n",
      "Epoch 4000, Loss: 0.048828598111867905\n",
      "Epoch 4500, Loss: 0.049874402582645416\n",
      "Epoch 4999, Loss: 0.04940538853406906\n",
      "Epoch 0, Loss: 0.6262241005897522\n",
      "Epoch 500, Loss: 0.0896754115819931\n",
      "Epoch 1000, Loss: 0.07932419329881668\n",
      "Epoch 1500, Loss: 0.07840773463249207\n",
      "Epoch 2000, Loss: 0.07655016332864761\n",
      "Epoch 2500, Loss: 0.07562585920095444\n",
      "Epoch 3000, Loss: 0.07459260523319244\n",
      "Epoch 3500, Loss: 0.07150442898273468\n",
      "Epoch 4000, Loss: 0.0732908844947815\n",
      "Epoch 4500, Loss: 0.07266706973314285\n",
      "Epoch 4999, Loss: 0.0726793184876442\n",
      "Epoch 0, Loss: 0.4996761679649353\n",
      "Epoch 500, Loss: 0.0773874893784523\n",
      "Epoch 1000, Loss: 0.06919501721858978\n",
      "Epoch 1500, Loss: 0.06595596671104431\n",
      "Epoch 2000, Loss: 0.06436293572187424\n",
      "Epoch 2500, Loss: 0.060525499284267426\n",
      "Epoch 3000, Loss: 0.06031731143593788\n",
      "Epoch 3500, Loss: 0.07652914524078369\n",
      "Epoch 4000, Loss: 0.06775067001581192\n",
      "Epoch 4500, Loss: 0.06702890992164612\n",
      "Epoch 4999, Loss: 0.06743581593036652\n",
      "Epoch 0, Loss: 0.44057199358940125\n",
      "Epoch 500, Loss: 0.06745190173387527\n",
      "Epoch 1000, Loss: 0.05874188244342804\n",
      "Epoch 1500, Loss: 0.06111781671643257\n",
      "Epoch 2000, Loss: 0.060038384050130844\n",
      "Epoch 2500, Loss: 0.05741613358259201\n",
      "Epoch 3000, Loss: 0.05837806686758995\n",
      "Epoch 3500, Loss: 0.059796445071697235\n",
      "Epoch 4000, Loss: 0.06144528090953827\n",
      "Epoch 4500, Loss: 0.05650484561920166\n",
      "Epoch 4999, Loss: 0.05513520538806915\n",
      "--- Experiment 11/20 ---\n",
      "Epoch 0, Loss: 0.40554261207580566\n",
      "Epoch 500, Loss: 0.05369389057159424\n",
      "Epoch 1000, Loss: 0.05395340174436569\n",
      "Epoch 1500, Loss: 0.052274543792009354\n",
      "Epoch 2000, Loss: 0.05429219454526901\n",
      "Epoch 2500, Loss: 0.049896255135536194\n",
      "Epoch 3000, Loss: 0.05139634758234024\n",
      "Epoch 3500, Loss: 0.04898707568645477\n",
      "Epoch 4000, Loss: 0.051570601761341095\n",
      "Epoch 4500, Loss: 0.044543348252773285\n",
      "Epoch 4999, Loss: 0.049623578786849976\n",
      "Epoch 0, Loss: 0.5616986155509949\n",
      "Epoch 500, Loss: 0.08727869391441345\n",
      "Epoch 1000, Loss: 0.08125518262386322\n",
      "Epoch 1500, Loss: 0.07770252227783203\n",
      "Epoch 2000, Loss: 0.07662198692560196\n",
      "Epoch 2500, Loss: 0.07653409987688065\n",
      "Epoch 3000, Loss: 0.07811000943183899\n",
      "Epoch 3500, Loss: 0.0735582783818245\n",
      "Epoch 4000, Loss: 0.0763385072350502\n",
      "Epoch 4500, Loss: 0.07403334975242615\n",
      "Epoch 4999, Loss: 0.0738656297326088\n",
      "Epoch 0, Loss: 0.5126072764396667\n",
      "Epoch 500, Loss: 0.07284892350435257\n",
      "Epoch 1000, Loss: 0.06818694621324539\n",
      "Epoch 1500, Loss: 0.06454852968454361\n",
      "Epoch 2000, Loss: 0.06376128643751144\n",
      "Epoch 2500, Loss: 0.06294584274291992\n",
      "Epoch 3000, Loss: 0.06394288688898087\n",
      "Epoch 3500, Loss: 0.06088182330131531\n",
      "Epoch 4000, Loss: 0.05856027826666832\n",
      "Epoch 4500, Loss: 0.0625523030757904\n",
      "Epoch 4999, Loss: 0.06243598833680153\n",
      "Epoch 0, Loss: 0.5661070346832275\n",
      "Epoch 500, Loss: 0.0662800595164299\n",
      "Epoch 1000, Loss: 0.06540217250585556\n",
      "Epoch 1500, Loss: 0.05990641564130783\n",
      "Epoch 2000, Loss: 0.061079010367393494\n",
      "Epoch 2500, Loss: 0.059576038271188736\n",
      "Epoch 3000, Loss: 0.05668838694691658\n",
      "Epoch 3500, Loss: 0.057993773370981216\n",
      "Epoch 4000, Loss: 0.05781240016222\n",
      "Epoch 4500, Loss: 0.06022866070270538\n",
      "Epoch 4999, Loss: 0.05576205998659134\n",
      "--- Experiment 12/20 ---\n",
      "Epoch 0, Loss: 0.5594822764396667\n",
      "Epoch 500, Loss: 0.059893183410167694\n",
      "Epoch 1000, Loss: 0.05647443234920502\n",
      "Epoch 1500, Loss: 0.05299040675163269\n",
      "Epoch 2000, Loss: 0.0500868521630764\n",
      "Epoch 2500, Loss: 0.05072440952062607\n",
      "Epoch 3000, Loss: 0.0518183633685112\n",
      "Epoch 3500, Loss: 0.05057181417942047\n",
      "Epoch 4000, Loss: 0.04837654531002045\n",
      "Epoch 4500, Loss: 0.050660640001297\n",
      "Epoch 4999, Loss: 0.0465623140335083\n",
      "Epoch 0, Loss: 0.521632194519043\n",
      "Epoch 500, Loss: 0.0858209878206253\n",
      "Epoch 1000, Loss: 0.07942143082618713\n",
      "Epoch 1500, Loss: 0.07849033921957016\n",
      "Epoch 2000, Loss: 0.07572883367538452\n",
      "Epoch 2500, Loss: 0.0751822292804718\n",
      "Epoch 3000, Loss: 0.07409825921058655\n",
      "Epoch 3500, Loss: 0.07441707700490952\n",
      "Epoch 4000, Loss: 0.07308528572320938\n",
      "Epoch 4500, Loss: 0.07159370929002762\n",
      "Epoch 4999, Loss: 0.07493553310632706\n",
      "Epoch 0, Loss: 0.6954922676086426\n",
      "Epoch 500, Loss: 0.07976275682449341\n",
      "Epoch 1000, Loss: 0.07159039378166199\n",
      "Epoch 1500, Loss: 0.06565987318754196\n",
      "Epoch 2000, Loss: 0.06987632066011429\n",
      "Epoch 2500, Loss: 0.06376476585865021\n",
      "Epoch 3000, Loss: 0.0634627714753151\n",
      "Epoch 3500, Loss: 0.06745180487632751\n",
      "Epoch 4000, Loss: 0.06583087146282196\n",
      "Epoch 4500, Loss: 0.06529521942138672\n",
      "Epoch 4999, Loss: 0.06474150717258453\n",
      "Epoch 0, Loss: 0.5458282828330994\n",
      "Epoch 500, Loss: 0.06838564574718475\n",
      "Epoch 1000, Loss: 0.059966765344142914\n",
      "Epoch 1500, Loss: 0.0606406033039093\n",
      "Epoch 2000, Loss: 0.06111462414264679\n",
      "Epoch 2500, Loss: 0.06369249522686005\n",
      "Epoch 3000, Loss: 0.055537253618240356\n",
      "Epoch 3500, Loss: 0.06091270595788956\n",
      "Epoch 4000, Loss: 0.05556841939687729\n",
      "Epoch 4500, Loss: 0.05482959747314453\n",
      "Epoch 4999, Loss: 0.05746389925479889\n",
      "--- Experiment 13/20 ---\n",
      "Epoch 0, Loss: 0.4262807369232178\n",
      "Epoch 500, Loss: 0.05833391100168228\n",
      "Epoch 1000, Loss: 0.05500951409339905\n",
      "Epoch 1500, Loss: 0.05268926918506622\n",
      "Epoch 2000, Loss: 0.05208861827850342\n",
      "Epoch 2500, Loss: 0.048854462802410126\n",
      "Epoch 3000, Loss: 0.048066798597574234\n",
      "Epoch 3500, Loss: 0.050371624529361725\n",
      "Epoch 4000, Loss: 0.05026163533329964\n",
      "Epoch 4500, Loss: 0.049529243260622025\n",
      "Epoch 4999, Loss: 0.04824803024530411\n",
      "Epoch 0, Loss: 0.5691907405853271\n",
      "Epoch 500, Loss: 0.08704110980033875\n",
      "Epoch 1000, Loss: 0.08294723182916641\n",
      "Epoch 1500, Loss: 0.08040238171815872\n",
      "Epoch 2000, Loss: 0.07566747814416885\n",
      "Epoch 2500, Loss: 0.07753634452819824\n",
      "Epoch 3000, Loss: 0.07836751639842987\n",
      "Epoch 3500, Loss: 0.07483203709125519\n",
      "Epoch 4000, Loss: 0.07272110134363174\n",
      "Epoch 4500, Loss: 0.07302263379096985\n",
      "Epoch 4999, Loss: 0.07241801172494888\n",
      "Epoch 0, Loss: 0.5369297862052917\n",
      "Epoch 500, Loss: 0.07597951591014862\n",
      "Epoch 1000, Loss: 0.06698240339756012\n",
      "Epoch 1500, Loss: 0.06057554483413696\n",
      "Epoch 2000, Loss: 0.0641542375087738\n",
      "Epoch 2500, Loss: 0.06212269514799118\n",
      "Epoch 3000, Loss: 0.06328271329402924\n",
      "Epoch 3500, Loss: 0.06257107853889465\n",
      "Epoch 4000, Loss: 0.06219561770558357\n",
      "Epoch 4500, Loss: 0.062130093574523926\n",
      "Epoch 4999, Loss: 0.060404691845178604\n",
      "Epoch 0, Loss: 0.3862171471118927\n",
      "Epoch 500, Loss: 0.0626361146569252\n",
      "Epoch 1000, Loss: 0.06299956887960434\n",
      "Epoch 1500, Loss: 0.06006617099046707\n",
      "Epoch 2000, Loss: 0.05605632811784744\n",
      "Epoch 2500, Loss: 0.058318980038166046\n",
      "Epoch 3000, Loss: 0.06021646410226822\n",
      "Epoch 3500, Loss: 0.05663641169667244\n",
      "Epoch 4000, Loss: 0.05741509050130844\n",
      "Epoch 4500, Loss: 0.057103388011455536\n",
      "Epoch 4999, Loss: 0.0578780397772789\n",
      "--- Experiment 14/20 ---\n",
      "Epoch 0, Loss: 0.42251741886138916\n",
      "Epoch 500, Loss: 0.056989409029483795\n",
      "Epoch 1000, Loss: 0.05703126639127731\n",
      "Epoch 1500, Loss: 0.05130120366811752\n",
      "Epoch 2000, Loss: 0.05314427614212036\n",
      "Epoch 2500, Loss: 0.04955007880926132\n",
      "Epoch 3000, Loss: 0.049062952399253845\n",
      "Epoch 3500, Loss: 0.050561923533678055\n",
      "Epoch 4000, Loss: 0.05169709026813507\n",
      "Epoch 4500, Loss: 0.051750198006629944\n",
      "Epoch 4999, Loss: 0.04793471097946167\n",
      "Epoch 0, Loss: 0.5339180827140808\n",
      "Epoch 500, Loss: 0.0892849713563919\n",
      "Epoch 1000, Loss: 0.08151184767484665\n",
      "Epoch 1500, Loss: 0.08114227652549744\n",
      "Epoch 2000, Loss: 0.08164297789335251\n",
      "Epoch 2500, Loss: 0.08037810027599335\n",
      "Epoch 3000, Loss: 0.07732836157083511\n",
      "Epoch 3500, Loss: 0.07530353963375092\n",
      "Epoch 4000, Loss: 0.08496701717376709\n",
      "Epoch 4500, Loss: 0.0748884305357933\n",
      "Epoch 4999, Loss: 0.07646509259939194\n",
      "Epoch 0, Loss: 0.5773341059684753\n",
      "Epoch 500, Loss: 0.08347582817077637\n",
      "Epoch 1000, Loss: 0.06822355091571808\n",
      "Epoch 1500, Loss: 0.06286226958036423\n",
      "Epoch 2000, Loss: 0.06448908895254135\n",
      "Epoch 2500, Loss: 0.0641874149441719\n",
      "Epoch 3000, Loss: 0.0642392486333847\n",
      "Epoch 3500, Loss: 0.06295032799243927\n",
      "Epoch 4000, Loss: 0.07136710733175278\n",
      "Epoch 4500, Loss: 0.0656076967716217\n",
      "Epoch 4999, Loss: 0.06531523168087006\n",
      "Epoch 0, Loss: 0.659034788608551\n",
      "Epoch 500, Loss: 0.06588412076234818\n",
      "Epoch 1000, Loss: 0.06231958046555519\n",
      "Epoch 1500, Loss: 0.05767039954662323\n",
      "Epoch 2000, Loss: 0.06185982748866081\n",
      "Epoch 2500, Loss: 0.056821372359991074\n",
      "Epoch 3000, Loss: 0.06048441678285599\n",
      "Epoch 3500, Loss: 0.058959417045116425\n",
      "Epoch 4000, Loss: 0.057014476507902145\n",
      "Epoch 4500, Loss: 0.05691187083721161\n",
      "Epoch 4999, Loss: 0.05353996902704239\n",
      "--- Experiment 15/20 ---\n",
      "Epoch 0, Loss: 0.44039061665534973\n",
      "Epoch 500, Loss: 0.05616798251867294\n",
      "Epoch 1000, Loss: 0.051468633115291595\n",
      "Epoch 1500, Loss: 0.05137141793966293\n",
      "Epoch 2000, Loss: 0.05265030264854431\n",
      "Epoch 2500, Loss: 0.051894769072532654\n",
      "Epoch 3000, Loss: 0.051712214946746826\n",
      "Epoch 3500, Loss: 0.050169795751571655\n",
      "Epoch 4000, Loss: 0.048604559153318405\n",
      "Epoch 4500, Loss: 0.04914864897727966\n",
      "Epoch 4999, Loss: 0.049216873943805695\n",
      "Epoch 0, Loss: 0.46989136934280396\n",
      "Epoch 500, Loss: 0.08643541485071182\n",
      "Epoch 1000, Loss: 0.08174897730350494\n",
      "Epoch 1500, Loss: 0.08181953430175781\n",
      "Epoch 2000, Loss: 0.07465659081935883\n",
      "Epoch 2500, Loss: 0.07431904226541519\n",
      "Epoch 3000, Loss: 0.07252070307731628\n",
      "Epoch 3500, Loss: 0.07456955313682556\n",
      "Epoch 4000, Loss: 0.07189544290304184\n",
      "Epoch 4500, Loss: 0.07489040493965149\n",
      "Epoch 4999, Loss: 0.07057537138462067\n",
      "Epoch 0, Loss: 0.4995678961277008\n",
      "Epoch 500, Loss: 0.07618548721075058\n",
      "Epoch 1000, Loss: 0.0692305639386177\n",
      "Epoch 1500, Loss: 0.06265632808208466\n",
      "Epoch 2000, Loss: 0.06331420689821243\n",
      "Epoch 2500, Loss: 0.06460955739021301\n",
      "Epoch 3000, Loss: 0.06246010214090347\n",
      "Epoch 3500, Loss: 0.06192782521247864\n",
      "Epoch 4000, Loss: 0.06335625052452087\n",
      "Epoch 4500, Loss: 0.05919788032770157\n",
      "Epoch 4999, Loss: 0.061931002885103226\n",
      "Epoch 0, Loss: 0.5637729167938232\n",
      "Epoch 500, Loss: 0.06583626568317413\n",
      "Epoch 1000, Loss: 0.06683612614870071\n",
      "Epoch 1500, Loss: 0.059134453535079956\n",
      "Epoch 2000, Loss: 0.06116413325071335\n",
      "Epoch 2500, Loss: 0.05848117917776108\n",
      "Epoch 3000, Loss: 0.054982349276542664\n",
      "Epoch 3500, Loss: 0.054394155740737915\n",
      "Epoch 4000, Loss: 0.060204315930604935\n",
      "Epoch 4500, Loss: 0.056439079344272614\n",
      "Epoch 4999, Loss: 0.06416179239749908\n",
      "--- Experiment 16/20 ---\n",
      "Epoch 0, Loss: 0.5071008801460266\n",
      "Epoch 500, Loss: 0.05831562355160713\n",
      "Epoch 1000, Loss: 0.05294378846883774\n",
      "Epoch 1500, Loss: 0.052310578525066376\n",
      "Epoch 2000, Loss: 0.05094320327043533\n",
      "Epoch 2500, Loss: 0.04978429898619652\n",
      "Epoch 3000, Loss: 0.05040276423096657\n",
      "Epoch 3500, Loss: 0.0489046536386013\n",
      "Epoch 4000, Loss: 0.048449281603097916\n",
      "Epoch 4500, Loss: 0.047117412090301514\n",
      "Epoch 4999, Loss: 0.05045546218752861\n",
      "Epoch 0, Loss: 0.5344859957695007\n",
      "Epoch 500, Loss: 0.08216647058725357\n",
      "Epoch 1000, Loss: 0.07945797592401505\n",
      "Epoch 1500, Loss: 0.07729719579219818\n",
      "Epoch 2000, Loss: 0.07527846097946167\n",
      "Epoch 2500, Loss: 0.07299802452325821\n",
      "Epoch 3000, Loss: 0.0748634785413742\n",
      "Epoch 3500, Loss: 0.07510370016098022\n",
      "Epoch 4000, Loss: 0.07336979359388351\n",
      "Epoch 4500, Loss: 0.07331156730651855\n",
      "Epoch 4999, Loss: 0.07436908781528473\n",
      "Epoch 0, Loss: 0.3771807551383972\n",
      "Epoch 500, Loss: 0.07167825102806091\n",
      "Epoch 1000, Loss: 0.06619249284267426\n",
      "Epoch 1500, Loss: 0.06533919274806976\n",
      "Epoch 2000, Loss: 0.0648159310221672\n",
      "Epoch 2500, Loss: 0.06347915530204773\n",
      "Epoch 3000, Loss: 0.06005210429430008\n",
      "Epoch 3500, Loss: 0.062101610004901886\n",
      "Epoch 4000, Loss: 0.06263558566570282\n",
      "Epoch 4500, Loss: 0.06801515072584152\n",
      "Epoch 4999, Loss: 0.06603562831878662\n",
      "Epoch 0, Loss: 0.34977152943611145\n",
      "Epoch 500, Loss: 0.06882348656654358\n",
      "Epoch 1000, Loss: 0.05087583512067795\n",
      "Epoch 1500, Loss: 0.051465608179569244\n",
      "Epoch 2000, Loss: 0.06126229465007782\n",
      "Epoch 2500, Loss: 0.0565510019659996\n",
      "Epoch 3000, Loss: 0.05855431407690048\n",
      "Epoch 3500, Loss: 0.06030617654323578\n",
      "Epoch 4000, Loss: 0.055755238980054855\n",
      "Epoch 4500, Loss: 0.05993645265698433\n",
      "Epoch 4999, Loss: 0.05794395133852959\n",
      "--- Experiment 17/20 ---\n",
      "Epoch 0, Loss: 0.5214803814888\n",
      "Epoch 500, Loss: 0.05911451578140259\n",
      "Epoch 1000, Loss: 0.04977338761091232\n",
      "Epoch 1500, Loss: 0.05302661657333374\n",
      "Epoch 2000, Loss: 0.05062779784202576\n",
      "Epoch 2500, Loss: 0.05038940906524658\n",
      "Epoch 3000, Loss: 0.052247304469347\n",
      "Epoch 3500, Loss: 0.04962123930454254\n",
      "Epoch 4000, Loss: 0.05065273120999336\n",
      "Epoch 4500, Loss: 0.045853063464164734\n",
      "Epoch 4999, Loss: 0.05032256245613098\n",
      "Epoch 0, Loss: 0.5012781023979187\n",
      "Epoch 500, Loss: 0.08544039726257324\n",
      "Epoch 1000, Loss: 0.08523710072040558\n",
      "Epoch 1500, Loss: 0.08070729672908783\n",
      "Epoch 2000, Loss: 0.07646284997463226\n",
      "Epoch 2500, Loss: 0.07990717887878418\n",
      "Epoch 3000, Loss: 0.07426411658525467\n",
      "Epoch 3500, Loss: 0.07255200296640396\n",
      "Epoch 4000, Loss: 0.07705622166395187\n",
      "Epoch 4500, Loss: 0.07476001232862473\n",
      "Epoch 4999, Loss: 0.07533641904592514\n",
      "Epoch 0, Loss: 0.35889315605163574\n",
      "Epoch 500, Loss: 0.07474295794963837\n",
      "Epoch 1000, Loss: 0.07124188542366028\n",
      "Epoch 1500, Loss: 0.0615319088101387\n",
      "Epoch 2000, Loss: 0.0631488785147667\n",
      "Epoch 2500, Loss: 0.06403733789920807\n",
      "Epoch 3000, Loss: 0.06294458359479904\n",
      "Epoch 3500, Loss: 0.06117430329322815\n",
      "Epoch 4000, Loss: 0.056410323828458786\n",
      "Epoch 4500, Loss: 0.06539250910282135\n",
      "Epoch 4999, Loss: 0.06631850451231003\n",
      "Epoch 0, Loss: 0.5038313269615173\n",
      "Epoch 500, Loss: 0.06749411672353745\n",
      "Epoch 1000, Loss: 0.06354911625385284\n",
      "Epoch 1500, Loss: 0.059631697833538055\n",
      "Epoch 2000, Loss: 0.05673767626285553\n",
      "Epoch 2500, Loss: 0.05883478373289108\n",
      "Epoch 3000, Loss: 0.054390035569667816\n",
      "Epoch 3500, Loss: 0.06020932272076607\n",
      "Epoch 4000, Loss: 0.059183426201343536\n",
      "Epoch 4500, Loss: 0.057029880583286285\n",
      "Epoch 4999, Loss: 0.06071702390909195\n",
      "--- Experiment 18/20 ---\n",
      "Epoch 0, Loss: 0.5093448758125305\n",
      "Epoch 500, Loss: 0.05501794442534447\n",
      "Epoch 1000, Loss: 0.055563028901815414\n",
      "Epoch 1500, Loss: 0.05102454125881195\n",
      "Epoch 2000, Loss: 0.050302691757678986\n",
      "Epoch 2500, Loss: 0.054025162011384964\n",
      "Epoch 3000, Loss: 0.047055769711732864\n",
      "Epoch 3500, Loss: 0.05083971470594406\n",
      "Epoch 4000, Loss: 0.049598194658756256\n",
      "Epoch 4500, Loss: 0.048848945647478104\n",
      "Epoch 4999, Loss: 0.04725068807601929\n",
      "Epoch 0, Loss: 0.42478835582733154\n",
      "Epoch 500, Loss: 0.08560565114021301\n",
      "Epoch 1000, Loss: 0.07889371365308762\n",
      "Epoch 1500, Loss: 0.07519044727087021\n",
      "Epoch 2000, Loss: 0.07590015232563019\n",
      "Epoch 2500, Loss: 0.0751352310180664\n",
      "Epoch 3000, Loss: 0.07479813694953918\n",
      "Epoch 3500, Loss: 0.07895504683256149\n",
      "Epoch 4000, Loss: 0.07377301901578903\n",
      "Epoch 4500, Loss: 0.07258045673370361\n",
      "Epoch 4999, Loss: 0.07189348340034485\n",
      "Epoch 0, Loss: 0.36867210268974304\n",
      "Epoch 500, Loss: 0.07358101010322571\n",
      "Epoch 1000, Loss: 0.0676325187087059\n",
      "Epoch 1500, Loss: 0.06709934771060944\n",
      "Epoch 2000, Loss: 0.06231699883937836\n",
      "Epoch 2500, Loss: 0.06628246605396271\n",
      "Epoch 3000, Loss: 0.06099478155374527\n",
      "Epoch 3500, Loss: 0.06569959223270416\n",
      "Epoch 4000, Loss: 0.06414555013179779\n",
      "Epoch 4500, Loss: 0.062474921345710754\n",
      "Epoch 4999, Loss: 0.06278032809495926\n",
      "Epoch 0, Loss: 0.6368719935417175\n",
      "Epoch 500, Loss: 0.06851846724748611\n",
      "Epoch 1000, Loss: 0.061768464744091034\n",
      "Epoch 1500, Loss: 0.06383509188890457\n",
      "Epoch 2000, Loss: 0.06156119331717491\n",
      "Epoch 2500, Loss: 0.056103192269802094\n",
      "Epoch 3000, Loss: 0.059930361807346344\n",
      "Epoch 3500, Loss: 0.05969737097620964\n",
      "Epoch 4000, Loss: 0.059478871524333954\n",
      "Epoch 4500, Loss: 0.05479016900062561\n",
      "Epoch 4999, Loss: 0.058186084032058716\n",
      "--- Experiment 19/20 ---\n",
      "Epoch 0, Loss: 0.627493143081665\n",
      "Epoch 500, Loss: 0.05782611668109894\n",
      "Epoch 1000, Loss: 0.05347510427236557\n",
      "Epoch 1500, Loss: 0.053491897881031036\n",
      "Epoch 2000, Loss: 0.054688841104507446\n",
      "Epoch 2500, Loss: 0.05145374685525894\n",
      "Epoch 3000, Loss: 0.04884577542543411\n",
      "Epoch 3500, Loss: 0.0529937706887722\n",
      "Epoch 4000, Loss: 0.04719216749072075\n",
      "Epoch 4500, Loss: 0.05123833194375038\n",
      "Epoch 4999, Loss: 0.04650014266371727\n",
      "Epoch 0, Loss: 0.3921359181404114\n",
      "Epoch 500, Loss: 0.08462986350059509\n",
      "Epoch 1000, Loss: 0.08678300678730011\n",
      "Epoch 1500, Loss: 0.07882516086101532\n",
      "Epoch 2000, Loss: 0.07834494113922119\n",
      "Epoch 2500, Loss: 0.07628795504570007\n",
      "Epoch 3000, Loss: 0.07382254302501678\n",
      "Epoch 3500, Loss: 0.07353170961141586\n",
      "Epoch 4000, Loss: 0.07260718941688538\n",
      "Epoch 4500, Loss: 0.07358424365520477\n",
      "Epoch 4999, Loss: 0.07676072418689728\n",
      "Epoch 0, Loss: 0.5198906660079956\n",
      "Epoch 500, Loss: 0.07974134385585785\n",
      "Epoch 1000, Loss: 0.07114913314580917\n",
      "Epoch 1500, Loss: 0.0662510022521019\n",
      "Epoch 2000, Loss: 0.06750942766666412\n",
      "Epoch 2500, Loss: 0.0637623518705368\n",
      "Epoch 3000, Loss: 0.06299692392349243\n",
      "Epoch 3500, Loss: 0.07321345806121826\n",
      "Epoch 4000, Loss: 0.06528535485267639\n",
      "Epoch 4500, Loss: 0.06468204408884048\n",
      "Epoch 4999, Loss: 0.06728348135948181\n",
      "Epoch 0, Loss: 0.45368391275405884\n",
      "Epoch 500, Loss: 0.06718475371599197\n",
      "Epoch 1000, Loss: 0.060834046453237534\n",
      "Epoch 1500, Loss: 0.06196657940745354\n",
      "Epoch 2000, Loss: 0.06393583118915558\n",
      "Epoch 2500, Loss: 0.06299666315317154\n",
      "Epoch 3000, Loss: 0.0569777637720108\n",
      "Epoch 3500, Loss: 0.06000915542244911\n",
      "Epoch 4000, Loss: 0.05858899652957916\n",
      "Epoch 4500, Loss: 0.05567259341478348\n",
      "Epoch 4999, Loss: 0.06130107119679451\n",
      "--- Experiment 20/20 ---\n",
      "Epoch 0, Loss: 0.3812252879142761\n",
      "Epoch 500, Loss: 0.05885418504476547\n",
      "Epoch 1000, Loss: 0.05582863837480545\n",
      "Epoch 1500, Loss: 0.05330890417098999\n",
      "Epoch 2000, Loss: 0.05076617747545242\n",
      "Epoch 2500, Loss: 0.05191035568714142\n",
      "Epoch 3000, Loss: 0.05130612105131149\n",
      "Epoch 3500, Loss: 0.050852201879024506\n",
      "Epoch 4000, Loss: 0.05005063861608505\n",
      "Epoch 4500, Loss: 0.05133727192878723\n",
      "Epoch 4999, Loss: 0.04847697168588638\n",
      "Epoch 0, Loss: 0.3569607734680176\n",
      "Epoch 500, Loss: 0.0851975530385971\n",
      "Epoch 1000, Loss: 0.0788075253367424\n",
      "Epoch 1500, Loss: 0.07652482390403748\n",
      "Epoch 2000, Loss: 0.07800878584384918\n",
      "Epoch 2500, Loss: 0.07513868808746338\n",
      "Epoch 3000, Loss: 0.07502840459346771\n",
      "Epoch 3500, Loss: 0.07460818439722061\n",
      "Epoch 4000, Loss: 0.07116648554801941\n",
      "Epoch 4500, Loss: 0.07307510077953339\n",
      "Epoch 4999, Loss: 0.07277578115463257\n",
      "Epoch 0, Loss: 0.47043734788894653\n",
      "Epoch 500, Loss: 0.07508981972932816\n",
      "Epoch 1000, Loss: 0.06935261189937592\n",
      "Epoch 1500, Loss: 0.06465163081884384\n",
      "Epoch 2000, Loss: 0.0644911378622055\n",
      "Epoch 2500, Loss: 0.06636612117290497\n",
      "Epoch 3000, Loss: 0.06649425625801086\n",
      "Epoch 3500, Loss: 0.06419902294874191\n",
      "Epoch 4000, Loss: 0.06310141086578369\n",
      "Epoch 4500, Loss: 0.06285843253135681\n",
      "Epoch 4999, Loss: 0.06496480107307434\n",
      "Epoch 0, Loss: 0.6098359823226929\n",
      "Epoch 500, Loss: 0.06711658090353012\n",
      "Epoch 1000, Loss: 0.06301328539848328\n",
      "Epoch 1500, Loss: 0.05459221452474594\n",
      "Epoch 2000, Loss: 0.054345257580280304\n",
      "Epoch 2500, Loss: 0.05140607804059982\n",
      "Epoch 3000, Loss: 0.059920988976955414\n",
      "Epoch 3500, Loss: 0.058669477701187134\n",
      "Epoch 4000, Loss: 0.05840669572353363\n",
      "Epoch 4500, Loss: 0.05502482131123543\n",
      "Epoch 4999, Loss: 0.05532069504261017\n",
      "Average Validation Loss (Base):   0.048095 ± 0.000735\n",
      "Average Validation Loss (Import): 0.063659 ± 0.001540\n",
      "Average Validation Loss (GP Out): 0.056300 ± 0.001599\n",
      "Average Validation Loss (GP Res): 0.054435 ± 0.004868\n",
      "\n",
      "==== Running experiments for IC: step, BC: periodic ====\n",
      "--- Experiment 1/20 ---\n",
      "Epoch 0, Loss: 0.4708404541015625\n",
      "Epoch 500, Loss: 0.055940233170986176\n",
      "Epoch 1000, Loss: 0.05228384584188461\n",
      "Epoch 1500, Loss: 0.051525093615055084\n",
      "Epoch 2000, Loss: 0.051566291600465775\n",
      "Epoch 2500, Loss: 0.05083701014518738\n",
      "Epoch 3000, Loss: 0.050151895731687546\n",
      "Epoch 3500, Loss: 0.04960719496011734\n",
      "Epoch 4000, Loss: 0.047651585191488266\n",
      "Epoch 4500, Loss: 0.04832886904478073\n",
      "Epoch 4999, Loss: 0.045303694903850555\n",
      "Epoch 0, Loss: 0.4663698077201843\n",
      "Epoch 500, Loss: 0.090155228972435\n",
      "Epoch 1000, Loss: 0.08486116677522659\n",
      "Epoch 1500, Loss: 0.08038932830095291\n",
      "Epoch 2000, Loss: 0.07901990413665771\n",
      "Epoch 2500, Loss: 0.07587525248527527\n",
      "Epoch 3000, Loss: 0.07378146797418594\n",
      "Epoch 3500, Loss: 0.07482556253671646\n",
      "Epoch 4000, Loss: 0.0762767642736435\n",
      "Epoch 4500, Loss: 0.07392238825559616\n",
      "Epoch 4999, Loss: 0.07427174597978592\n",
      "Epoch 0, Loss: 0.42101284861564636\n",
      "Epoch 500, Loss: 0.07381473481655121\n",
      "Epoch 1000, Loss: 0.06738457083702087\n",
      "Epoch 1500, Loss: 0.06142595037817955\n",
      "Epoch 2000, Loss: 0.06548671424388885\n",
      "Epoch 2500, Loss: 0.05924345552921295\n",
      "Epoch 3000, Loss: 0.062471237033605576\n",
      "Epoch 3500, Loss: 0.07047337293624878\n",
      "Epoch 4000, Loss: 0.061970531940460205\n",
      "Epoch 4500, Loss: 0.06288916617631912\n",
      "Epoch 4999, Loss: 0.06165559962391853\n",
      "Epoch 0, Loss: 0.3465712070465088\n",
      "Epoch 500, Loss: 0.06632797420024872\n",
      "Epoch 1000, Loss: 0.059506431221961975\n",
      "Epoch 1500, Loss: 0.05966171994805336\n",
      "Epoch 2000, Loss: 0.06018870323896408\n",
      "Epoch 2500, Loss: 0.0595870316028595\n",
      "Epoch 3000, Loss: 0.05758178234100342\n",
      "Epoch 3500, Loss: 0.05928576737642288\n",
      "Epoch 4000, Loss: 0.056850943714380264\n",
      "Epoch 4500, Loss: 0.057126447558403015\n",
      "Epoch 4999, Loss: 0.05254366621375084\n",
      "--- Experiment 2/20 ---\n",
      "Epoch 0, Loss: 0.40651413798332214\n",
      "Epoch 500, Loss: 0.056212637573480606\n",
      "Epoch 1000, Loss: 0.053887657821178436\n",
      "Epoch 1500, Loss: 0.04917456582188606\n",
      "Epoch 2000, Loss: 0.04955241084098816\n",
      "Epoch 2500, Loss: 0.05443798005580902\n",
      "Epoch 3000, Loss: 0.050152719020843506\n",
      "Epoch 3500, Loss: 0.04787255823612213\n",
      "Epoch 4000, Loss: 0.04747386276721954\n",
      "Epoch 4500, Loss: 0.05023563653230667\n",
      "Epoch 4999, Loss: 0.04741717875003815\n",
      "Epoch 0, Loss: 0.42057371139526367\n",
      "Epoch 500, Loss: 0.08331969380378723\n",
      "Epoch 1000, Loss: 0.08043860644102097\n",
      "Epoch 1500, Loss: 0.07860541343688965\n",
      "Epoch 2000, Loss: 0.07686886936426163\n",
      "Epoch 2500, Loss: 0.07437452673912048\n",
      "Epoch 3000, Loss: 0.0732286125421524\n",
      "Epoch 3500, Loss: 0.0737944096326828\n",
      "Epoch 4000, Loss: 0.07318433374166489\n",
      "Epoch 4500, Loss: 0.07382547855377197\n",
      "Epoch 4999, Loss: 0.07124976068735123\n",
      "Epoch 0, Loss: 0.36808475852012634\n",
      "Epoch 500, Loss: 0.07400059700012207\n",
      "Epoch 1000, Loss: 0.06832710653543472\n",
      "Epoch 1500, Loss: 0.06637744605541229\n",
      "Epoch 2000, Loss: 0.06585225462913513\n",
      "Epoch 2500, Loss: 0.06393176317214966\n",
      "Epoch 3000, Loss: 0.06259102374315262\n",
      "Epoch 3500, Loss: 0.07061679661273956\n",
      "Epoch 4000, Loss: 0.0613747164607048\n",
      "Epoch 4500, Loss: 0.06305430829524994\n",
      "Epoch 4999, Loss: 0.06330842524766922\n",
      "Epoch 0, Loss: 0.6637868881225586\n",
      "Epoch 500, Loss: 0.06665663421154022\n",
      "Epoch 1000, Loss: 0.0685410276055336\n",
      "Epoch 1500, Loss: 0.059839218854904175\n",
      "Epoch 2000, Loss: 0.062372028827667236\n",
      "Epoch 2500, Loss: 0.06214594095945358\n",
      "Epoch 3000, Loss: 0.05903990939259529\n",
      "Epoch 3500, Loss: 0.06042289361357689\n",
      "Epoch 4000, Loss: 0.05774229019880295\n",
      "Epoch 4500, Loss: 0.05793103575706482\n",
      "Epoch 4999, Loss: 0.059821270406246185\n",
      "--- Experiment 3/20 ---\n",
      "Epoch 0, Loss: 0.48360466957092285\n",
      "Epoch 500, Loss: 0.05642439424991608\n",
      "Epoch 1000, Loss: 0.052517060190439224\n",
      "Epoch 1500, Loss: 0.05177004635334015\n",
      "Epoch 2000, Loss: 0.049201324582099915\n",
      "Epoch 2500, Loss: 0.04972120374441147\n",
      "Epoch 3000, Loss: 0.04935793578624725\n",
      "Epoch 3500, Loss: 0.05303756147623062\n",
      "Epoch 4000, Loss: 0.04790722578763962\n",
      "Epoch 4500, Loss: 0.04854597896337509\n",
      "Epoch 4999, Loss: 0.049194514751434326\n",
      "Epoch 0, Loss: 0.4304110109806061\n",
      "Epoch 500, Loss: 0.08530933409929276\n",
      "Epoch 1000, Loss: 0.08196652680635452\n",
      "Epoch 1500, Loss: 0.08352939784526825\n",
      "Epoch 2000, Loss: 0.07773063331842422\n",
      "Epoch 2500, Loss: 0.08015084266662598\n",
      "Epoch 3000, Loss: 0.07612520456314087\n",
      "Epoch 3500, Loss: 0.07662305980920792\n",
      "Epoch 4000, Loss: 0.07918526232242584\n",
      "Epoch 4500, Loss: 0.07350905984640121\n",
      "Epoch 4999, Loss: 0.07427723705768585\n",
      "Epoch 0, Loss: 0.6679458618164062\n",
      "Epoch 500, Loss: 0.07692281156778336\n",
      "Epoch 1000, Loss: 0.06829286366701126\n",
      "Epoch 1500, Loss: 0.06596429646015167\n",
      "Epoch 2000, Loss: 0.0666034147143364\n",
      "Epoch 2500, Loss: 0.06505483388900757\n",
      "Epoch 3000, Loss: 0.06488914787769318\n",
      "Epoch 3500, Loss: 0.06309603154659271\n",
      "Epoch 4000, Loss: 0.062459658831357956\n",
      "Epoch 4500, Loss: 0.06799963116645813\n",
      "Epoch 4999, Loss: 0.06276555359363556\n",
      "Epoch 0, Loss: 0.6998534798622131\n",
      "Epoch 500, Loss: 0.06856872141361237\n",
      "Epoch 1000, Loss: 0.06338643282651901\n",
      "Epoch 1500, Loss: 0.055544883012771606\n",
      "Epoch 2000, Loss: 0.05577346682548523\n",
      "Epoch 2500, Loss: 0.05618327111005783\n",
      "Epoch 3000, Loss: 0.05978069454431534\n",
      "Epoch 3500, Loss: 0.05172758921980858\n",
      "Epoch 4000, Loss: 0.054595697671175\n",
      "Epoch 4500, Loss: 0.06389126926660538\n",
      "Epoch 4999, Loss: 0.06047767400741577\n",
      "--- Experiment 4/20 ---\n",
      "Epoch 0, Loss: 0.7746209502220154\n",
      "Epoch 500, Loss: 0.0582091398537159\n",
      "Epoch 1000, Loss: 0.055372629314661026\n",
      "Epoch 1500, Loss: 0.05230826139450073\n",
      "Epoch 2000, Loss: 0.050829894840717316\n",
      "Epoch 2500, Loss: 0.052947744727134705\n",
      "Epoch 3000, Loss: 0.050598230212926865\n",
      "Epoch 3500, Loss: 0.052828557789325714\n",
      "Epoch 4000, Loss: 0.04770907014608383\n",
      "Epoch 4500, Loss: 0.04801679402589798\n",
      "Epoch 4999, Loss: 0.043596550822257996\n",
      "Epoch 0, Loss: 0.5707525014877319\n",
      "Epoch 500, Loss: 0.08533639460802078\n",
      "Epoch 1000, Loss: 0.08483707159757614\n",
      "Epoch 1500, Loss: 0.07846374809741974\n",
      "Epoch 2000, Loss: 0.07505690306425095\n",
      "Epoch 2500, Loss: 0.07632005214691162\n",
      "Epoch 3000, Loss: 0.07290356606245041\n",
      "Epoch 3500, Loss: 0.07671422511339188\n",
      "Epoch 4000, Loss: 0.07317957282066345\n",
      "Epoch 4500, Loss: 0.07443414628505707\n",
      "Epoch 4999, Loss: 0.07599511742591858\n",
      "Epoch 0, Loss: 0.46586474776268005\n",
      "Epoch 500, Loss: 0.07159507274627686\n",
      "Epoch 1000, Loss: 0.06797372549772263\n",
      "Epoch 1500, Loss: 0.06327854841947556\n",
      "Epoch 2000, Loss: 0.06595434248447418\n",
      "Epoch 2500, Loss: 0.06426233053207397\n",
      "Epoch 3000, Loss: 0.06497298926115036\n",
      "Epoch 3500, Loss: 0.0672985166311264\n",
      "Epoch 4000, Loss: 0.06510559469461441\n",
      "Epoch 4500, Loss: 0.06083191931247711\n",
      "Epoch 4999, Loss: 0.06499955803155899\n",
      "Epoch 0, Loss: 0.6726093292236328\n",
      "Epoch 500, Loss: 0.0692635178565979\n",
      "Epoch 1000, Loss: 0.06497381627559662\n",
      "Epoch 1500, Loss: 0.05805956572294235\n",
      "Epoch 2000, Loss: 0.05781419202685356\n",
      "Epoch 2500, Loss: 0.06542818993330002\n",
      "Epoch 3000, Loss: 0.05854836851358414\n",
      "Epoch 3500, Loss: 0.06850644201040268\n",
      "Epoch 4000, Loss: 0.06006802245974541\n",
      "Epoch 4500, Loss: 0.05947243794798851\n",
      "Epoch 4999, Loss: 0.05623670667409897\n",
      "--- Experiment 5/20 ---\n",
      "Epoch 0, Loss: 0.4341547191143036\n",
      "Epoch 500, Loss: 0.05540496110916138\n",
      "Epoch 1000, Loss: 0.05392551049590111\n",
      "Epoch 1500, Loss: 0.05298060178756714\n",
      "Epoch 2000, Loss: 0.05576230585575104\n",
      "Epoch 2500, Loss: 0.049560271203517914\n",
      "Epoch 3000, Loss: 0.0522657074034214\n",
      "Epoch 3500, Loss: 0.0508846677839756\n",
      "Epoch 4000, Loss: 0.04836780205368996\n",
      "Epoch 4500, Loss: 0.04711354896426201\n",
      "Epoch 4999, Loss: 0.04510851949453354\n",
      "Epoch 0, Loss: 0.45294439792633057\n",
      "Epoch 500, Loss: 0.08755684643983841\n",
      "Epoch 1000, Loss: 0.08357574045658112\n",
      "Epoch 1500, Loss: 0.07818788290023804\n",
      "Epoch 2000, Loss: 0.07510741800069809\n",
      "Epoch 2500, Loss: 0.07563222199678421\n",
      "Epoch 3000, Loss: 0.07568633556365967\n",
      "Epoch 3500, Loss: 0.0737239271402359\n",
      "Epoch 4000, Loss: 0.07494691014289856\n",
      "Epoch 4500, Loss: 0.0729951411485672\n",
      "Epoch 4999, Loss: 0.07129954546689987\n",
      "Epoch 0, Loss: 0.5039241313934326\n",
      "Epoch 500, Loss: 0.07574133574962616\n",
      "Epoch 1000, Loss: 0.06533248722553253\n",
      "Epoch 1500, Loss: 0.06474082171916962\n",
      "Epoch 2000, Loss: 0.0648566335439682\n",
      "Epoch 2500, Loss: 0.0663771778345108\n",
      "Epoch 3000, Loss: 0.0651758462190628\n",
      "Epoch 3500, Loss: 0.06333629786968231\n",
      "Epoch 4000, Loss: 0.06568700820207596\n",
      "Epoch 4500, Loss: 0.06410892307758331\n",
      "Epoch 4999, Loss: 0.060848310589790344\n",
      "Epoch 0, Loss: 0.5141800045967102\n",
      "Epoch 500, Loss: 0.06880371272563934\n",
      "Epoch 1000, Loss: 0.0606517419219017\n",
      "Epoch 1500, Loss: 0.06348077207803726\n",
      "Epoch 2000, Loss: 0.05755060911178589\n",
      "Epoch 2500, Loss: 0.05573499947786331\n",
      "Epoch 3000, Loss: 0.05951130390167236\n",
      "Epoch 3500, Loss: 0.059865325689315796\n",
      "Epoch 4000, Loss: 0.058090463280677795\n",
      "Epoch 4500, Loss: 0.054032664746046066\n",
      "Epoch 4999, Loss: 0.058215927332639694\n",
      "--- Experiment 6/20 ---\n",
      "Epoch 0, Loss: 0.313972532749176\n",
      "Epoch 500, Loss: 0.05530860647559166\n",
      "Epoch 1000, Loss: 0.052838750183582306\n",
      "Epoch 1500, Loss: 0.049916766583919525\n",
      "Epoch 2000, Loss: 0.05183219909667969\n",
      "Epoch 2500, Loss: 0.05182228982448578\n",
      "Epoch 3000, Loss: 0.04814494028687477\n",
      "Epoch 3500, Loss: 0.05270485207438469\n",
      "Epoch 4000, Loss: 0.049703292548656464\n",
      "Epoch 4500, Loss: 0.049591705203056335\n",
      "Epoch 4999, Loss: 0.04659552872180939\n",
      "Epoch 0, Loss: 0.6864504814147949\n",
      "Epoch 500, Loss: 0.08642801642417908\n",
      "Epoch 1000, Loss: 0.08474740386009216\n",
      "Epoch 1500, Loss: 0.07856674492359161\n",
      "Epoch 2000, Loss: 0.0801776647567749\n",
      "Epoch 2500, Loss: 0.07699193805456161\n",
      "Epoch 3000, Loss: 0.07587049901485443\n",
      "Epoch 3500, Loss: 0.07644376903772354\n",
      "Epoch 4000, Loss: 0.07558093965053558\n",
      "Epoch 4500, Loss: 0.07776111364364624\n",
      "Epoch 4999, Loss: 0.07295705378055573\n",
      "Epoch 0, Loss: 0.5516382455825806\n",
      "Epoch 500, Loss: 0.07690649479627609\n",
      "Epoch 1000, Loss: 0.06739072501659393\n",
      "Epoch 1500, Loss: 0.0630200058221817\n",
      "Epoch 2000, Loss: 0.06485450267791748\n",
      "Epoch 2500, Loss: 0.0643591582775116\n",
      "Epoch 3000, Loss: 0.06332280486822128\n",
      "Epoch 3500, Loss: 0.06250497698783875\n",
      "Epoch 4000, Loss: 0.06244185194373131\n",
      "Epoch 4500, Loss: 0.05439735949039459\n",
      "Epoch 4999, Loss: 0.06138985604047775\n",
      "Epoch 0, Loss: 0.44061750173568726\n",
      "Epoch 500, Loss: 0.06510377675294876\n",
      "Epoch 1000, Loss: 0.06179342418909073\n",
      "Epoch 1500, Loss: 0.056259796023368835\n",
      "Epoch 2000, Loss: 0.059205569326877594\n",
      "Epoch 2500, Loss: 0.060403745621442795\n",
      "Epoch 3000, Loss: 0.05759710073471069\n",
      "Epoch 3500, Loss: 0.05914244055747986\n",
      "Epoch 4000, Loss: 0.05969971790909767\n",
      "Epoch 4500, Loss: 0.055612027645111084\n",
      "Epoch 4999, Loss: 0.0493876077234745\n",
      "--- Experiment 7/20 ---\n",
      "Epoch 0, Loss: 0.49273088574409485\n",
      "Epoch 500, Loss: 0.05929882079362869\n",
      "Epoch 1000, Loss: 0.05521300435066223\n",
      "Epoch 1500, Loss: 0.05086801201105118\n",
      "Epoch 2000, Loss: 0.04912310466170311\n",
      "Epoch 2500, Loss: 0.05131691321730614\n",
      "Epoch 3000, Loss: 0.05090031027793884\n",
      "Epoch 3500, Loss: 0.0507790744304657\n",
      "Epoch 4000, Loss: 0.05107413977384567\n",
      "Epoch 4500, Loss: 0.04982452094554901\n",
      "Epoch 4999, Loss: 0.05269237607717514\n",
      "Epoch 0, Loss: 0.7764986157417297\n",
      "Epoch 500, Loss: 0.09229172766208649\n",
      "Epoch 1000, Loss: 0.08081275969743729\n",
      "Epoch 1500, Loss: 0.08117899298667908\n",
      "Epoch 2000, Loss: 0.08051902055740356\n",
      "Epoch 2500, Loss: 0.0744079202413559\n",
      "Epoch 3000, Loss: 0.07505926489830017\n",
      "Epoch 3500, Loss: 0.07380877435207367\n",
      "Epoch 4000, Loss: 0.07562397420406342\n",
      "Epoch 4500, Loss: 0.07140210270881653\n",
      "Epoch 4999, Loss: 0.071128711104393\n",
      "Epoch 0, Loss: 0.32327204942703247\n",
      "Epoch 500, Loss: 0.07223507016897202\n",
      "Epoch 1000, Loss: 0.06691434979438782\n",
      "Epoch 1500, Loss: 0.06203243136405945\n",
      "Epoch 2000, Loss: 0.0619165301322937\n",
      "Epoch 2500, Loss: 0.06574998795986176\n",
      "Epoch 3000, Loss: 0.06421968340873718\n",
      "Epoch 3500, Loss: 0.06495687365531921\n",
      "Epoch 4000, Loss: 0.06587988138198853\n",
      "Epoch 4500, Loss: 0.06445153802633286\n",
      "Epoch 4999, Loss: 0.05914296582341194\n",
      "Epoch 0, Loss: 0.3231911361217499\n",
      "Epoch 500, Loss: 0.06800247728824615\n",
      "Epoch 1000, Loss: 0.06429874897003174\n",
      "Epoch 1500, Loss: 0.06091025844216347\n",
      "Epoch 2000, Loss: 0.06082561984658241\n",
      "Epoch 2500, Loss: 0.062098823487758636\n",
      "Epoch 3000, Loss: 0.06182992458343506\n",
      "Epoch 3500, Loss: 0.05451909452676773\n",
      "Epoch 4000, Loss: 0.0540555901825428\n",
      "Epoch 4500, Loss: 0.05195499211549759\n",
      "Epoch 4999, Loss: 0.05328459292650223\n",
      "--- Experiment 8/20 ---\n",
      "Epoch 0, Loss: 0.6250261068344116\n",
      "Epoch 500, Loss: 0.056347984820604324\n",
      "Epoch 1000, Loss: 0.05477999150753021\n",
      "Epoch 1500, Loss: 0.054219335317611694\n",
      "Epoch 2000, Loss: 0.05180322378873825\n",
      "Epoch 2500, Loss: 0.04941897094249725\n",
      "Epoch 3000, Loss: 0.04861152172088623\n",
      "Epoch 3500, Loss: 0.046773478388786316\n",
      "Epoch 4000, Loss: 0.04924486577510834\n",
      "Epoch 4500, Loss: 0.047432348132133484\n",
      "Epoch 4999, Loss: 0.05123159661889076\n",
      "Epoch 0, Loss: 0.4204825758934021\n",
      "Epoch 500, Loss: 0.0881771594285965\n",
      "Epoch 1000, Loss: 0.0815044492483139\n",
      "Epoch 1500, Loss: 0.07926806062459946\n",
      "Epoch 2000, Loss: 0.07841576635837555\n",
      "Epoch 2500, Loss: 0.07615972310304642\n",
      "Epoch 3000, Loss: 0.07412676513195038\n",
      "Epoch 3500, Loss: 0.07474197447299957\n",
      "Epoch 4000, Loss: 0.073348768055439\n",
      "Epoch 4500, Loss: 0.0734778419137001\n",
      "Epoch 4999, Loss: 0.07040930539369583\n",
      "Epoch 0, Loss: 0.7667614817619324\n",
      "Epoch 500, Loss: 0.07545371353626251\n",
      "Epoch 1000, Loss: 0.06615392863750458\n",
      "Epoch 1500, Loss: 0.06612059473991394\n",
      "Epoch 2000, Loss: 0.06566382944583893\n",
      "Epoch 2500, Loss: 0.06231467425823212\n",
      "Epoch 3000, Loss: 0.06506292521953583\n",
      "Epoch 3500, Loss: 0.06262800842523575\n",
      "Epoch 4000, Loss: 0.06695875525474548\n",
      "Epoch 4500, Loss: 0.06305950880050659\n",
      "Epoch 4999, Loss: 0.07018952071666718\n",
      "Epoch 0, Loss: 0.8170095682144165\n",
      "Epoch 500, Loss: 0.07175913453102112\n",
      "Epoch 1000, Loss: 0.06305843591690063\n",
      "Epoch 1500, Loss: 0.06160346418619156\n",
      "Epoch 2000, Loss: 0.05619250237941742\n",
      "Epoch 2500, Loss: 0.05706235021352768\n",
      "Epoch 3000, Loss: 0.060597024857997894\n",
      "Epoch 3500, Loss: 0.059759266674518585\n",
      "Epoch 4000, Loss: 0.0635998472571373\n",
      "Epoch 4500, Loss: 0.05968581885099411\n",
      "Epoch 4999, Loss: 0.05650464445352554\n",
      "--- Experiment 9/20 ---\n",
      "Epoch 0, Loss: 0.48473379015922546\n",
      "Epoch 500, Loss: 0.05724451318383217\n",
      "Epoch 1000, Loss: 0.05492741987109184\n",
      "Epoch 1500, Loss: 0.05112623795866966\n",
      "Epoch 2000, Loss: 0.05290668457746506\n",
      "Epoch 2500, Loss: 0.05139978229999542\n",
      "Epoch 3000, Loss: 0.05252803862094879\n",
      "Epoch 3500, Loss: 0.05104833468794823\n",
      "Epoch 4000, Loss: 0.05115264654159546\n",
      "Epoch 4500, Loss: 0.046283967792987823\n",
      "Epoch 4999, Loss: 0.04769420996308327\n",
      "Epoch 0, Loss: 0.5420477986335754\n",
      "Epoch 500, Loss: 0.08792760223150253\n",
      "Epoch 1000, Loss: 0.0835118219256401\n",
      "Epoch 1500, Loss: 0.07977239787578583\n",
      "Epoch 2000, Loss: 0.07715295255184174\n",
      "Epoch 2500, Loss: 0.07492508739233017\n",
      "Epoch 3000, Loss: 0.07343301177024841\n",
      "Epoch 3500, Loss: 0.07360068708658218\n",
      "Epoch 4000, Loss: 0.0715290904045105\n",
      "Epoch 4500, Loss: 0.07160501927137375\n",
      "Epoch 4999, Loss: 0.07359077781438828\n",
      "Epoch 0, Loss: 0.6624158024787903\n",
      "Epoch 500, Loss: 0.08080057054758072\n",
      "Epoch 1000, Loss: 0.07087988406419754\n",
      "Epoch 1500, Loss: 0.06498678028583527\n",
      "Epoch 2000, Loss: 0.06454621255397797\n",
      "Epoch 2500, Loss: 0.06288333982229233\n",
      "Epoch 3000, Loss: 0.06297019124031067\n",
      "Epoch 3500, Loss: 0.06269931048154831\n",
      "Epoch 4000, Loss: 0.06312213093042374\n",
      "Epoch 4500, Loss: 0.06175673380494118\n",
      "Epoch 4999, Loss: 0.06264105439186096\n",
      "Epoch 0, Loss: 0.5854482650756836\n",
      "Epoch 500, Loss: 0.07032858580350876\n",
      "Epoch 1000, Loss: 0.0640912652015686\n",
      "Epoch 1500, Loss: 0.06287537515163422\n",
      "Epoch 2000, Loss: 0.06104467064142227\n",
      "Epoch 2500, Loss: 0.061456792056560516\n",
      "Epoch 3000, Loss: 0.0602257177233696\n",
      "Epoch 3500, Loss: 0.056666940450668335\n",
      "Epoch 4000, Loss: 0.06083136051893234\n",
      "Epoch 4500, Loss: 0.062392350286245346\n",
      "Epoch 4999, Loss: 0.055794887244701385\n",
      "--- Experiment 10/20 ---\n",
      "Epoch 0, Loss: 0.4299386739730835\n",
      "Epoch 500, Loss: 0.05999503284692764\n",
      "Epoch 1000, Loss: 0.05448176711797714\n",
      "Epoch 1500, Loss: 0.050049275159835815\n",
      "Epoch 2000, Loss: 0.04816504567861557\n",
      "Epoch 2500, Loss: 0.04922306537628174\n",
      "Epoch 3000, Loss: 0.048673514276742935\n",
      "Epoch 3500, Loss: 0.04691225290298462\n",
      "Epoch 4000, Loss: 0.04794555902481079\n",
      "Epoch 4500, Loss: 0.046791065484285355\n",
      "Epoch 4999, Loss: 0.04658152908086777\n",
      "Epoch 0, Loss: 0.42046961188316345\n",
      "Epoch 500, Loss: 0.08154961466789246\n",
      "Epoch 1000, Loss: 0.08248473703861237\n",
      "Epoch 1500, Loss: 0.07920314371585846\n",
      "Epoch 2000, Loss: 0.07439888268709183\n",
      "Epoch 2500, Loss: 0.07354383170604706\n",
      "Epoch 3000, Loss: 0.07188116014003754\n",
      "Epoch 3500, Loss: 0.07745533436536789\n",
      "Epoch 4000, Loss: 0.07924125343561172\n",
      "Epoch 4500, Loss: 0.07376091182231903\n",
      "Epoch 4999, Loss: 0.0720243826508522\n",
      "Epoch 0, Loss: 0.339115709066391\n",
      "Epoch 500, Loss: 0.0719086080789566\n",
      "Epoch 1000, Loss: 0.06631818413734436\n",
      "Epoch 1500, Loss: 0.06581563502550125\n",
      "Epoch 2000, Loss: 0.06358417868614197\n",
      "Epoch 2500, Loss: 0.06387486308813095\n",
      "Epoch 3000, Loss: 0.0634048581123352\n",
      "Epoch 3500, Loss: 0.062444619834423065\n",
      "Epoch 4000, Loss: 0.06481414288282394\n",
      "Epoch 4500, Loss: 0.06284807622432709\n",
      "Epoch 4999, Loss: 0.060065872967243195\n",
      "Epoch 0, Loss: 0.6474666595458984\n",
      "Epoch 500, Loss: 0.06687476485967636\n",
      "Epoch 1000, Loss: 0.061544232070446014\n",
      "Epoch 1500, Loss: 0.05924874544143677\n",
      "Epoch 2000, Loss: 0.05727288872003555\n",
      "Epoch 2500, Loss: 0.05990983173251152\n",
      "Epoch 3000, Loss: 0.0581124871969223\n",
      "Epoch 3500, Loss: 0.06560882925987244\n",
      "Epoch 4000, Loss: 0.060156792402267456\n",
      "Epoch 4500, Loss: 0.05835719406604767\n",
      "Epoch 4999, Loss: 0.06070854514837265\n",
      "--- Experiment 11/20 ---\n",
      "Epoch 0, Loss: 0.5552747249603271\n",
      "Epoch 500, Loss: 0.05538925528526306\n",
      "Epoch 1000, Loss: 0.05527922138571739\n",
      "Epoch 1500, Loss: 0.0553927943110466\n",
      "Epoch 2000, Loss: 0.051006659865379333\n",
      "Epoch 2500, Loss: 0.048773013055324554\n",
      "Epoch 3000, Loss: 0.05251400172710419\n",
      "Epoch 3500, Loss: 0.04653794318437576\n",
      "Epoch 4000, Loss: 0.04844371974468231\n",
      "Epoch 4500, Loss: 0.049682214856147766\n",
      "Epoch 4999, Loss: 0.04663415253162384\n",
      "Epoch 0, Loss: 0.561782956123352\n",
      "Epoch 500, Loss: 0.09078098088502884\n",
      "Epoch 1000, Loss: 0.08232971280813217\n",
      "Epoch 1500, Loss: 0.07950849086046219\n",
      "Epoch 2000, Loss: 0.0787794291973114\n",
      "Epoch 2500, Loss: 0.08365650475025177\n",
      "Epoch 3000, Loss: 0.07507883012294769\n",
      "Epoch 3500, Loss: 0.0754910558462143\n",
      "Epoch 4000, Loss: 0.07366428524255753\n",
      "Epoch 4500, Loss: 0.07486408948898315\n",
      "Epoch 4999, Loss: 0.07591605186462402\n",
      "Epoch 0, Loss: 0.5748350024223328\n",
      "Epoch 500, Loss: 0.07052414119243622\n",
      "Epoch 1000, Loss: 0.06578915566205978\n",
      "Epoch 1500, Loss: 0.06494803726673126\n",
      "Epoch 2000, Loss: 0.06000446528196335\n",
      "Epoch 2500, Loss: 0.06140939146280289\n",
      "Epoch 3000, Loss: 0.06255690008401871\n",
      "Epoch 3500, Loss: 0.07343503087759018\n",
      "Epoch 4000, Loss: 0.05987291783094406\n",
      "Epoch 4500, Loss: 0.06248579919338226\n",
      "Epoch 4999, Loss: 0.06182155758142471\n",
      "Epoch 0, Loss: 0.3474487066268921\n",
      "Epoch 500, Loss: 0.06414175033569336\n",
      "Epoch 1000, Loss: 0.06262160837650299\n",
      "Epoch 1500, Loss: 0.06007193401455879\n",
      "Epoch 2000, Loss: 0.05847027152776718\n",
      "Epoch 2500, Loss: 0.05530957132577896\n",
      "Epoch 3000, Loss: 0.06050882488489151\n",
      "Epoch 3500, Loss: 0.05819544196128845\n",
      "Epoch 4000, Loss: 0.058114491403102875\n",
      "Epoch 4500, Loss: 0.052548639476299286\n",
      "Epoch 4999, Loss: 0.05946093425154686\n",
      "--- Experiment 12/20 ---\n",
      "Epoch 0, Loss: 0.39364340901374817\n",
      "Epoch 500, Loss: 0.059827759861946106\n",
      "Epoch 1000, Loss: 0.05001543089747429\n",
      "Epoch 1500, Loss: 0.05266522988677025\n",
      "Epoch 2000, Loss: 0.04942545294761658\n",
      "Epoch 2500, Loss: 0.05254428833723068\n",
      "Epoch 3000, Loss: 0.0492405965924263\n",
      "Epoch 3500, Loss: 0.046925608068704605\n",
      "Epoch 4000, Loss: 0.04590250179171562\n",
      "Epoch 4500, Loss: 0.04613786190748215\n",
      "Epoch 4999, Loss: 0.04569818079471588\n",
      "Epoch 0, Loss: 0.5158201456069946\n",
      "Epoch 500, Loss: 0.08555538207292557\n",
      "Epoch 1000, Loss: 0.07661829143762589\n",
      "Epoch 1500, Loss: 0.0789027065038681\n",
      "Epoch 2000, Loss: 0.07215876877307892\n",
      "Epoch 2500, Loss: 0.07513929158449173\n",
      "Epoch 3000, Loss: 0.07616083323955536\n",
      "Epoch 3500, Loss: 0.07393515110015869\n",
      "Epoch 4000, Loss: 0.07220658659934998\n",
      "Epoch 4500, Loss: 0.07611501216888428\n",
      "Epoch 4999, Loss: 0.07239410281181335\n",
      "Epoch 0, Loss: 0.717866063117981\n",
      "Epoch 500, Loss: 0.0771271362900734\n",
      "Epoch 1000, Loss: 0.06891696155071259\n",
      "Epoch 1500, Loss: 0.0663686990737915\n",
      "Epoch 2000, Loss: 0.06432349979877472\n",
      "Epoch 2500, Loss: 0.06480413675308228\n",
      "Epoch 3000, Loss: 0.06326325237751007\n",
      "Epoch 3500, Loss: 0.06303578615188599\n",
      "Epoch 4000, Loss: 0.06189582124352455\n",
      "Epoch 4500, Loss: 0.06722392141819\n",
      "Epoch 4999, Loss: 0.0628480613231659\n",
      "Epoch 0, Loss: 0.6127670407295227\n",
      "Epoch 500, Loss: 0.06734999269247055\n",
      "Epoch 1000, Loss: 0.06278838217258453\n",
      "Epoch 1500, Loss: 0.059218764305114746\n",
      "Epoch 2000, Loss: 0.05323902517557144\n",
      "Epoch 2500, Loss: 0.05990450084209442\n",
      "Epoch 3000, Loss: 0.060508113354444504\n",
      "Epoch 3500, Loss: 0.058818452060222626\n",
      "Epoch 4000, Loss: 0.055686719715595245\n",
      "Epoch 4500, Loss: 0.05944225192070007\n",
      "Epoch 4999, Loss: 0.05503394827246666\n",
      "--- Experiment 13/20 ---\n",
      "Epoch 0, Loss: 0.4968448877334595\n",
      "Epoch 500, Loss: 0.056252412497997284\n",
      "Epoch 1000, Loss: 0.054520316421985626\n",
      "Epoch 1500, Loss: 0.05062369257211685\n",
      "Epoch 2000, Loss: 0.04870635271072388\n",
      "Epoch 2500, Loss: 0.052603110671043396\n",
      "Epoch 3000, Loss: 0.04940718039870262\n",
      "Epoch 3500, Loss: 0.04906073212623596\n",
      "Epoch 4000, Loss: 0.04950571060180664\n",
      "Epoch 4500, Loss: 0.047567594796419144\n",
      "Epoch 4999, Loss: 0.0479910708963871\n",
      "Epoch 0, Loss: 0.5643816590309143\n",
      "Epoch 500, Loss: 0.08970940858125687\n",
      "Epoch 1000, Loss: 0.08384449034929276\n",
      "Epoch 1500, Loss: 0.0758226066827774\n",
      "Epoch 2000, Loss: 0.07848533242940903\n",
      "Epoch 2500, Loss: 0.08463121950626373\n",
      "Epoch 3000, Loss: 0.07252274453639984\n",
      "Epoch 3500, Loss: 0.07615973800420761\n",
      "Epoch 4000, Loss: 0.07209401577711105\n",
      "Epoch 4500, Loss: 0.07211660593748093\n",
      "Epoch 4999, Loss: 0.0726919025182724\n",
      "Epoch 0, Loss: 0.5775253772735596\n",
      "Epoch 500, Loss: 0.07782462239265442\n",
      "Epoch 1000, Loss: 0.06909798830747604\n",
      "Epoch 1500, Loss: 0.06692254543304443\n",
      "Epoch 2000, Loss: 0.06598871946334839\n",
      "Epoch 2500, Loss: 0.06433504819869995\n",
      "Epoch 3000, Loss: 0.060502421110868454\n",
      "Epoch 3500, Loss: 0.06332211941480637\n",
      "Epoch 4000, Loss: 0.06302110105752945\n",
      "Epoch 4500, Loss: 0.061484724283218384\n",
      "Epoch 4999, Loss: 0.06186412274837494\n",
      "Epoch 0, Loss: 0.44252726435661316\n",
      "Epoch 500, Loss: 0.06463710963726044\n",
      "Epoch 1000, Loss: 0.05992057919502258\n",
      "Epoch 1500, Loss: 0.05980169028043747\n",
      "Epoch 2000, Loss: 0.06047483906149864\n",
      "Epoch 2500, Loss: 0.058061033487319946\n",
      "Epoch 3000, Loss: 0.05096491426229477\n",
      "Epoch 3500, Loss: 0.05674334987998009\n",
      "Epoch 4000, Loss: 0.057309821248054504\n",
      "Epoch 4500, Loss: 0.06134454160928726\n",
      "Epoch 4999, Loss: 0.050701841711997986\n",
      "--- Experiment 14/20 ---\n",
      "Epoch 0, Loss: 0.4576110243797302\n",
      "Epoch 500, Loss: 0.057386867702007294\n",
      "Epoch 1000, Loss: 0.054655395448207855\n",
      "Epoch 1500, Loss: 0.051894016563892365\n",
      "Epoch 2000, Loss: 0.051507167518138885\n",
      "Epoch 2500, Loss: 0.05059134215116501\n",
      "Epoch 3000, Loss: 0.05108093470335007\n",
      "Epoch 3500, Loss: 0.05079299211502075\n",
      "Epoch 4000, Loss: 0.04965926706790924\n",
      "Epoch 4500, Loss: 0.04979277402162552\n",
      "Epoch 4999, Loss: 0.050326935946941376\n",
      "Epoch 0, Loss: 0.5353500247001648\n",
      "Epoch 500, Loss: 0.088679738342762\n",
      "Epoch 1000, Loss: 0.08083362877368927\n",
      "Epoch 1500, Loss: 0.07540566474199295\n",
      "Epoch 2000, Loss: 0.08115249872207642\n",
      "Epoch 2500, Loss: 0.0762307345867157\n",
      "Epoch 3000, Loss: 0.0755322277545929\n",
      "Epoch 3500, Loss: 0.08296294510364532\n",
      "Epoch 4000, Loss: 0.07399256527423859\n",
      "Epoch 4500, Loss: 0.08603432774543762\n",
      "Epoch 4999, Loss: 0.07231872528791428\n",
      "Epoch 0, Loss: 0.4513912498950958\n",
      "Epoch 500, Loss: 0.07566366344690323\n",
      "Epoch 1000, Loss: 0.06826353818178177\n",
      "Epoch 1500, Loss: 0.06474586576223373\n",
      "Epoch 2000, Loss: 0.06384900212287903\n",
      "Epoch 2500, Loss: 0.06701593101024628\n",
      "Epoch 3000, Loss: 0.06293909251689911\n",
      "Epoch 3500, Loss: 0.06292823702096939\n",
      "Epoch 4000, Loss: 0.0631285011768341\n",
      "Epoch 4500, Loss: 0.06336966156959534\n",
      "Epoch 4999, Loss: 0.059097107499837875\n",
      "Epoch 0, Loss: 0.5347107648849487\n",
      "Epoch 500, Loss: 0.06752081215381622\n",
      "Epoch 1000, Loss: 0.061447370797395706\n",
      "Epoch 1500, Loss: 0.061856742948293686\n",
      "Epoch 2000, Loss: 0.05792107433080673\n",
      "Epoch 2500, Loss: 0.0498044416308403\n",
      "Epoch 3000, Loss: 0.06247755140066147\n",
      "Epoch 3500, Loss: 0.056977130472660065\n",
      "Epoch 4000, Loss: 0.057692818343639374\n",
      "Epoch 4500, Loss: 0.06040221080183983\n",
      "Epoch 4999, Loss: 0.056218039244413376\n",
      "--- Experiment 15/20 ---\n",
      "Epoch 0, Loss: 0.4134153723716736\n",
      "Epoch 500, Loss: 0.056297749280929565\n",
      "Epoch 1000, Loss: 0.053764741867780685\n",
      "Epoch 1500, Loss: 0.0517757385969162\n",
      "Epoch 2000, Loss: 0.04904090613126755\n",
      "Epoch 2500, Loss: 0.04823606088757515\n",
      "Epoch 3000, Loss: 0.05036550760269165\n",
      "Epoch 3500, Loss: 0.05006338655948639\n",
      "Epoch 4000, Loss: 0.04818325117230415\n",
      "Epoch 4500, Loss: 0.05217617005109787\n",
      "Epoch 4999, Loss: 0.05000933259725571\n",
      "Epoch 0, Loss: 0.48495206236839294\n",
      "Epoch 500, Loss: 0.08734643459320068\n",
      "Epoch 1000, Loss: 0.08472561091184616\n",
      "Epoch 1500, Loss: 0.08118273317813873\n",
      "Epoch 2000, Loss: 0.0798976719379425\n",
      "Epoch 2500, Loss: 0.07405657321214676\n",
      "Epoch 3000, Loss: 0.07495637238025665\n",
      "Epoch 3500, Loss: 0.07569370418787003\n",
      "Epoch 4000, Loss: 0.07240466773509979\n",
      "Epoch 4500, Loss: 0.07255300879478455\n",
      "Epoch 4999, Loss: 0.07300486415624619\n",
      "Epoch 0, Loss: 0.3767718970775604\n",
      "Epoch 500, Loss: 0.07019601762294769\n",
      "Epoch 1000, Loss: 0.06843215227127075\n",
      "Epoch 1500, Loss: 0.06420663744211197\n",
      "Epoch 2000, Loss: 0.06092815846204758\n",
      "Epoch 2500, Loss: 0.06318683922290802\n",
      "Epoch 3000, Loss: 0.060735832899808884\n",
      "Epoch 3500, Loss: 0.06666626036167145\n",
      "Epoch 4000, Loss: 0.060263313353061676\n",
      "Epoch 4500, Loss: 0.06204763054847717\n",
      "Epoch 4999, Loss: 0.0640687644481659\n",
      "Epoch 0, Loss: 0.7934689521789551\n",
      "Epoch 500, Loss: 0.0680743157863617\n",
      "Epoch 1000, Loss: 0.061046238988637924\n",
      "Epoch 1500, Loss: 0.06345009803771973\n",
      "Epoch 2000, Loss: 0.06012289226055145\n",
      "Epoch 2500, Loss: 0.055697403848171234\n",
      "Epoch 3000, Loss: 0.05549756810069084\n",
      "Epoch 3500, Loss: 0.059501342475414276\n",
      "Epoch 4000, Loss: 0.059095632284879684\n",
      "Epoch 4500, Loss: 0.05542062968015671\n",
      "Epoch 4999, Loss: 0.05955733358860016\n",
      "--- Experiment 16/20 ---\n",
      "Epoch 0, Loss: 0.6036825180053711\n",
      "Epoch 500, Loss: 0.06059463322162628\n",
      "Epoch 1000, Loss: 0.05557861924171448\n",
      "Epoch 1500, Loss: 0.05442783981561661\n",
      "Epoch 2000, Loss: 0.05263437330722809\n",
      "Epoch 2500, Loss: 0.05088678374886513\n",
      "Epoch 3000, Loss: 0.050442688167095184\n",
      "Epoch 3500, Loss: 0.052100032567977905\n",
      "Epoch 4000, Loss: 0.049279019236564636\n",
      "Epoch 4500, Loss: 0.04952307417988777\n",
      "Epoch 4999, Loss: 0.051916420459747314\n",
      "Epoch 0, Loss: 0.6347737908363342\n",
      "Epoch 500, Loss: 0.08618458360433578\n",
      "Epoch 1000, Loss: 0.08488249033689499\n",
      "Epoch 1500, Loss: 0.07924769818782806\n",
      "Epoch 2000, Loss: 0.08144215494394302\n",
      "Epoch 2500, Loss: 0.07569973915815353\n",
      "Epoch 3000, Loss: 0.07462794333696365\n",
      "Epoch 3500, Loss: 0.07643276453018188\n",
      "Epoch 4000, Loss: 0.07205933332443237\n",
      "Epoch 4500, Loss: 0.07379412651062012\n",
      "Epoch 4999, Loss: 0.07204758375883102\n",
      "Epoch 0, Loss: 0.36780449748039246\n",
      "Epoch 500, Loss: 0.07187334448099136\n",
      "Epoch 1000, Loss: 0.06502383202314377\n",
      "Epoch 1500, Loss: 0.06158234551548958\n",
      "Epoch 2000, Loss: 0.06478266417980194\n",
      "Epoch 2500, Loss: 0.06296320259571075\n",
      "Epoch 3000, Loss: 0.06351949274539948\n",
      "Epoch 3500, Loss: 0.06342723965644836\n",
      "Epoch 4000, Loss: 0.06172330677509308\n",
      "Epoch 4500, Loss: 0.06477584689855576\n",
      "Epoch 4999, Loss: 0.06181533262133598\n",
      "Epoch 0, Loss: 0.33670303225517273\n",
      "Epoch 500, Loss: 0.06645894050598145\n",
      "Epoch 1000, Loss: 0.061539702117443085\n",
      "Epoch 1500, Loss: 0.06294257938861847\n",
      "Epoch 2000, Loss: 0.059206489473581314\n",
      "Epoch 2500, Loss: 0.06131508946418762\n",
      "Epoch 3000, Loss: 0.05886782333254814\n",
      "Epoch 3500, Loss: 0.058648861944675446\n",
      "Epoch 4000, Loss: 0.05999794602394104\n",
      "Epoch 4500, Loss: 0.059499591588974\n",
      "Epoch 4999, Loss: 0.054524119943380356\n",
      "--- Experiment 17/20 ---\n",
      "Epoch 0, Loss: 0.45889440178871155\n",
      "Epoch 500, Loss: 0.05795148015022278\n",
      "Epoch 1000, Loss: 0.05436573922634125\n",
      "Epoch 1500, Loss: 0.04982621222734451\n",
      "Epoch 2000, Loss: 0.052538368850946426\n",
      "Epoch 2500, Loss: 0.051650386303663254\n",
      "Epoch 3000, Loss: 0.05304552614688873\n",
      "Epoch 3500, Loss: 0.051492657512426376\n",
      "Epoch 4000, Loss: 0.04793529212474823\n",
      "Epoch 4500, Loss: 0.052088357508182526\n",
      "Epoch 4999, Loss: 0.04861343652009964\n",
      "Epoch 0, Loss: 0.45494914054870605\n",
      "Epoch 500, Loss: 0.08447182923555374\n",
      "Epoch 1000, Loss: 0.07805214822292328\n",
      "Epoch 1500, Loss: 0.07888274639844894\n",
      "Epoch 2000, Loss: 0.07553426921367645\n",
      "Epoch 2500, Loss: 0.07580213993787766\n",
      "Epoch 3000, Loss: 0.07327912002801895\n",
      "Epoch 3500, Loss: 0.07740209996700287\n",
      "Epoch 4000, Loss: 0.07144146412611008\n",
      "Epoch 4500, Loss: 0.0716034471988678\n",
      "Epoch 4999, Loss: 0.07149498909711838\n",
      "Epoch 0, Loss: 0.5050210952758789\n",
      "Epoch 500, Loss: 0.07670015096664429\n",
      "Epoch 1000, Loss: 0.0680936723947525\n",
      "Epoch 1500, Loss: 0.0638657808303833\n",
      "Epoch 2000, Loss: 0.06310742348432541\n",
      "Epoch 2500, Loss: 0.06452909857034683\n",
      "Epoch 3000, Loss: 0.06595608592033386\n",
      "Epoch 3500, Loss: 0.06292814016342163\n",
      "Epoch 4000, Loss: 0.06551648676395416\n",
      "Epoch 4500, Loss: 0.062251977622509\n",
      "Epoch 4999, Loss: 0.05868464708328247\n",
      "Epoch 0, Loss: 0.3402023911476135\n",
      "Epoch 500, Loss: 0.0626986026763916\n",
      "Epoch 1000, Loss: 0.06287722289562225\n",
      "Epoch 1500, Loss: 0.06304827332496643\n",
      "Epoch 2000, Loss: 0.057654429227113724\n",
      "Epoch 2500, Loss: 0.057973463088274\n",
      "Epoch 3000, Loss: 0.0589313805103302\n",
      "Epoch 3500, Loss: 0.0585298091173172\n",
      "Epoch 4000, Loss: 0.06072056293487549\n",
      "Epoch 4500, Loss: 0.0581863708794117\n",
      "Epoch 4999, Loss: 0.0586024671792984\n",
      "--- Experiment 18/20 ---\n",
      "Epoch 0, Loss: 0.8250381350517273\n",
      "Epoch 500, Loss: 0.05885491147637367\n",
      "Epoch 1000, Loss: 0.051740311086177826\n",
      "Epoch 1500, Loss: 0.05145026743412018\n",
      "Epoch 2000, Loss: 0.05192459002137184\n",
      "Epoch 2500, Loss: 0.05364760383963585\n",
      "Epoch 3000, Loss: 0.05190768092870712\n",
      "Epoch 3500, Loss: 0.0502685084939003\n",
      "Epoch 4000, Loss: 0.05236367881298065\n",
      "Epoch 4500, Loss: 0.04764851555228233\n",
      "Epoch 4999, Loss: 0.047728732228279114\n",
      "Epoch 0, Loss: 0.4228278696537018\n",
      "Epoch 500, Loss: 0.0870823934674263\n",
      "Epoch 1000, Loss: 0.08397895097732544\n",
      "Epoch 1500, Loss: 0.07647263258695602\n",
      "Epoch 2000, Loss: 0.0827588438987732\n",
      "Epoch 2500, Loss: 0.07409094274044037\n",
      "Epoch 3000, Loss: 0.0747154951095581\n",
      "Epoch 3500, Loss: 0.07211173325777054\n",
      "Epoch 4000, Loss: 0.07304021716117859\n",
      "Epoch 4500, Loss: 0.07379646599292755\n",
      "Epoch 4999, Loss: 0.0742190033197403\n",
      "Epoch 0, Loss: 0.556019127368927\n",
      "Epoch 500, Loss: 0.07738286256790161\n",
      "Epoch 1000, Loss: 0.06708714365959167\n",
      "Epoch 1500, Loss: 0.0666305422782898\n",
      "Epoch 2000, Loss: 0.06389611959457397\n",
      "Epoch 2500, Loss: 0.0649462640285492\n",
      "Epoch 3000, Loss: 0.06272338330745697\n",
      "Epoch 3500, Loss: 0.06068534776568413\n",
      "Epoch 4000, Loss: 0.06434997171163559\n",
      "Epoch 4500, Loss: 0.06172875314950943\n",
      "Epoch 4999, Loss: 0.062373075634241104\n",
      "Epoch 0, Loss: 0.48035484552383423\n",
      "Epoch 500, Loss: 0.06429628282785416\n",
      "Epoch 1000, Loss: 0.05891820788383484\n",
      "Epoch 1500, Loss: 0.062170036137104034\n",
      "Epoch 2000, Loss: 0.0587683767080307\n",
      "Epoch 2500, Loss: 0.060556359589099884\n",
      "Epoch 3000, Loss: 0.05691618472337723\n",
      "Epoch 3500, Loss: 0.05822580307722092\n",
      "Epoch 4000, Loss: 0.06339345872402191\n",
      "Epoch 4500, Loss: 0.05948313698172569\n",
      "Epoch 4999, Loss: 0.05818886309862137\n",
      "--- Experiment 19/20 ---\n",
      "Epoch 0, Loss: 0.3375168740749359\n",
      "Epoch 500, Loss: 0.05876275897026062\n",
      "Epoch 1000, Loss: 0.054434776306152344\n",
      "Epoch 1500, Loss: 0.05090590566396713\n",
      "Epoch 2000, Loss: 0.05207686126232147\n",
      "Epoch 2500, Loss: 0.051492299884557724\n",
      "Epoch 3000, Loss: 0.049798719584941864\n",
      "Epoch 3500, Loss: 0.051948919892311096\n",
      "Epoch 4000, Loss: 0.04684323072433472\n",
      "Epoch 4500, Loss: 0.047262005507946014\n",
      "Epoch 4999, Loss: 0.04491845518350601\n",
      "Epoch 0, Loss: 0.4694052040576935\n",
      "Epoch 500, Loss: 0.08396883308887482\n",
      "Epoch 1000, Loss: 0.08479977399110794\n",
      "Epoch 1500, Loss: 0.07931232452392578\n",
      "Epoch 2000, Loss: 0.08136855065822601\n",
      "Epoch 2500, Loss: 0.0792887955904007\n",
      "Epoch 3000, Loss: 0.07416641712188721\n",
      "Epoch 3500, Loss: 0.07410790771245956\n",
      "Epoch 4000, Loss: 0.07357863336801529\n",
      "Epoch 4500, Loss: 0.07233372330665588\n",
      "Epoch 4999, Loss: 0.07260078936815262\n",
      "Epoch 0, Loss: 0.5526197552680969\n",
      "Epoch 500, Loss: 0.0783676952123642\n",
      "Epoch 1000, Loss: 0.06971816718578339\n",
      "Epoch 1500, Loss: 0.06765961647033691\n",
      "Epoch 2000, Loss: 0.06619605422019958\n",
      "Epoch 2500, Loss: 0.06392468512058258\n",
      "Epoch 3000, Loss: 0.06502695381641388\n",
      "Epoch 3500, Loss: 0.06455816328525543\n",
      "Epoch 4000, Loss: 0.062438905239105225\n",
      "Epoch 4500, Loss: 0.05957251042127609\n",
      "Epoch 4999, Loss: 0.06181535869836807\n",
      "Epoch 0, Loss: 0.48765116930007935\n",
      "Epoch 500, Loss: 0.06474278122186661\n",
      "Epoch 1000, Loss: 0.0614725798368454\n",
      "Epoch 1500, Loss: 0.053803712129592896\n",
      "Epoch 2000, Loss: 0.06093055382370949\n",
      "Epoch 2500, Loss: 0.06032365560531616\n",
      "Epoch 3000, Loss: 0.0583154559135437\n",
      "Epoch 3500, Loss: 0.05204086750745773\n",
      "Epoch 4000, Loss: 0.05731898546218872\n",
      "Epoch 4500, Loss: 0.05912701040506363\n",
      "Epoch 4999, Loss: 0.05671233683824539\n",
      "--- Experiment 20/20 ---\n",
      "Epoch 0, Loss: 0.6128125190734863\n",
      "Epoch 500, Loss: 0.05879552662372589\n",
      "Epoch 1000, Loss: 0.054721400141716\n",
      "Epoch 1500, Loss: 0.056013643741607666\n",
      "Epoch 2000, Loss: 0.05195188149809837\n",
      "Epoch 2500, Loss: 0.05069388449192047\n",
      "Epoch 3000, Loss: 0.04869057983160019\n",
      "Epoch 3500, Loss: 0.04912791773676872\n",
      "Epoch 4000, Loss: 0.04821976274251938\n",
      "Epoch 4500, Loss: 0.04753867909312248\n",
      "Epoch 4999, Loss: 0.050138071179389954\n",
      "Epoch 0, Loss: 0.4216294288635254\n",
      "Epoch 500, Loss: 0.08953987807035446\n",
      "Epoch 1000, Loss: 0.08225025236606598\n",
      "Epoch 1500, Loss: 0.08192503452301025\n",
      "Epoch 2000, Loss: 0.08029063045978546\n",
      "Epoch 2500, Loss: 0.07635178416967392\n",
      "Epoch 3000, Loss: 0.07775399833917618\n",
      "Epoch 3500, Loss: 0.07691629230976105\n",
      "Epoch 4000, Loss: 0.07381518930196762\n",
      "Epoch 4500, Loss: 0.07664789259433746\n",
      "Epoch 4999, Loss: 0.07294701039791107\n",
      "Epoch 0, Loss: 0.6979179382324219\n",
      "Epoch 500, Loss: 0.0788612961769104\n",
      "Epoch 1000, Loss: 0.06635649502277374\n",
      "Epoch 1500, Loss: 0.06900762766599655\n",
      "Epoch 2000, Loss: 0.06425561010837555\n",
      "Epoch 2500, Loss: 0.06762337684631348\n",
      "Epoch 3000, Loss: 0.06345827877521515\n",
      "Epoch 3500, Loss: 0.06150120496749878\n",
      "Epoch 4000, Loss: 0.06431755423545837\n",
      "Epoch 4500, Loss: 0.06291750073432922\n",
      "Epoch 4999, Loss: 0.061868563294410706\n",
      "Epoch 0, Loss: 0.7361021637916565\n",
      "Epoch 500, Loss: 0.0698140487074852\n",
      "Epoch 1000, Loss: 0.06381697207689285\n",
      "Epoch 1500, Loss: 0.05946904420852661\n",
      "Epoch 2000, Loss: 0.05753353238105774\n",
      "Epoch 2500, Loss: 0.059666942805051804\n",
      "Epoch 3000, Loss: 0.056080467998981476\n",
      "Epoch 3500, Loss: 0.055125508457422256\n",
      "Epoch 4000, Loss: 0.0605456568300724\n",
      "Epoch 4500, Loss: 0.05913548171520233\n",
      "Epoch 4999, Loss: 0.05703013017773628\n",
      "Average Validation Loss (Base):   0.047940 ± 0.000614\n",
      "Average Validation Loss (Import): 0.063368 ± 0.001567\n",
      "Average Validation Loss (GP Out): 0.055806 ± 0.002336\n",
      "Average Validation Loss (GP Res): 0.056691 ± 0.006769\n",
      "\n",
      "==== Running experiments for IC: gaussian, BC: dirichlet ====\n",
      "--- Experiment 1/20 ---\n",
      "Epoch 0, Loss: 0.1180209368467331\n",
      "Epoch 500, Loss: 0.02063436061143875\n",
      "Epoch 1000, Loss: 0.02155616506934166\n",
      "Epoch 1500, Loss: 0.018717415630817413\n",
      "Epoch 2000, Loss: 0.018824072554707527\n",
      "Epoch 2500, Loss: 0.017888467758893967\n",
      "Epoch 3000, Loss: 0.016124241054058075\n",
      "Epoch 3500, Loss: 0.01749970205128193\n",
      "Epoch 4000, Loss: 0.017703808844089508\n",
      "Epoch 4500, Loss: 0.01747770980000496\n",
      "Epoch 4999, Loss: 0.016560036689043045\n",
      "Epoch 0, Loss: 0.26039308309555054\n",
      "Epoch 500, Loss: 0.04081304743885994\n",
      "Epoch 1000, Loss: 0.04020064324140549\n",
      "Epoch 1500, Loss: 0.033608097583055496\n",
      "Epoch 2000, Loss: 0.037130266427993774\n",
      "Epoch 2500, Loss: 0.034315984696149826\n",
      "Epoch 3000, Loss: 0.03387569636106491\n",
      "Epoch 3500, Loss: 0.034237250685691833\n",
      "Epoch 4000, Loss: 0.03125801682472229\n",
      "Epoch 4500, Loss: 0.03402886167168617\n",
      "Epoch 4999, Loss: 0.030957140028476715\n",
      "Epoch 0, Loss: 0.24179770052433014\n",
      "Epoch 500, Loss: 0.03269467502832413\n",
      "Epoch 1000, Loss: 0.029002761468291283\n",
      "Epoch 1500, Loss: 0.028031542897224426\n",
      "Epoch 2000, Loss: 0.025330809876322746\n",
      "Epoch 2500, Loss: 0.026000726968050003\n",
      "Epoch 3000, Loss: 0.025565244257450104\n",
      "Epoch 3500, Loss: 0.02429586462676525\n",
      "Epoch 4000, Loss: 0.025866376236081123\n",
      "Epoch 4500, Loss: 0.023947564885020256\n",
      "Epoch 4999, Loss: 0.024648314341902733\n",
      "Epoch 0, Loss: 0.2106090933084488\n",
      "Epoch 500, Loss: 0.02954505756497383\n",
      "Epoch 1000, Loss: 0.023791562765836716\n",
      "Epoch 1500, Loss: 0.020742587745189667\n",
      "Epoch 2000, Loss: 0.023443955928087234\n",
      "Epoch 2500, Loss: 0.02201073057949543\n",
      "Epoch 3000, Loss: 0.023764602839946747\n",
      "Epoch 3500, Loss: 0.022215092554688454\n",
      "Epoch 4000, Loss: 0.02209310233592987\n",
      "Epoch 4500, Loss: 0.024405747652053833\n",
      "Epoch 4999, Loss: 0.024821029976010323\n",
      "--- Experiment 2/20 ---\n",
      "Epoch 0, Loss: 0.1394960880279541\n",
      "Epoch 500, Loss: 0.023557305335998535\n",
      "Epoch 1000, Loss: 0.020236250013113022\n",
      "Epoch 1500, Loss: 0.020330820232629776\n",
      "Epoch 2000, Loss: 0.018917545676231384\n",
      "Epoch 2500, Loss: 0.018715888261795044\n",
      "Epoch 3000, Loss: 0.01589876413345337\n",
      "Epoch 3500, Loss: 0.018461715430021286\n",
      "Epoch 4000, Loss: 0.01823524758219719\n",
      "Epoch 4500, Loss: 0.018101323395967484\n",
      "Epoch 4999, Loss: 0.017765913158655167\n",
      "Epoch 0, Loss: 0.15041586756706238\n",
      "Epoch 500, Loss: 0.04554352909326553\n",
      "Epoch 1000, Loss: 0.038208022713661194\n",
      "Epoch 1500, Loss: 0.03682028502225876\n",
      "Epoch 2000, Loss: 0.03456646949052811\n",
      "Epoch 2500, Loss: 0.03145432844758034\n",
      "Epoch 3000, Loss: 0.03187916427850723\n",
      "Epoch 3500, Loss: 0.03102708049118519\n",
      "Epoch 4000, Loss: 0.029848553240299225\n",
      "Epoch 4500, Loss: 0.030201170593500137\n",
      "Epoch 4999, Loss: 0.028864670544862747\n",
      "Epoch 0, Loss: 0.18140000104904175\n",
      "Epoch 500, Loss: 0.03233623504638672\n",
      "Epoch 1000, Loss: 0.02799396961927414\n",
      "Epoch 1500, Loss: 0.025540422648191452\n",
      "Epoch 2000, Loss: 0.024823836982250214\n",
      "Epoch 2500, Loss: 0.024440165609121323\n",
      "Epoch 3000, Loss: 0.028817659243941307\n",
      "Epoch 3500, Loss: 0.026985157281160355\n",
      "Epoch 4000, Loss: 0.02561112307012081\n",
      "Epoch 4500, Loss: 0.023583225905895233\n",
      "Epoch 4999, Loss: 0.025113096460700035\n",
      "Epoch 0, Loss: 0.18746227025985718\n",
      "Epoch 500, Loss: 0.0288224034011364\n",
      "Epoch 1000, Loss: 0.02572876773774624\n",
      "Epoch 1500, Loss: 0.024712568148970604\n",
      "Epoch 2000, Loss: 0.02340487577021122\n",
      "Epoch 2500, Loss: 0.02437082678079605\n",
      "Epoch 3000, Loss: 0.021631132811307907\n",
      "Epoch 3500, Loss: 0.023313382640480995\n",
      "Epoch 4000, Loss: 0.023370813578367233\n",
      "Epoch 4500, Loss: 0.023157503455877304\n",
      "Epoch 4999, Loss: 0.022085271775722504\n",
      "--- Experiment 3/20 ---\n",
      "Epoch 0, Loss: 0.16396182775497437\n",
      "Epoch 500, Loss: 0.02461189031600952\n",
      "Epoch 1000, Loss: 0.019055712968111038\n",
      "Epoch 1500, Loss: 0.019814498722553253\n",
      "Epoch 2000, Loss: 0.018704509362578392\n",
      "Epoch 2500, Loss: 0.018525786697864532\n",
      "Epoch 3000, Loss: 0.018657486885786057\n",
      "Epoch 3500, Loss: 0.017493220046162605\n",
      "Epoch 4000, Loss: 0.018177982419729233\n",
      "Epoch 4500, Loss: 0.017970265820622444\n",
      "Epoch 4999, Loss: 0.01765581965446472\n",
      "Epoch 0, Loss: 0.16138605773448944\n",
      "Epoch 500, Loss: 0.041386302560567856\n",
      "Epoch 1000, Loss: 0.03995664790272713\n",
      "Epoch 1500, Loss: 0.0367136150598526\n",
      "Epoch 2000, Loss: 0.03396425023674965\n",
      "Epoch 2500, Loss: 0.03325037285685539\n",
      "Epoch 3000, Loss: 0.03189173713326454\n",
      "Epoch 3500, Loss: 0.033947065472602844\n",
      "Epoch 4000, Loss: 0.032740890979766846\n",
      "Epoch 4500, Loss: 0.03400816395878792\n",
      "Epoch 4999, Loss: 0.030723661184310913\n",
      "Epoch 0, Loss: 0.1341887265443802\n",
      "Epoch 500, Loss: 0.029791533946990967\n",
      "Epoch 1000, Loss: 0.027157817035913467\n",
      "Epoch 1500, Loss: 0.02712022326886654\n",
      "Epoch 2000, Loss: 0.02554495632648468\n",
      "Epoch 2500, Loss: 0.025201182812452316\n",
      "Epoch 3000, Loss: 0.023147540166974068\n",
      "Epoch 3500, Loss: 0.024129696190357208\n",
      "Epoch 4000, Loss: 0.023397231474518776\n",
      "Epoch 4500, Loss: 0.02578245848417282\n",
      "Epoch 4999, Loss: 0.027870342135429382\n",
      "Epoch 0, Loss: 0.2015688419342041\n",
      "Epoch 500, Loss: 0.028850942850112915\n",
      "Epoch 1000, Loss: 0.02448023110628128\n",
      "Epoch 1500, Loss: 0.021125730127096176\n",
      "Epoch 2000, Loss: 0.022541044279932976\n",
      "Epoch 2500, Loss: 0.020587105304002762\n",
      "Epoch 3000, Loss: 0.022247442975640297\n",
      "Epoch 3500, Loss: 0.023234091699123383\n",
      "Epoch 4000, Loss: 0.02365071140229702\n",
      "Epoch 4500, Loss: 0.021935470402240753\n",
      "Epoch 4999, Loss: 0.022087855264544487\n",
      "--- Experiment 4/20 ---\n",
      "Epoch 0, Loss: 0.12376242876052856\n",
      "Epoch 500, Loss: 0.02243306115269661\n",
      "Epoch 1000, Loss: 0.01787460222840309\n",
      "Epoch 1500, Loss: 0.018461789935827255\n",
      "Epoch 2000, Loss: 0.019110942259430885\n",
      "Epoch 2500, Loss: 0.01875118538737297\n",
      "Epoch 3000, Loss: 0.019097134470939636\n",
      "Epoch 3500, Loss: 0.01687179133296013\n",
      "Epoch 4000, Loss: 0.016169730573892593\n",
      "Epoch 4500, Loss: 0.01727694272994995\n",
      "Epoch 4999, Loss: 0.016465406864881516\n",
      "Epoch 0, Loss: 0.2827548682689667\n",
      "Epoch 500, Loss: 0.04084104672074318\n",
      "Epoch 1000, Loss: 0.03575894981622696\n",
      "Epoch 1500, Loss: 0.0333000086247921\n",
      "Epoch 2000, Loss: 0.03301346302032471\n",
      "Epoch 2500, Loss: 0.032328519970178604\n",
      "Epoch 3000, Loss: 0.029915357008576393\n",
      "Epoch 3500, Loss: 0.030095228925347328\n",
      "Epoch 4000, Loss: 0.030051007866859436\n",
      "Epoch 4500, Loss: 0.031834233552217484\n",
      "Epoch 4999, Loss: 0.02824697457253933\n",
      "Epoch 0, Loss: 0.31211182475090027\n",
      "Epoch 500, Loss: 0.03430268168449402\n",
      "Epoch 1000, Loss: 0.02803364023566246\n",
      "Epoch 1500, Loss: 0.027341851964592934\n",
      "Epoch 2000, Loss: 0.02480950951576233\n",
      "Epoch 2500, Loss: 0.025903943926095963\n",
      "Epoch 3000, Loss: 0.024534478783607483\n",
      "Epoch 3500, Loss: 0.023555967956781387\n",
      "Epoch 4000, Loss: 0.023831233382225037\n",
      "Epoch 4500, Loss: 0.02419660985469818\n",
      "Epoch 4999, Loss: 0.023456107825040817\n",
      "Epoch 0, Loss: 0.16380846500396729\n",
      "Epoch 500, Loss: 0.028599323704838753\n",
      "Epoch 1000, Loss: 0.025164928287267685\n",
      "Epoch 1500, Loss: 0.023177072405815125\n",
      "Epoch 2000, Loss: 0.02258945256471634\n",
      "Epoch 2500, Loss: 0.024852503091096878\n",
      "Epoch 3000, Loss: 0.023460960015654564\n",
      "Epoch 3500, Loss: 0.02174343168735504\n",
      "Epoch 4000, Loss: 0.022282717749476433\n",
      "Epoch 4500, Loss: 0.020540917292237282\n",
      "Epoch 4999, Loss: 0.022019851952791214\n",
      "--- Experiment 5/20 ---\n",
      "Epoch 0, Loss: 0.19368156790733337\n",
      "Epoch 500, Loss: 0.022287443280220032\n",
      "Epoch 1000, Loss: 0.019029000774025917\n",
      "Epoch 1500, Loss: 0.019641220569610596\n",
      "Epoch 2000, Loss: 0.02001068741083145\n",
      "Epoch 2500, Loss: 0.01860429346561432\n",
      "Epoch 3000, Loss: 0.01833285763859749\n",
      "Epoch 3500, Loss: 0.01715933158993721\n",
      "Epoch 4000, Loss: 0.017141500487923622\n",
      "Epoch 4500, Loss: 0.016606664285063744\n",
      "Epoch 4999, Loss: 0.01742073893547058\n",
      "Epoch 0, Loss: 0.232468843460083\n",
      "Epoch 500, Loss: 0.04162519797682762\n",
      "Epoch 1000, Loss: 0.040006015449762344\n",
      "Epoch 1500, Loss: 0.035408519208431244\n",
      "Epoch 2000, Loss: 0.032645486295223236\n",
      "Epoch 2500, Loss: 0.03227544203400612\n",
      "Epoch 3000, Loss: 0.031078815460205078\n",
      "Epoch 3500, Loss: 0.029586922377347946\n",
      "Epoch 4000, Loss: 0.029519226402044296\n",
      "Epoch 4500, Loss: 0.0321633480489254\n",
      "Epoch 4999, Loss: 0.02994241937994957\n",
      "Epoch 0, Loss: 0.20396868884563446\n",
      "Epoch 500, Loss: 0.032175954431295395\n",
      "Epoch 1000, Loss: 0.028168631717562675\n",
      "Epoch 1500, Loss: 0.02677728608250618\n",
      "Epoch 2000, Loss: 0.025436051189899445\n",
      "Epoch 2500, Loss: 0.025666911154985428\n",
      "Epoch 3000, Loss: 0.024300560355186462\n",
      "Epoch 3500, Loss: 0.023239973932504654\n",
      "Epoch 4000, Loss: 0.024408122524619102\n",
      "Epoch 4500, Loss: 0.023535557091236115\n",
      "Epoch 4999, Loss: 0.024258572608232498\n",
      "Epoch 0, Loss: 0.24524140357971191\n",
      "Epoch 500, Loss: 0.02818790078163147\n",
      "Epoch 1000, Loss: 0.02340741455554962\n",
      "Epoch 1500, Loss: 0.02388889342546463\n",
      "Epoch 2000, Loss: 0.02436373010277748\n",
      "Epoch 2500, Loss: 0.02268938720226288\n",
      "Epoch 3000, Loss: 0.021259969100356102\n",
      "Epoch 3500, Loss: 0.02140093594789505\n",
      "Epoch 4000, Loss: 0.0238087996840477\n",
      "Epoch 4500, Loss: 0.02181471511721611\n",
      "Epoch 4999, Loss: 0.023313533514738083\n",
      "--- Experiment 6/20 ---\n",
      "Epoch 0, Loss: 0.1501007229089737\n",
      "Epoch 500, Loss: 0.02433551475405693\n",
      "Epoch 1000, Loss: 0.01993902027606964\n",
      "Epoch 1500, Loss: 0.01740305684506893\n",
      "Epoch 2000, Loss: 0.01765637844800949\n",
      "Epoch 2500, Loss: 0.017228221520781517\n",
      "Epoch 3000, Loss: 0.017641399055719376\n",
      "Epoch 3500, Loss: 0.018287841230630875\n",
      "Epoch 4000, Loss: 0.017529405653476715\n",
      "Epoch 4500, Loss: 0.01581171341240406\n",
      "Epoch 4999, Loss: 0.016408443450927734\n",
      "Epoch 0, Loss: 0.14699029922485352\n",
      "Epoch 500, Loss: 0.043827835470438004\n",
      "Epoch 1000, Loss: 0.04241333529353142\n",
      "Epoch 1500, Loss: 0.0374029278755188\n",
      "Epoch 2000, Loss: 0.03828386217355728\n",
      "Epoch 2500, Loss: 0.035924993455410004\n",
      "Epoch 3000, Loss: 0.03348638117313385\n",
      "Epoch 3500, Loss: 0.031656283885240555\n",
      "Epoch 4000, Loss: 0.032144904136657715\n",
      "Epoch 4500, Loss: 0.030949268490076065\n",
      "Epoch 4999, Loss: 0.03233923017978668\n",
      "Epoch 0, Loss: 0.23589764535427094\n",
      "Epoch 500, Loss: 0.03418216481804848\n",
      "Epoch 1000, Loss: 0.028633341193199158\n",
      "Epoch 1500, Loss: 0.02709249034523964\n",
      "Epoch 2000, Loss: 0.026511799544095993\n",
      "Epoch 2500, Loss: 0.025769999250769615\n",
      "Epoch 3000, Loss: 0.024676667526364326\n",
      "Epoch 3500, Loss: 0.024889150634407997\n",
      "Epoch 4000, Loss: 0.026015467941761017\n",
      "Epoch 4500, Loss: 0.02516195923089981\n",
      "Epoch 4999, Loss: 0.025108380243182182\n",
      "Epoch 0, Loss: 0.12716040015220642\n",
      "Epoch 500, Loss: 0.028305411338806152\n",
      "Epoch 1000, Loss: 0.021391479298472404\n",
      "Epoch 1500, Loss: 0.022426778450608253\n",
      "Epoch 2000, Loss: 0.021014979109168053\n",
      "Epoch 2500, Loss: 0.023288732394576073\n",
      "Epoch 3000, Loss: 0.02174876444041729\n",
      "Epoch 3500, Loss: 0.02373046614229679\n",
      "Epoch 4000, Loss: 0.02286541648209095\n",
      "Epoch 4500, Loss: 0.01939326338469982\n",
      "Epoch 4999, Loss: 0.020208006724715233\n",
      "--- Experiment 7/20 ---\n",
      "Epoch 0, Loss: 0.2143525779247284\n",
      "Epoch 500, Loss: 0.02230093628168106\n",
      "Epoch 1000, Loss: 0.0202404223382473\n",
      "Epoch 1500, Loss: 0.020079076290130615\n",
      "Epoch 2000, Loss: 0.019617829471826553\n",
      "Epoch 2500, Loss: 0.017398711293935776\n",
      "Epoch 3000, Loss: 0.018462058156728745\n",
      "Epoch 3500, Loss: 0.017250362783670425\n",
      "Epoch 4000, Loss: 0.01739223301410675\n",
      "Epoch 4500, Loss: 0.016849374398589134\n",
      "Epoch 4999, Loss: 0.016507277265191078\n",
      "Epoch 0, Loss: 0.25349873304367065\n",
      "Epoch 500, Loss: 0.04195048660039902\n",
      "Epoch 1000, Loss: 0.036689963191747665\n",
      "Epoch 1500, Loss: 0.03216632083058357\n",
      "Epoch 2000, Loss: 0.032842863351106644\n",
      "Epoch 2500, Loss: 0.03135138377547264\n",
      "Epoch 3000, Loss: 0.03138163685798645\n",
      "Epoch 3500, Loss: 0.029847748577594757\n",
      "Epoch 4000, Loss: 0.029872996732592583\n",
      "Epoch 4500, Loss: 0.032133739441633224\n",
      "Epoch 4999, Loss: 0.02928038500249386\n",
      "Epoch 0, Loss: 0.15100984275341034\n",
      "Epoch 500, Loss: 0.033040303736925125\n",
      "Epoch 1000, Loss: 0.030813533812761307\n",
      "Epoch 1500, Loss: 0.02554459124803543\n",
      "Epoch 2000, Loss: 0.025783132761716843\n",
      "Epoch 2500, Loss: 0.024419765919446945\n",
      "Epoch 3000, Loss: 0.024615133181214333\n",
      "Epoch 3500, Loss: 0.024413641542196274\n",
      "Epoch 4000, Loss: 0.025363897904753685\n",
      "Epoch 4500, Loss: 0.02420024760067463\n",
      "Epoch 4999, Loss: 0.02381742186844349\n",
      "Epoch 0, Loss: 0.15964774787425995\n",
      "Epoch 500, Loss: 0.027991898357868195\n",
      "Epoch 1000, Loss: 0.026121435686945915\n",
      "Epoch 1500, Loss: 0.023290732875466347\n",
      "Epoch 2000, Loss: 0.024970900267362595\n",
      "Epoch 2500, Loss: 0.023774193599820137\n",
      "Epoch 3000, Loss: 0.022694995626807213\n",
      "Epoch 3500, Loss: 0.02192297950387001\n",
      "Epoch 4000, Loss: 0.023744437843561172\n",
      "Epoch 4500, Loss: 0.02200731635093689\n",
      "Epoch 4999, Loss: 0.02238459140062332\n",
      "--- Experiment 8/20 ---\n",
      "Epoch 0, Loss: 0.21600520610809326\n",
      "Epoch 500, Loss: 0.02569476328790188\n",
      "Epoch 1000, Loss: 0.022981910035014153\n",
      "Epoch 1500, Loss: 0.020922191441059113\n",
      "Epoch 2000, Loss: 0.02104099467396736\n",
      "Epoch 2500, Loss: 0.02229783684015274\n",
      "Epoch 3000, Loss: 0.018513280898332596\n",
      "Epoch 3500, Loss: 0.020377647131681442\n",
      "Epoch 4000, Loss: 0.018072400242090225\n",
      "Epoch 4500, Loss: 0.019704487174749374\n",
      "Epoch 4999, Loss: 0.019155345857143402\n",
      "Epoch 0, Loss: 0.11624802649021149\n",
      "Epoch 500, Loss: 0.03956194967031479\n",
      "Epoch 1000, Loss: 0.03603731468319893\n",
      "Epoch 1500, Loss: 0.03314361721277237\n",
      "Epoch 2000, Loss: 0.03330964967608452\n",
      "Epoch 2500, Loss: 0.03301088511943817\n",
      "Epoch 3000, Loss: 0.03144460543990135\n",
      "Epoch 3500, Loss: 0.03137589618563652\n",
      "Epoch 4000, Loss: 0.03295843303203583\n",
      "Epoch 4500, Loss: 0.029500748962163925\n",
      "Epoch 4999, Loss: 0.029929712414741516\n",
      "Epoch 0, Loss: 0.16973863542079926\n",
      "Epoch 500, Loss: 0.03230871260166168\n",
      "Epoch 1000, Loss: 0.027216006070375443\n",
      "Epoch 1500, Loss: 0.02677878923714161\n",
      "Epoch 2000, Loss: 0.025054246187210083\n",
      "Epoch 2500, Loss: 0.027750331908464432\n",
      "Epoch 3000, Loss: 0.024937815964221954\n",
      "Epoch 3500, Loss: 0.024272939190268517\n",
      "Epoch 4000, Loss: 0.024006811901926994\n",
      "Epoch 4500, Loss: 0.02358531765639782\n",
      "Epoch 4999, Loss: 0.02426180988550186\n",
      "Epoch 0, Loss: 0.20039942860603333\n",
      "Epoch 500, Loss: 0.03134090453386307\n",
      "Epoch 1000, Loss: 0.02700868248939514\n",
      "Epoch 1500, Loss: 0.02495739236474037\n",
      "Epoch 2000, Loss: 0.02423899620771408\n",
      "Epoch 2500, Loss: 0.024795927107334137\n",
      "Epoch 3000, Loss: 0.02407223917543888\n",
      "Epoch 3500, Loss: 0.024433817714452744\n",
      "Epoch 4000, Loss: 0.021638816222548485\n",
      "Epoch 4500, Loss: 0.025853820145130157\n",
      "Epoch 4999, Loss: 0.02403949573636055\n",
      "--- Experiment 9/20 ---\n",
      "Epoch 0, Loss: 0.2748219668865204\n",
      "Epoch 500, Loss: 0.023428939282894135\n",
      "Epoch 1000, Loss: 0.019319329410791397\n",
      "Epoch 1500, Loss: 0.021176395937800407\n",
      "Epoch 2000, Loss: 0.017191534861922264\n",
      "Epoch 2500, Loss: 0.019563155248761177\n",
      "Epoch 3000, Loss: 0.01810583472251892\n",
      "Epoch 3500, Loss: 0.018096180632710457\n",
      "Epoch 4000, Loss: 0.017555980011820793\n",
      "Epoch 4500, Loss: 0.01859939470887184\n",
      "Epoch 4999, Loss: 0.01648576557636261\n",
      "Epoch 0, Loss: 0.12913456559181213\n",
      "Epoch 500, Loss: 0.04124538600444794\n",
      "Epoch 1000, Loss: 0.0367063507437706\n",
      "Epoch 1500, Loss: 0.03322945907711983\n",
      "Epoch 2000, Loss: 0.035112738609313965\n",
      "Epoch 2500, Loss: 0.03380876034498215\n",
      "Epoch 3000, Loss: 0.03185104578733444\n",
      "Epoch 3500, Loss: 0.03264204412698746\n",
      "Epoch 4000, Loss: 0.03133228048682213\n",
      "Epoch 4500, Loss: 0.030412059277296066\n",
      "Epoch 4999, Loss: 0.029864560812711716\n",
      "Epoch 0, Loss: 0.2738794982433319\n",
      "Epoch 500, Loss: 0.03308151289820671\n",
      "Epoch 1000, Loss: 0.02737700380384922\n",
      "Epoch 1500, Loss: 0.025440454483032227\n",
      "Epoch 2000, Loss: 0.026245422661304474\n",
      "Epoch 2500, Loss: 0.023265399038791656\n",
      "Epoch 3000, Loss: 0.024944555014371872\n",
      "Epoch 3500, Loss: 0.02428525686264038\n",
      "Epoch 4000, Loss: 0.024347443133592606\n",
      "Epoch 4500, Loss: 0.02453227899968624\n",
      "Epoch 4999, Loss: 0.023775573819875717\n",
      "Epoch 0, Loss: 0.139176145195961\n",
      "Epoch 500, Loss: 0.024742774665355682\n",
      "Epoch 1000, Loss: 0.0245014950633049\n",
      "Epoch 1500, Loss: 0.02387128211557865\n",
      "Epoch 2000, Loss: 0.02299981564283371\n",
      "Epoch 2500, Loss: 0.021574147045612335\n",
      "Epoch 3000, Loss: 0.02098524570465088\n",
      "Epoch 3500, Loss: 0.02391701564192772\n",
      "Epoch 4000, Loss: 0.023039821535348892\n",
      "Epoch 4500, Loss: 0.022437293082475662\n",
      "Epoch 4999, Loss: 0.020485050976276398\n",
      "--- Experiment 10/20 ---\n",
      "Epoch 0, Loss: 0.19605599343776703\n",
      "Epoch 500, Loss: 0.02202926203608513\n",
      "Epoch 1000, Loss: 0.019049135968089104\n",
      "Epoch 1500, Loss: 0.018264926970005035\n",
      "Epoch 2000, Loss: 0.01669192500412464\n",
      "Epoch 2500, Loss: 0.019729452207684517\n",
      "Epoch 3000, Loss: 0.01802835986018181\n",
      "Epoch 3500, Loss: 0.01657285913825035\n",
      "Epoch 4000, Loss: 0.01630989834666252\n",
      "Epoch 4500, Loss: 0.019459575414657593\n",
      "Epoch 4999, Loss: 0.01620258390903473\n",
      "Epoch 0, Loss: 0.16065622866153717\n",
      "Epoch 500, Loss: 0.0400475449860096\n",
      "Epoch 1000, Loss: 0.036319293081760406\n",
      "Epoch 1500, Loss: 0.03142308443784714\n",
      "Epoch 2000, Loss: 0.03357337787747383\n",
      "Epoch 2500, Loss: 0.030599579215049744\n",
      "Epoch 3000, Loss: 0.031028378754854202\n",
      "Epoch 3500, Loss: 0.030003221705555916\n",
      "Epoch 4000, Loss: 0.03073417954146862\n",
      "Epoch 4500, Loss: 0.029814966022968292\n",
      "Epoch 4999, Loss: 0.026802707463502884\n",
      "Epoch 0, Loss: 0.1831347942352295\n",
      "Epoch 500, Loss: 0.031748488545417786\n",
      "Epoch 1000, Loss: 0.027869349345564842\n",
      "Epoch 1500, Loss: 0.024777594953775406\n",
      "Epoch 2000, Loss: 0.025215571746230125\n",
      "Epoch 2500, Loss: 0.024875784292817116\n",
      "Epoch 3000, Loss: 0.0244552381336689\n",
      "Epoch 3500, Loss: 0.032124973833560944\n",
      "Epoch 4000, Loss: 0.029176238924264908\n",
      "Epoch 4500, Loss: 0.028044503182172775\n",
      "Epoch 4999, Loss: 0.025696568191051483\n",
      "Epoch 0, Loss: 0.17978014051914215\n",
      "Epoch 500, Loss: 0.024395499378442764\n",
      "Epoch 1000, Loss: 0.025329863652586937\n",
      "Epoch 1500, Loss: 0.0222520362585783\n",
      "Epoch 2000, Loss: 0.022674813866615295\n",
      "Epoch 2500, Loss: 0.02265956997871399\n",
      "Epoch 3000, Loss: 0.022925881668925285\n",
      "Epoch 3500, Loss: 0.0199637059122324\n",
      "Epoch 4000, Loss: 0.023386087268590927\n",
      "Epoch 4500, Loss: 0.02369559556245804\n",
      "Epoch 4999, Loss: 0.022291673347353935\n",
      "--- Experiment 11/20 ---\n",
      "Epoch 0, Loss: 0.18151310086250305\n",
      "Epoch 500, Loss: 0.022571172565221786\n",
      "Epoch 1000, Loss: 0.019824769347906113\n",
      "Epoch 1500, Loss: 0.021050456911325455\n",
      "Epoch 2000, Loss: 0.017769532278180122\n",
      "Epoch 2500, Loss: 0.01798553392291069\n",
      "Epoch 3000, Loss: 0.019572606310248375\n",
      "Epoch 3500, Loss: 0.01657542772591114\n",
      "Epoch 4000, Loss: 0.01647043228149414\n",
      "Epoch 4500, Loss: 0.016945844516158104\n",
      "Epoch 4999, Loss: 0.017461581155657768\n",
      "Epoch 0, Loss: 0.14581745862960815\n",
      "Epoch 500, Loss: 0.04052210599184036\n",
      "Epoch 1000, Loss: 0.040042318403720856\n",
      "Epoch 1500, Loss: 0.03758222982287407\n",
      "Epoch 2000, Loss: 0.034960195422172546\n",
      "Epoch 2500, Loss: 0.031830769032239914\n",
      "Epoch 3000, Loss: 0.032424233853816986\n",
      "Epoch 3500, Loss: 0.030883051455020905\n",
      "Epoch 4000, Loss: 0.030464813113212585\n",
      "Epoch 4500, Loss: 0.031204823404550552\n",
      "Epoch 4999, Loss: 0.03059212863445282\n",
      "Epoch 0, Loss: 0.16220897436141968\n",
      "Epoch 500, Loss: 0.031454913318157196\n",
      "Epoch 1000, Loss: 0.027502935379743576\n",
      "Epoch 1500, Loss: 0.025656715035438538\n",
      "Epoch 2000, Loss: 0.024212511256337166\n",
      "Epoch 2500, Loss: 0.0248631052672863\n",
      "Epoch 3000, Loss: 0.025031477212905884\n",
      "Epoch 3500, Loss: 0.022846676409244537\n",
      "Epoch 4000, Loss: 0.02333167754113674\n",
      "Epoch 4500, Loss: 0.02839910238981247\n",
      "Epoch 4999, Loss: 0.0253082774579525\n",
      "Epoch 0, Loss: 0.1143094077706337\n",
      "Epoch 500, Loss: 0.027227964252233505\n",
      "Epoch 1000, Loss: 0.024997562170028687\n",
      "Epoch 1500, Loss: 0.02416282147169113\n",
      "Epoch 2000, Loss: 0.022200314328074455\n",
      "Epoch 2500, Loss: 0.02324911765754223\n",
      "Epoch 3000, Loss: 0.024329889565706253\n",
      "Epoch 3500, Loss: 0.021718235686421394\n",
      "Epoch 4000, Loss: 0.024824481457471848\n",
      "Epoch 4500, Loss: 0.01903747394680977\n",
      "Epoch 4999, Loss: 0.022116072475910187\n",
      "--- Experiment 12/20 ---\n",
      "Epoch 0, Loss: 0.2740493416786194\n",
      "Epoch 500, Loss: 0.024787621572613716\n",
      "Epoch 1000, Loss: 0.019308526068925858\n",
      "Epoch 1500, Loss: 0.017107781022787094\n",
      "Epoch 2000, Loss: 0.01932963728904724\n",
      "Epoch 2500, Loss: 0.01757521741092205\n",
      "Epoch 3000, Loss: 0.017679035663604736\n",
      "Epoch 3500, Loss: 0.017712123692035675\n",
      "Epoch 4000, Loss: 0.01637091301381588\n",
      "Epoch 4500, Loss: 0.01666072942316532\n",
      "Epoch 4999, Loss: 0.01902405545115471\n",
      "Epoch 0, Loss: 0.2089485079050064\n",
      "Epoch 500, Loss: 0.042113251984119415\n",
      "Epoch 1000, Loss: 0.037764836102724075\n",
      "Epoch 1500, Loss: 0.035104233771562576\n",
      "Epoch 2000, Loss: 0.032726041972637177\n",
      "Epoch 2500, Loss: 0.03308546170592308\n",
      "Epoch 3000, Loss: 0.03075244277715683\n",
      "Epoch 3500, Loss: 0.03163200989365578\n",
      "Epoch 4000, Loss: 0.03122738003730774\n",
      "Epoch 4500, Loss: 0.030499959364533424\n",
      "Epoch 4999, Loss: 0.032205577939748764\n",
      "Epoch 0, Loss: 0.1941700428724289\n",
      "Epoch 500, Loss: 0.03344040364027023\n",
      "Epoch 1000, Loss: 0.030557703226804733\n",
      "Epoch 1500, Loss: 0.02726641111075878\n",
      "Epoch 2000, Loss: 0.027136610820889473\n",
      "Epoch 2500, Loss: 0.025173736736178398\n",
      "Epoch 3000, Loss: 0.024882622063159943\n",
      "Epoch 3500, Loss: 0.024037782102823257\n",
      "Epoch 4000, Loss: 0.029422922059893608\n",
      "Epoch 4500, Loss: 0.02639290876686573\n",
      "Epoch 4999, Loss: 0.025550896301865578\n",
      "Epoch 0, Loss: 0.17990495264530182\n",
      "Epoch 500, Loss: 0.027324944734573364\n",
      "Epoch 1000, Loss: 0.02466989867389202\n",
      "Epoch 1500, Loss: 0.01907246932387352\n",
      "Epoch 2000, Loss: 0.019791267812252045\n",
      "Epoch 2500, Loss: 0.02413436956703663\n",
      "Epoch 3000, Loss: 0.022748753428459167\n",
      "Epoch 3500, Loss: 0.020894870162010193\n",
      "Epoch 4000, Loss: 0.024136602878570557\n",
      "Epoch 4500, Loss: 0.023248419165611267\n",
      "Epoch 4999, Loss: 0.021458040922880173\n",
      "--- Experiment 13/20 ---\n",
      "Epoch 0, Loss: 0.2036413550376892\n",
      "Epoch 500, Loss: 0.022625315934419632\n",
      "Epoch 1000, Loss: 0.02026224136352539\n",
      "Epoch 1500, Loss: 0.018879486247897148\n",
      "Epoch 2000, Loss: 0.01869174838066101\n",
      "Epoch 2500, Loss: 0.017847098410129547\n",
      "Epoch 3000, Loss: 0.018323492258787155\n",
      "Epoch 3500, Loss: 0.016694054007530212\n",
      "Epoch 4000, Loss: 0.0195391234010458\n",
      "Epoch 4500, Loss: 0.020996633917093277\n",
      "Epoch 4999, Loss: 0.018570920452475548\n",
      "Epoch 0, Loss: 0.19007915258407593\n",
      "Epoch 500, Loss: 0.04124797135591507\n",
      "Epoch 1000, Loss: 0.03632522374391556\n",
      "Epoch 1500, Loss: 0.033781811594963074\n",
      "Epoch 2000, Loss: 0.032512366771698\n",
      "Epoch 2500, Loss: 0.03314859792590141\n",
      "Epoch 3000, Loss: 0.029297199100255966\n",
      "Epoch 3500, Loss: 0.02863464131951332\n",
      "Epoch 4000, Loss: 0.028211232274770737\n",
      "Epoch 4500, Loss: 0.028572315350174904\n",
      "Epoch 4999, Loss: 0.02768689952790737\n",
      "Epoch 0, Loss: 0.22194446623325348\n",
      "Epoch 500, Loss: 0.031142834573984146\n",
      "Epoch 1000, Loss: 0.026600219309329987\n",
      "Epoch 1500, Loss: 0.025026485323905945\n",
      "Epoch 2000, Loss: 0.0280121061950922\n",
      "Epoch 2500, Loss: 0.02639033831655979\n",
      "Epoch 3000, Loss: 0.025463908910751343\n",
      "Epoch 3500, Loss: 0.024507803842425346\n",
      "Epoch 4000, Loss: 0.02855628728866577\n",
      "Epoch 4500, Loss: 0.02698317915201187\n",
      "Epoch 4999, Loss: 0.0249471552670002\n",
      "Epoch 0, Loss: 0.1720317155122757\n",
      "Epoch 500, Loss: 0.03094402700662613\n",
      "Epoch 1000, Loss: 0.024745911359786987\n",
      "Epoch 1500, Loss: 0.024176282808184624\n",
      "Epoch 2000, Loss: 0.023199936375021935\n",
      "Epoch 2500, Loss: 0.02121860906481743\n",
      "Epoch 3000, Loss: 0.02198830619454384\n",
      "Epoch 3500, Loss: 0.020509023219347\n",
      "Epoch 4000, Loss: 0.021802296862006187\n",
      "Epoch 4500, Loss: 0.02329009398818016\n",
      "Epoch 4999, Loss: 0.021654358133673668\n",
      "--- Experiment 14/20 ---\n",
      "Epoch 0, Loss: 0.2538447976112366\n",
      "Epoch 500, Loss: 0.023989465087652206\n",
      "Epoch 1000, Loss: 0.018813207745552063\n",
      "Epoch 1500, Loss: 0.020418643951416016\n",
      "Epoch 2000, Loss: 0.01763898879289627\n",
      "Epoch 2500, Loss: 0.018847182393074036\n",
      "Epoch 3000, Loss: 0.01911519654095173\n",
      "Epoch 3500, Loss: 0.01827912963926792\n",
      "Epoch 4000, Loss: 0.016684021800756454\n",
      "Epoch 4500, Loss: 0.01758664660155773\n",
      "Epoch 4999, Loss: 0.016300393268465996\n",
      "Epoch 0, Loss: 0.15412990748882294\n",
      "Epoch 500, Loss: 0.04090568423271179\n",
      "Epoch 1000, Loss: 0.03463544324040413\n",
      "Epoch 1500, Loss: 0.03436032682657242\n",
      "Epoch 2000, Loss: 0.031835779547691345\n",
      "Epoch 2500, Loss: 0.032036054879426956\n",
      "Epoch 3000, Loss: 0.029834706336259842\n",
      "Epoch 3500, Loss: 0.02986951544880867\n",
      "Epoch 4000, Loss: 0.03100426495075226\n",
      "Epoch 4500, Loss: 0.02778218686580658\n",
      "Epoch 4999, Loss: 0.028744356706738472\n",
      "Epoch 0, Loss: 0.2539415955543518\n",
      "Epoch 500, Loss: 0.03378285467624664\n",
      "Epoch 1000, Loss: 0.028039852157235146\n",
      "Epoch 1500, Loss: 0.027778396382927895\n",
      "Epoch 2000, Loss: 0.024837341159582138\n",
      "Epoch 2500, Loss: 0.025441499426960945\n",
      "Epoch 3000, Loss: 0.024392569437623024\n",
      "Epoch 3500, Loss: 0.024895114824175835\n",
      "Epoch 4000, Loss: 0.023865075781941414\n",
      "Epoch 4500, Loss: 0.024209536612033844\n",
      "Epoch 4999, Loss: 0.023969855159521103\n",
      "Epoch 0, Loss: 0.2016458809375763\n",
      "Epoch 500, Loss: 0.029726453125476837\n",
      "Epoch 1000, Loss: 0.02535873092710972\n",
      "Epoch 1500, Loss: 0.025049153715372086\n",
      "Epoch 2000, Loss: 0.022708391770720482\n",
      "Epoch 2500, Loss: 0.022158432751893997\n",
      "Epoch 3000, Loss: 0.021534118801355362\n",
      "Epoch 3500, Loss: 0.019811788573861122\n",
      "Epoch 4000, Loss: 0.022664804011583328\n",
      "Epoch 4500, Loss: 0.020151296630501747\n",
      "Epoch 4999, Loss: 0.02141571044921875\n",
      "--- Experiment 15/20 ---\n",
      "Epoch 0, Loss: 0.18792058527469635\n",
      "Epoch 500, Loss: 0.0236219335347414\n",
      "Epoch 1000, Loss: 0.020411044359207153\n",
      "Epoch 1500, Loss: 0.01813986897468567\n",
      "Epoch 2000, Loss: 0.018140044063329697\n",
      "Epoch 2500, Loss: 0.017662789672613144\n",
      "Epoch 3000, Loss: 0.01638104021549225\n",
      "Epoch 3500, Loss: 0.020014524459838867\n",
      "Epoch 4000, Loss: 0.018548185005784035\n",
      "Epoch 4500, Loss: 0.016992677003145218\n",
      "Epoch 4999, Loss: 0.016607318073511124\n",
      "Epoch 0, Loss: 0.15886364877223969\n",
      "Epoch 500, Loss: 0.041298992931842804\n",
      "Epoch 1000, Loss: 0.03717755898833275\n",
      "Epoch 1500, Loss: 0.0339558869600296\n",
      "Epoch 2000, Loss: 0.033896345645189285\n",
      "Epoch 2500, Loss: 0.031135817989706993\n",
      "Epoch 3000, Loss: 0.03151567280292511\n",
      "Epoch 3500, Loss: 0.030923470854759216\n",
      "Epoch 4000, Loss: 0.031051592901349068\n",
      "Epoch 4500, Loss: 0.03258850798010826\n",
      "Epoch 4999, Loss: 0.028652619570493698\n",
      "Epoch 0, Loss: 0.13341732323169708\n",
      "Epoch 500, Loss: 0.034043364226818085\n",
      "Epoch 1000, Loss: 0.029354162514209747\n",
      "Epoch 1500, Loss: 0.02701127529144287\n",
      "Epoch 2000, Loss: 0.026262152940034866\n",
      "Epoch 2500, Loss: 0.02599964290857315\n",
      "Epoch 3000, Loss: 0.02457381784915924\n",
      "Epoch 3500, Loss: 0.025822389870882034\n",
      "Epoch 4000, Loss: 0.02667045220732689\n",
      "Epoch 4500, Loss: 0.02441065013408661\n",
      "Epoch 4999, Loss: 0.02859867364168167\n",
      "Epoch 0, Loss: 0.2657189965248108\n",
      "Epoch 500, Loss: 0.029280880466103554\n",
      "Epoch 1000, Loss: 0.02471145987510681\n",
      "Epoch 1500, Loss: 0.023891013115644455\n",
      "Epoch 2000, Loss: 0.020098011940717697\n",
      "Epoch 2500, Loss: 0.02275913953781128\n",
      "Epoch 3000, Loss: 0.023045044392347336\n",
      "Epoch 3500, Loss: 0.02327299304306507\n",
      "Epoch 4000, Loss: 0.025309719145298004\n",
      "Epoch 4500, Loss: 0.02362877130508423\n",
      "Epoch 4999, Loss: 0.023121461272239685\n",
      "--- Experiment 16/20 ---\n",
      "Epoch 0, Loss: 0.22717487812042236\n",
      "Epoch 500, Loss: 0.02429099753499031\n",
      "Epoch 1000, Loss: 0.02202790230512619\n",
      "Epoch 1500, Loss: 0.019659265875816345\n",
      "Epoch 2000, Loss: 0.01936274766921997\n",
      "Epoch 2500, Loss: 0.02104036696255207\n",
      "Epoch 3000, Loss: 0.018065888434648514\n",
      "Epoch 3500, Loss: 0.019342871382832527\n",
      "Epoch 4000, Loss: 0.01797519251704216\n",
      "Epoch 4500, Loss: 0.01944853737950325\n",
      "Epoch 4999, Loss: 0.017055662348866463\n",
      "Epoch 0, Loss: 0.12894801795482635\n",
      "Epoch 500, Loss: 0.04188820719718933\n",
      "Epoch 1000, Loss: 0.03422962501645088\n",
      "Epoch 1500, Loss: 0.033692046999931335\n",
      "Epoch 2000, Loss: 0.03229794651269913\n",
      "Epoch 2500, Loss: 0.03004850447177887\n",
      "Epoch 3000, Loss: 0.029545307159423828\n",
      "Epoch 3500, Loss: 0.02956387773156166\n",
      "Epoch 4000, Loss: 0.029346348717808723\n",
      "Epoch 4500, Loss: 0.02844098024070263\n",
      "Epoch 4999, Loss: 0.0273303110152483\n",
      "Epoch 0, Loss: 0.22496119141578674\n",
      "Epoch 500, Loss: 0.03355346620082855\n",
      "Epoch 1000, Loss: 0.02773439511656761\n",
      "Epoch 1500, Loss: 0.026019107550382614\n",
      "Epoch 2000, Loss: 0.025804830715060234\n",
      "Epoch 2500, Loss: 0.030810285359621048\n",
      "Epoch 3000, Loss: 0.02617826871573925\n",
      "Epoch 3500, Loss: 0.02649974822998047\n",
      "Epoch 4000, Loss: 0.02453208714723587\n",
      "Epoch 4500, Loss: 0.02439231052994728\n",
      "Epoch 4999, Loss: 0.02533373236656189\n",
      "Epoch 0, Loss: 0.12051250785589218\n",
      "Epoch 500, Loss: 0.03196251392364502\n",
      "Epoch 1000, Loss: 0.025599785149097443\n",
      "Epoch 1500, Loss: 0.021120691671967506\n",
      "Epoch 2000, Loss: 0.021296244114637375\n",
      "Epoch 2500, Loss: 0.022754454985260963\n",
      "Epoch 3000, Loss: 0.024745749309659004\n",
      "Epoch 3500, Loss: 0.019694726914167404\n",
      "Epoch 4000, Loss: 0.022722329944372177\n",
      "Epoch 4500, Loss: 0.021375685930252075\n",
      "Epoch 4999, Loss: 0.019125182181596756\n",
      "--- Experiment 17/20 ---\n",
      "Epoch 0, Loss: 0.15350309014320374\n",
      "Epoch 500, Loss: 0.022730384021997452\n",
      "Epoch 1000, Loss: 0.02006661146879196\n",
      "Epoch 1500, Loss: 0.017329033464193344\n",
      "Epoch 2000, Loss: 0.018253330141305923\n",
      "Epoch 2500, Loss: 0.018486984074115753\n",
      "Epoch 3000, Loss: 0.016897529363632202\n",
      "Epoch 3500, Loss: 0.018747154623270035\n",
      "Epoch 4000, Loss: 0.01815178245306015\n",
      "Epoch 4500, Loss: 0.019296269863843918\n",
      "Epoch 4999, Loss: 0.016750440001487732\n",
      "Epoch 0, Loss: 0.12243245542049408\n",
      "Epoch 500, Loss: 0.04148514196276665\n",
      "Epoch 1000, Loss: 0.03701538220047951\n",
      "Epoch 1500, Loss: 0.03510359674692154\n",
      "Epoch 2000, Loss: 0.03277071192860603\n",
      "Epoch 2500, Loss: 0.030351364985108376\n",
      "Epoch 3000, Loss: 0.030259450897574425\n",
      "Epoch 3500, Loss: 0.031509559601545334\n",
      "Epoch 4000, Loss: 0.032463837414979935\n",
      "Epoch 4500, Loss: 0.03104718029499054\n",
      "Epoch 4999, Loss: 0.032642338424921036\n",
      "Epoch 0, Loss: 0.16516424715518951\n",
      "Epoch 500, Loss: 0.03219541907310486\n",
      "Epoch 1000, Loss: 0.02626075968146324\n",
      "Epoch 1500, Loss: 0.027183301746845245\n",
      "Epoch 2000, Loss: 0.02553591877222061\n",
      "Epoch 2500, Loss: 0.02947298251092434\n",
      "Epoch 3000, Loss: 0.027725107967853546\n",
      "Epoch 3500, Loss: 0.02446208894252777\n",
      "Epoch 4000, Loss: 0.02344944141805172\n",
      "Epoch 4500, Loss: 0.02414736896753311\n",
      "Epoch 4999, Loss: 0.02425655536353588\n",
      "Epoch 0, Loss: 0.2063719630241394\n",
      "Epoch 500, Loss: 0.03030388057231903\n",
      "Epoch 1000, Loss: 0.022461548447608948\n",
      "Epoch 1500, Loss: 0.02251104637980461\n",
      "Epoch 2000, Loss: 0.023434491828083992\n",
      "Epoch 2500, Loss: 0.022874929010868073\n",
      "Epoch 3000, Loss: 0.0225331112742424\n",
      "Epoch 3500, Loss: 0.023235980421304703\n",
      "Epoch 4000, Loss: 0.022177143022418022\n",
      "Epoch 4500, Loss: 0.02090974897146225\n",
      "Epoch 4999, Loss: 0.02103392593562603\n",
      "--- Experiment 18/20 ---\n",
      "Epoch 0, Loss: 0.2432616949081421\n",
      "Epoch 500, Loss: 0.022142168134450912\n",
      "Epoch 1000, Loss: 0.020107457414269447\n",
      "Epoch 1500, Loss: 0.017624713480472565\n",
      "Epoch 2000, Loss: 0.018841784447431564\n",
      "Epoch 2500, Loss: 0.017416659742593765\n",
      "Epoch 3000, Loss: 0.016239825636148453\n",
      "Epoch 3500, Loss: 0.017496434971690178\n",
      "Epoch 4000, Loss: 0.016322020441293716\n",
      "Epoch 4500, Loss: 0.01719028502702713\n",
      "Epoch 4999, Loss: 0.017195919528603554\n",
      "Epoch 0, Loss: 0.39053940773010254\n",
      "Epoch 500, Loss: 0.04305722191929817\n",
      "Epoch 1000, Loss: 0.03974365070462227\n",
      "Epoch 1500, Loss: 0.03783684968948364\n",
      "Epoch 2000, Loss: 0.03699995577335358\n",
      "Epoch 2500, Loss: 0.03385932743549347\n",
      "Epoch 3000, Loss: 0.032039232552051544\n",
      "Epoch 3500, Loss: 0.03246846795082092\n",
      "Epoch 4000, Loss: 0.032138433307409286\n",
      "Epoch 4500, Loss: 0.03086220659315586\n",
      "Epoch 4999, Loss: 0.029328562319278717\n",
      "Epoch 0, Loss: 0.2760799527168274\n",
      "Epoch 500, Loss: 0.032095905393362045\n",
      "Epoch 1000, Loss: 0.025744833052158356\n",
      "Epoch 1500, Loss: 0.027379650622606277\n",
      "Epoch 2000, Loss: 0.026788314804434776\n",
      "Epoch 2500, Loss: 0.024242855608463287\n",
      "Epoch 3000, Loss: 0.023536765947937965\n",
      "Epoch 3500, Loss: 0.021270878612995148\n",
      "Epoch 4000, Loss: 0.02521003782749176\n",
      "Epoch 4500, Loss: 0.02349647879600525\n",
      "Epoch 4999, Loss: 0.026438001543283463\n",
      "Epoch 0, Loss: 0.16756807267665863\n",
      "Epoch 500, Loss: 0.02670816332101822\n",
      "Epoch 1000, Loss: 0.02475273236632347\n",
      "Epoch 1500, Loss: 0.025220634415745735\n",
      "Epoch 2000, Loss: 0.022715693339705467\n",
      "Epoch 2500, Loss: 0.024905342608690262\n",
      "Epoch 3000, Loss: 0.02304047718644142\n",
      "Epoch 3500, Loss: 0.02276960015296936\n",
      "Epoch 4000, Loss: 0.02221260778605938\n",
      "Epoch 4500, Loss: 0.023097725585103035\n",
      "Epoch 4999, Loss: 0.022778015583753586\n",
      "--- Experiment 19/20 ---\n",
      "Epoch 0, Loss: 0.15754450857639313\n",
      "Epoch 500, Loss: 0.023897236213088036\n",
      "Epoch 1000, Loss: 0.019283344969153404\n",
      "Epoch 1500, Loss: 0.018924109637737274\n",
      "Epoch 2000, Loss: 0.018829450011253357\n",
      "Epoch 2500, Loss: 0.017783787101507187\n",
      "Epoch 3000, Loss: 0.016554923728108406\n",
      "Epoch 3500, Loss: 0.01821230724453926\n",
      "Epoch 4000, Loss: 0.015864340588450432\n",
      "Epoch 4500, Loss: 0.017707310616970062\n",
      "Epoch 4999, Loss: 0.015539893880486488\n",
      "Epoch 0, Loss: 0.2687883675098419\n",
      "Epoch 500, Loss: 0.04269963130354881\n",
      "Epoch 1000, Loss: 0.040347423404455185\n",
      "Epoch 1500, Loss: 0.03530452772974968\n",
      "Epoch 2000, Loss: 0.03263087943196297\n",
      "Epoch 2500, Loss: 0.03269379213452339\n",
      "Epoch 3000, Loss: 0.03062683343887329\n",
      "Epoch 3500, Loss: 0.031096547842025757\n",
      "Epoch 4000, Loss: 0.029692601412534714\n",
      "Epoch 4500, Loss: 0.029612045735120773\n",
      "Epoch 4999, Loss: 0.029006149619817734\n",
      "Epoch 0, Loss: 0.22786064445972443\n",
      "Epoch 500, Loss: 0.036694642156362534\n",
      "Epoch 1000, Loss: 0.030084772035479546\n",
      "Epoch 1500, Loss: 0.028636040166020393\n",
      "Epoch 2000, Loss: 0.0271857138723135\n",
      "Epoch 2500, Loss: 0.027072027325630188\n",
      "Epoch 3000, Loss: 0.02691216766834259\n",
      "Epoch 3500, Loss: 0.02601967751979828\n",
      "Epoch 4000, Loss: 0.026836443692445755\n",
      "Epoch 4500, Loss: 0.025517519563436508\n",
      "Epoch 4999, Loss: 0.023842746391892433\n",
      "Epoch 0, Loss: 0.18137209117412567\n",
      "Epoch 500, Loss: 0.03000515326857567\n",
      "Epoch 1000, Loss: 0.024617472663521767\n",
      "Epoch 1500, Loss: 0.024380479007959366\n",
      "Epoch 2000, Loss: 0.022591350600123405\n",
      "Epoch 2500, Loss: 0.024684185162186623\n",
      "Epoch 3000, Loss: 0.020755896344780922\n",
      "Epoch 3500, Loss: 0.02418474853038788\n",
      "Epoch 4000, Loss: 0.02290136180818081\n",
      "Epoch 4500, Loss: 0.02467050403356552\n",
      "Epoch 4999, Loss: 0.02181006409227848\n",
      "--- Experiment 20/20 ---\n",
      "Epoch 0, Loss: 0.18341679871082306\n",
      "Epoch 500, Loss: 0.024277526885271072\n",
      "Epoch 1000, Loss: 0.01968405954539776\n",
      "Epoch 1500, Loss: 0.019728275015950203\n",
      "Epoch 2000, Loss: 0.01911650411784649\n",
      "Epoch 2500, Loss: 0.01789359748363495\n",
      "Epoch 3000, Loss: 0.017725922167301178\n",
      "Epoch 3500, Loss: 0.019221635535359383\n",
      "Epoch 4000, Loss: 0.01818227209150791\n",
      "Epoch 4500, Loss: 0.01753341220319271\n",
      "Epoch 4999, Loss: 0.017377153038978577\n",
      "Epoch 0, Loss: 0.1250842958688736\n",
      "Epoch 500, Loss: 0.04113736003637314\n",
      "Epoch 1000, Loss: 0.03728053718805313\n",
      "Epoch 1500, Loss: 0.03417779877781868\n",
      "Epoch 2000, Loss: 0.03228721767663956\n",
      "Epoch 2500, Loss: 0.03156885504722595\n",
      "Epoch 3000, Loss: 0.030307579785585403\n",
      "Epoch 3500, Loss: 0.031517393887043\n",
      "Epoch 4000, Loss: 0.029254505410790443\n",
      "Epoch 4500, Loss: 0.02951374091207981\n",
      "Epoch 4999, Loss: 0.029831182211637497\n",
      "Epoch 0, Loss: 0.2853800654411316\n",
      "Epoch 500, Loss: 0.031207585707306862\n",
      "Epoch 1000, Loss: 0.02898300625383854\n",
      "Epoch 1500, Loss: 0.024450328201055527\n",
      "Epoch 2000, Loss: 0.025630181655287743\n",
      "Epoch 2500, Loss: 0.025074059143662453\n",
      "Epoch 3000, Loss: 0.02347160130739212\n",
      "Epoch 3500, Loss: 0.022722430527210236\n",
      "Epoch 4000, Loss: 0.02438034862279892\n",
      "Epoch 4500, Loss: 0.023139839991927147\n",
      "Epoch 4999, Loss: 0.02429209277033806\n",
      "Epoch 0, Loss: 0.319335013628006\n",
      "Epoch 500, Loss: 0.030140355229377747\n",
      "Epoch 1000, Loss: 0.027756784111261368\n",
      "Epoch 1500, Loss: 0.022455252707004547\n",
      "Epoch 2000, Loss: 0.021958276629447937\n",
      "Epoch 2500, Loss: 0.020657949149608612\n",
      "Epoch 3000, Loss: 0.02294313907623291\n",
      "Epoch 3500, Loss: 0.021763881668448448\n",
      "Epoch 4000, Loss: 0.023599298670887947\n",
      "Epoch 4500, Loss: 0.022783342748880386\n",
      "Epoch 4999, Loss: 0.022564496845006943\n",
      "Average Validation Loss (Base):   0.017101 ± 0.000478\n",
      "Average Validation Loss (Import): 0.025441 ± 0.000819\n",
      "Average Validation Loss (GP Out): 0.022224 ± 0.001295\n",
      "Average Validation Loss (GP Res): 0.020037 ± 0.001869\n",
      "\n",
      "==== Running experiments for IC: gaussian, BC: neumann ====\n",
      "--- Experiment 1/20 ---\n",
      "Epoch 0, Loss: 0.2122247964143753\n",
      "Epoch 500, Loss: 0.022786952555179596\n",
      "Epoch 1000, Loss: 0.019388612359762192\n",
      "Epoch 1500, Loss: 0.017389822751283646\n",
      "Epoch 2000, Loss: 0.018157973885536194\n",
      "Epoch 2500, Loss: 0.018177097663283348\n",
      "Epoch 3000, Loss: 0.01703115552663803\n",
      "Epoch 3500, Loss: 0.01747952401638031\n",
      "Epoch 4000, Loss: 0.016688410192728043\n",
      "Epoch 4500, Loss: 0.017383720725774765\n",
      "Epoch 4999, Loss: 0.0173632949590683\n",
      "Epoch 0, Loss: 0.1856919378042221\n",
      "Epoch 500, Loss: 0.042564842849969864\n",
      "Epoch 1000, Loss: 0.03930359333753586\n",
      "Epoch 1500, Loss: 0.0373542457818985\n",
      "Epoch 2000, Loss: 0.03292039409279823\n",
      "Epoch 2500, Loss: 0.03273770958185196\n",
      "Epoch 3000, Loss: 0.02941507287323475\n",
      "Epoch 3500, Loss: 0.03269106522202492\n",
      "Epoch 4000, Loss: 0.031087631359696388\n",
      "Epoch 4500, Loss: 0.03211188316345215\n",
      "Epoch 4999, Loss: 0.0333285927772522\n",
      "Epoch 0, Loss: 0.35629189014434814\n",
      "Epoch 500, Loss: 0.03413804620504379\n",
      "Epoch 1000, Loss: 0.0283624529838562\n",
      "Epoch 1500, Loss: 0.027047012001276016\n",
      "Epoch 2000, Loss: 0.0276468675583601\n",
      "Epoch 2500, Loss: 0.026417288929224014\n",
      "Epoch 3000, Loss: 0.02655862644314766\n",
      "Epoch 3500, Loss: 0.025564080104231834\n",
      "Epoch 4000, Loss: 0.022881416603922844\n",
      "Epoch 4500, Loss: 0.024218376725912094\n",
      "Epoch 4999, Loss: 0.02387932687997818\n",
      "Epoch 0, Loss: 0.19201381504535675\n",
      "Epoch 500, Loss: 0.02771543525159359\n",
      "Epoch 1000, Loss: 0.026800934225320816\n",
      "Epoch 1500, Loss: 0.024021407589316368\n",
      "Epoch 2000, Loss: 0.022461336106061935\n",
      "Epoch 2500, Loss: 0.024543803185224533\n",
      "Epoch 3000, Loss: 0.025584515184164047\n",
      "Epoch 3500, Loss: 0.02490883693099022\n",
      "Epoch 4000, Loss: 0.02316429652273655\n",
      "Epoch 4500, Loss: 0.021854525431990623\n",
      "Epoch 4999, Loss: 0.02383628860116005\n",
      "--- Experiment 2/20 ---\n",
      "Epoch 0, Loss: 0.12991096079349518\n",
      "Epoch 500, Loss: 0.024642188102006912\n",
      "Epoch 1000, Loss: 0.0217459537088871\n",
      "Epoch 1500, Loss: 0.019913770258426666\n",
      "Epoch 2000, Loss: 0.018392901867628098\n",
      "Epoch 2500, Loss: 0.01957065984606743\n",
      "Epoch 3000, Loss: 0.017888542264699936\n",
      "Epoch 3500, Loss: 0.017990371212363243\n",
      "Epoch 4000, Loss: 0.017701052129268646\n",
      "Epoch 4500, Loss: 0.016515465453267097\n",
      "Epoch 4999, Loss: 0.016935590654611588\n",
      "Epoch 0, Loss: 0.1286839097738266\n",
      "Epoch 500, Loss: 0.042169973254203796\n",
      "Epoch 1000, Loss: 0.036684952676296234\n",
      "Epoch 1500, Loss: 0.03404783457517624\n",
      "Epoch 2000, Loss: 0.033641424030065536\n",
      "Epoch 2500, Loss: 0.0339924581348896\n",
      "Epoch 3000, Loss: 0.031064482405781746\n",
      "Epoch 3500, Loss: 0.02968679741024971\n",
      "Epoch 4000, Loss: 0.030039558187127113\n",
      "Epoch 4500, Loss: 0.03143860399723053\n",
      "Epoch 4999, Loss: 0.02902158349752426\n",
      "Epoch 0, Loss: 0.1567094773054123\n",
      "Epoch 500, Loss: 0.032194964587688446\n",
      "Epoch 1000, Loss: 0.027729440480470657\n",
      "Epoch 1500, Loss: 0.02584817074239254\n",
      "Epoch 2000, Loss: 0.024621661752462387\n",
      "Epoch 2500, Loss: 0.027565056458115578\n",
      "Epoch 3000, Loss: 0.027043471112847328\n",
      "Epoch 3500, Loss: 0.025508495047688484\n",
      "Epoch 4000, Loss: 0.024894559755921364\n",
      "Epoch 4500, Loss: 0.023749958723783493\n",
      "Epoch 4999, Loss: 0.02547364868223667\n",
      "Epoch 0, Loss: 0.2890048325061798\n",
      "Epoch 500, Loss: 0.030866064131259918\n",
      "Epoch 1000, Loss: 0.026422813534736633\n",
      "Epoch 1500, Loss: 0.023997999727725983\n",
      "Epoch 2000, Loss: 0.021504245698451996\n",
      "Epoch 2500, Loss: 0.02315475419163704\n",
      "Epoch 3000, Loss: 0.024070894345641136\n",
      "Epoch 3500, Loss: 0.020138254389166832\n",
      "Epoch 4000, Loss: 0.021864570677280426\n",
      "Epoch 4500, Loss: 0.02272120490670204\n",
      "Epoch 4999, Loss: 0.022548934444785118\n",
      "--- Experiment 3/20 ---\n",
      "Epoch 0, Loss: 0.3148570954799652\n",
      "Epoch 500, Loss: 0.02241816371679306\n",
      "Epoch 1000, Loss: 0.02176014333963394\n",
      "Epoch 1500, Loss: 0.018398091197013855\n",
      "Epoch 2000, Loss: 0.01894638128578663\n",
      "Epoch 2500, Loss: 0.017885148525238037\n",
      "Epoch 3000, Loss: 0.018038809299468994\n",
      "Epoch 3500, Loss: 0.01877625659108162\n",
      "Epoch 4000, Loss: 0.01728125289082527\n",
      "Epoch 4500, Loss: 0.0171298086643219\n",
      "Epoch 4999, Loss: 0.01800820790231228\n",
      "Epoch 0, Loss: 0.23663052916526794\n",
      "Epoch 500, Loss: 0.038843944668769836\n",
      "Epoch 1000, Loss: 0.0349898599088192\n",
      "Epoch 1500, Loss: 0.034970734268426895\n",
      "Epoch 2000, Loss: 0.03167513757944107\n",
      "Epoch 2500, Loss: 0.03436196222901344\n",
      "Epoch 3000, Loss: 0.03289384767413139\n",
      "Epoch 3500, Loss: 0.03132916986942291\n",
      "Epoch 4000, Loss: 0.030244620516896248\n",
      "Epoch 4500, Loss: 0.029928337782621384\n",
      "Epoch 4999, Loss: 0.029112037271261215\n",
      "Epoch 0, Loss: 0.2165437489748001\n",
      "Epoch 500, Loss: 0.0317988246679306\n",
      "Epoch 1000, Loss: 0.02774162031710148\n",
      "Epoch 1500, Loss: 0.027374643832445145\n",
      "Epoch 2000, Loss: 0.02497437596321106\n",
      "Epoch 2500, Loss: 0.025154735893011093\n",
      "Epoch 3000, Loss: 0.022732164710760117\n",
      "Epoch 3500, Loss: 0.028214789927005768\n",
      "Epoch 4000, Loss: 0.025205951184034348\n",
      "Epoch 4500, Loss: 0.024274680763483047\n",
      "Epoch 4999, Loss: 0.024504881352186203\n",
      "Epoch 0, Loss: 0.13131801784038544\n",
      "Epoch 500, Loss: 0.026415782049298286\n",
      "Epoch 1000, Loss: 0.02486466057598591\n",
      "Epoch 1500, Loss: 0.023278582841157913\n",
      "Epoch 2000, Loss: 0.02367742918431759\n",
      "Epoch 2500, Loss: 0.024386735633015633\n",
      "Epoch 3000, Loss: 0.023028980940580368\n",
      "Epoch 3500, Loss: 0.021149540320038795\n",
      "Epoch 4000, Loss: 0.022277945652604103\n",
      "Epoch 4500, Loss: 0.01939966157078743\n",
      "Epoch 4999, Loss: 0.019995298236608505\n",
      "--- Experiment 4/20 ---\n",
      "Epoch 0, Loss: 0.22245322167873383\n",
      "Epoch 500, Loss: 0.02080269157886505\n",
      "Epoch 1000, Loss: 0.020347369834780693\n",
      "Epoch 1500, Loss: 0.017892496660351753\n",
      "Epoch 2000, Loss: 0.018825892359018326\n",
      "Epoch 2500, Loss: 0.017807643860578537\n",
      "Epoch 3000, Loss: 0.017269127070903778\n",
      "Epoch 3500, Loss: 0.01739833876490593\n",
      "Epoch 4000, Loss: 0.017445050179958344\n",
      "Epoch 4500, Loss: 0.016716700047254562\n",
      "Epoch 4999, Loss: 0.017976470291614532\n",
      "Epoch 0, Loss: 0.2844763398170471\n",
      "Epoch 500, Loss: 0.0434490405023098\n",
      "Epoch 1000, Loss: 0.0393061526119709\n",
      "Epoch 1500, Loss: 0.0377785786986351\n",
      "Epoch 2000, Loss: 0.03461986780166626\n",
      "Epoch 2500, Loss: 0.03259705379605293\n",
      "Epoch 3000, Loss: 0.03268665447831154\n",
      "Epoch 3500, Loss: 0.033726055175065994\n",
      "Epoch 4000, Loss: 0.030531354248523712\n",
      "Epoch 4500, Loss: 0.02998972125351429\n",
      "Epoch 4999, Loss: 0.03098737634718418\n",
      "Epoch 0, Loss: 0.3098278045654297\n",
      "Epoch 500, Loss: 0.03492112457752228\n",
      "Epoch 1000, Loss: 0.02983667328953743\n",
      "Epoch 1500, Loss: 0.027775121852755547\n",
      "Epoch 2000, Loss: 0.02587725967168808\n",
      "Epoch 2500, Loss: 0.025216661393642426\n",
      "Epoch 3000, Loss: 0.02554098702967167\n",
      "Epoch 3500, Loss: 0.02789881080389023\n",
      "Epoch 4000, Loss: 0.022897571325302124\n",
      "Epoch 4500, Loss: 0.023912448436021805\n",
      "Epoch 4999, Loss: 0.023793814703822136\n",
      "Epoch 0, Loss: 0.16217415034770966\n",
      "Epoch 500, Loss: 0.02740003913640976\n",
      "Epoch 1000, Loss: 0.02316119708120823\n",
      "Epoch 1500, Loss: 0.022855229675769806\n",
      "Epoch 2000, Loss: 0.021620070561766624\n",
      "Epoch 2500, Loss: 0.021041307598352432\n",
      "Epoch 3000, Loss: 0.017513969913125038\n",
      "Epoch 3500, Loss: 0.023013949394226074\n",
      "Epoch 4000, Loss: 0.021927012130618095\n",
      "Epoch 4500, Loss: 0.021530091762542725\n",
      "Epoch 4999, Loss: 0.02215990237891674\n",
      "--- Experiment 5/20 ---\n",
      "Epoch 0, Loss: 0.4145864248275757\n",
      "Epoch 500, Loss: 0.024602532386779785\n",
      "Epoch 1000, Loss: 0.023074612021446228\n",
      "Epoch 1500, Loss: 0.019709846004843712\n",
      "Epoch 2000, Loss: 0.0184963159263134\n",
      "Epoch 2500, Loss: 0.017527829855680466\n",
      "Epoch 3000, Loss: 0.019195783883333206\n",
      "Epoch 3500, Loss: 0.01874931901693344\n",
      "Epoch 4000, Loss: 0.01830149069428444\n",
      "Epoch 4500, Loss: 0.018701836466789246\n",
      "Epoch 4999, Loss: 0.017722662538290024\n",
      "Epoch 0, Loss: 0.13161493837833405\n",
      "Epoch 500, Loss: 0.03948766738176346\n",
      "Epoch 1000, Loss: 0.035901494324207306\n",
      "Epoch 1500, Loss: 0.03450235351920128\n",
      "Epoch 2000, Loss: 0.031286291778087616\n",
      "Epoch 2500, Loss: 0.0313088521361351\n",
      "Epoch 3000, Loss: 0.03196809068322182\n",
      "Epoch 3500, Loss: 0.031217340379953384\n",
      "Epoch 4000, Loss: 0.0296640582382679\n",
      "Epoch 4500, Loss: 0.03023742511868477\n",
      "Epoch 4999, Loss: 0.036996159702539444\n",
      "Epoch 0, Loss: 0.19087940454483032\n",
      "Epoch 500, Loss: 0.03229019045829773\n",
      "Epoch 1000, Loss: 0.029054051265120506\n",
      "Epoch 1500, Loss: 0.027750330045819283\n",
      "Epoch 2000, Loss: 0.02504977025091648\n",
      "Epoch 2500, Loss: 0.02548762783408165\n",
      "Epoch 3000, Loss: 0.025294626131653786\n",
      "Epoch 3500, Loss: 0.02515259198844433\n",
      "Epoch 4000, Loss: 0.02344825118780136\n",
      "Epoch 4500, Loss: 0.024163100868463516\n",
      "Epoch 4999, Loss: 0.024114564061164856\n",
      "Epoch 0, Loss: 0.14775419235229492\n",
      "Epoch 500, Loss: 0.029397785663604736\n",
      "Epoch 1000, Loss: 0.024242345243692398\n",
      "Epoch 1500, Loss: 0.02169482409954071\n",
      "Epoch 2000, Loss: 0.023887071758508682\n",
      "Epoch 2500, Loss: 0.02361903339624405\n",
      "Epoch 3000, Loss: 0.01645553484559059\n",
      "Epoch 3500, Loss: 0.022972967475652695\n",
      "Epoch 4000, Loss: 0.023913847282528877\n",
      "Epoch 4500, Loss: 0.02196764387190342\n",
      "Epoch 4999, Loss: 0.02149033732712269\n",
      "--- Experiment 6/20 ---\n",
      "Epoch 0, Loss: 0.17462237179279327\n",
      "Epoch 500, Loss: 0.02388457953929901\n",
      "Epoch 1000, Loss: 0.01864740625023842\n",
      "Epoch 1500, Loss: 0.018877819180488586\n",
      "Epoch 2000, Loss: 0.019851401448249817\n",
      "Epoch 2500, Loss: 0.01926393061876297\n",
      "Epoch 3000, Loss: 0.018331659957766533\n",
      "Epoch 3500, Loss: 0.017942111939191818\n",
      "Epoch 4000, Loss: 0.016054240986704826\n",
      "Epoch 4500, Loss: 0.017167333513498306\n",
      "Epoch 4999, Loss: 0.01675853133201599\n",
      "Epoch 0, Loss: 0.12626394629478455\n",
      "Epoch 500, Loss: 0.041691724210977554\n",
      "Epoch 1000, Loss: 0.039307381957769394\n",
      "Epoch 1500, Loss: 0.038057759404182434\n",
      "Epoch 2000, Loss: 0.034443601965904236\n",
      "Epoch 2500, Loss: 0.033228494226932526\n",
      "Epoch 3000, Loss: 0.033779434859752655\n",
      "Epoch 3500, Loss: 0.03191530704498291\n",
      "Epoch 4000, Loss: 0.03350171074271202\n",
      "Epoch 4500, Loss: 0.030720535665750504\n",
      "Epoch 4999, Loss: 0.029884520918130875\n",
      "Epoch 0, Loss: 0.19147594273090363\n",
      "Epoch 500, Loss: 0.03450290486216545\n",
      "Epoch 1000, Loss: 0.02559630014002323\n",
      "Epoch 1500, Loss: 0.026537522673606873\n",
      "Epoch 2000, Loss: 0.025381997227668762\n",
      "Epoch 2500, Loss: 0.022534599527716637\n",
      "Epoch 3000, Loss: 0.024144060909748077\n",
      "Epoch 3500, Loss: 0.0314609557390213\n",
      "Epoch 4000, Loss: 0.0244436077773571\n",
      "Epoch 4500, Loss: 0.03019414097070694\n",
      "Epoch 4999, Loss: 0.027099229395389557\n",
      "Epoch 0, Loss: 0.1393253654241562\n",
      "Epoch 500, Loss: 0.029285196214914322\n",
      "Epoch 1000, Loss: 0.02536010555922985\n",
      "Epoch 1500, Loss: 0.025389617308974266\n",
      "Epoch 2000, Loss: 0.023878637701272964\n",
      "Epoch 2500, Loss: 0.021505780518054962\n",
      "Epoch 3000, Loss: 0.022632088512182236\n",
      "Epoch 3500, Loss: 0.020271694287657738\n",
      "Epoch 4000, Loss: 0.02442239411175251\n",
      "Epoch 4500, Loss: 0.02488330937922001\n",
      "Epoch 4999, Loss: 0.023070640861988068\n",
      "--- Experiment 7/20 ---\n",
      "Epoch 0, Loss: 0.30628615617752075\n",
      "Epoch 500, Loss: 0.023020604625344276\n",
      "Epoch 1000, Loss: 0.01984415017068386\n",
      "Epoch 1500, Loss: 0.018775567412376404\n",
      "Epoch 2000, Loss: 0.017430908977985382\n",
      "Epoch 2500, Loss: 0.017432130873203278\n",
      "Epoch 3000, Loss: 0.017719175666570663\n",
      "Epoch 3500, Loss: 0.01648017019033432\n",
      "Epoch 4000, Loss: 0.02060314267873764\n",
      "Epoch 4500, Loss: 0.01641867496073246\n",
      "Epoch 4999, Loss: 0.0176071897149086\n",
      "Epoch 0, Loss: 0.23909060657024384\n",
      "Epoch 500, Loss: 0.04091142863035202\n",
      "Epoch 1000, Loss: 0.036737214773893356\n",
      "Epoch 1500, Loss: 0.034736890345811844\n",
      "Epoch 2000, Loss: 0.034640878438949585\n",
      "Epoch 2500, Loss: 0.028942681849002838\n",
      "Epoch 3000, Loss: 0.030689850449562073\n",
      "Epoch 3500, Loss: 0.029114602133631706\n",
      "Epoch 4000, Loss: 0.028459517285227776\n",
      "Epoch 4500, Loss: 0.027732331305742264\n",
      "Epoch 4999, Loss: 0.028335250914096832\n",
      "Epoch 0, Loss: 0.14407235383987427\n",
      "Epoch 500, Loss: 0.032080888748168945\n",
      "Epoch 1000, Loss: 0.028005028143525124\n",
      "Epoch 1500, Loss: 0.02587837725877762\n",
      "Epoch 2000, Loss: 0.024328723549842834\n",
      "Epoch 2500, Loss: 0.02469445951282978\n",
      "Epoch 3000, Loss: 0.024583907797932625\n",
      "Epoch 3500, Loss: 0.02406938374042511\n",
      "Epoch 4000, Loss: 0.024994200095534325\n",
      "Epoch 4500, Loss: 0.024734443053603172\n",
      "Epoch 4999, Loss: 0.02302020788192749\n",
      "Epoch 0, Loss: 0.28011438250541687\n",
      "Epoch 500, Loss: 0.02705797180533409\n",
      "Epoch 1000, Loss: 0.02630160003900528\n",
      "Epoch 1500, Loss: 0.02581303007900715\n",
      "Epoch 2000, Loss: 0.021980132907629013\n",
      "Epoch 2500, Loss: 0.02361563965678215\n",
      "Epoch 3000, Loss: 0.020717870444059372\n",
      "Epoch 3500, Loss: 0.021937549114227295\n",
      "Epoch 4000, Loss: 0.02287997305393219\n",
      "Epoch 4500, Loss: 0.020210105925798416\n",
      "Epoch 4999, Loss: 0.02195434458553791\n",
      "--- Experiment 8/20 ---\n",
      "Epoch 0, Loss: 0.17708072066307068\n",
      "Epoch 500, Loss: 0.023700546473264694\n",
      "Epoch 1000, Loss: 0.019318601116538048\n",
      "Epoch 1500, Loss: 0.019316982477903366\n",
      "Epoch 2000, Loss: 0.01920044980943203\n",
      "Epoch 2500, Loss: 0.02016584202647209\n",
      "Epoch 3000, Loss: 0.01753687486052513\n",
      "Epoch 3500, Loss: 0.016951795667409897\n",
      "Epoch 4000, Loss: 0.016894537955522537\n",
      "Epoch 4500, Loss: 0.01750773936510086\n",
      "Epoch 4999, Loss: 0.01663101091980934\n",
      "Epoch 0, Loss: 0.1327863186597824\n",
      "Epoch 500, Loss: 0.04212799668312073\n",
      "Epoch 1000, Loss: 0.036471519619226456\n",
      "Epoch 1500, Loss: 0.03769351541996002\n",
      "Epoch 2000, Loss: 0.035719070583581924\n",
      "Epoch 2500, Loss: 0.03232830390334129\n",
      "Epoch 3000, Loss: 0.030376899987459183\n",
      "Epoch 3500, Loss: 0.03036660887300968\n",
      "Epoch 4000, Loss: 0.02979726530611515\n",
      "Epoch 4500, Loss: 0.028889795765280724\n",
      "Epoch 4999, Loss: 0.029627233743667603\n",
      "Epoch 0, Loss: 0.15391062200069427\n",
      "Epoch 500, Loss: 0.030496422201395035\n",
      "Epoch 1000, Loss: 0.02742873877286911\n",
      "Epoch 1500, Loss: 0.02712271735072136\n",
      "Epoch 2000, Loss: 0.025208763778209686\n",
      "Epoch 2500, Loss: 0.025503505021333694\n",
      "Epoch 3000, Loss: 0.024372611194849014\n",
      "Epoch 3500, Loss: 0.023362580686807632\n",
      "Epoch 4000, Loss: 0.023312576115131378\n",
      "Epoch 4500, Loss: 0.024845488369464874\n",
      "Epoch 4999, Loss: 0.02420106530189514\n",
      "Epoch 0, Loss: 0.24225929379463196\n",
      "Epoch 500, Loss: 0.029097940772771835\n",
      "Epoch 1000, Loss: 0.026096928864717484\n",
      "Epoch 1500, Loss: 0.022702887654304504\n",
      "Epoch 2000, Loss: 0.02411792427301407\n",
      "Epoch 2500, Loss: 0.021518539637327194\n",
      "Epoch 3000, Loss: 0.024134436622262\n",
      "Epoch 3500, Loss: 0.023439906537532806\n",
      "Epoch 4000, Loss: 0.022983945906162262\n",
      "Epoch 4500, Loss: 0.022006334736943245\n",
      "Epoch 4999, Loss: 0.026108210906386375\n",
      "--- Experiment 9/20 ---\n",
      "Epoch 0, Loss: 0.15552915632724762\n",
      "Epoch 500, Loss: 0.02386675775051117\n",
      "Epoch 1000, Loss: 0.019430143758654594\n",
      "Epoch 1500, Loss: 0.01903710514307022\n",
      "Epoch 2000, Loss: 0.01752093806862831\n",
      "Epoch 2500, Loss: 0.017984429374337196\n",
      "Epoch 3000, Loss: 0.016375858336687088\n",
      "Epoch 3500, Loss: 0.018841784447431564\n",
      "Epoch 4000, Loss: 0.016921907663345337\n",
      "Epoch 4500, Loss: 0.01732534170150757\n",
      "Epoch 4999, Loss: 0.02095005288720131\n",
      "Epoch 0, Loss: 0.2067314237356186\n",
      "Epoch 500, Loss: 0.04373385012149811\n",
      "Epoch 1000, Loss: 0.035266149789094925\n",
      "Epoch 1500, Loss: 0.034526389092206955\n",
      "Epoch 2000, Loss: 0.03252461552619934\n",
      "Epoch 2500, Loss: 0.03299461305141449\n",
      "Epoch 3000, Loss: 0.03274937719106674\n",
      "Epoch 3500, Loss: 0.03254704922437668\n",
      "Epoch 4000, Loss: 0.029850680381059647\n",
      "Epoch 4500, Loss: 0.029295019805431366\n",
      "Epoch 4999, Loss: 0.030158627778291702\n",
      "Epoch 0, Loss: 0.2788406014442444\n",
      "Epoch 500, Loss: 0.03295035660266876\n",
      "Epoch 1000, Loss: 0.028499677777290344\n",
      "Epoch 1500, Loss: 0.028084341436624527\n",
      "Epoch 2000, Loss: 0.02719007432460785\n",
      "Epoch 2500, Loss: 0.026777425780892372\n",
      "Epoch 3000, Loss: 0.025711575523018837\n",
      "Epoch 3500, Loss: 0.02623993344604969\n",
      "Epoch 4000, Loss: 0.024968495592474937\n",
      "Epoch 4500, Loss: 0.024347670376300812\n",
      "Epoch 4999, Loss: 0.025161202996969223\n",
      "Epoch 0, Loss: 0.1541985720396042\n",
      "Epoch 500, Loss: 0.027037354186177254\n",
      "Epoch 1000, Loss: 0.023926660418510437\n",
      "Epoch 1500, Loss: 0.02155817300081253\n",
      "Epoch 2000, Loss: 0.02321060188114643\n",
      "Epoch 2500, Loss: 0.020750265568494797\n",
      "Epoch 3000, Loss: 0.023096872493624687\n",
      "Epoch 3500, Loss: 0.023031119257211685\n",
      "Epoch 4000, Loss: 0.021887412294745445\n",
      "Epoch 4500, Loss: 0.021004699170589447\n",
      "Epoch 4999, Loss: 0.020097235217690468\n",
      "--- Experiment 10/20 ---\n",
      "Epoch 0, Loss: 0.18474896252155304\n",
      "Epoch 500, Loss: 0.023121710866689682\n",
      "Epoch 1000, Loss: 0.019470585510134697\n",
      "Epoch 1500, Loss: 0.018811261281371117\n",
      "Epoch 2000, Loss: 0.017089582979679108\n",
      "Epoch 2500, Loss: 0.01770438626408577\n",
      "Epoch 3000, Loss: 0.018057703971862793\n",
      "Epoch 3500, Loss: 0.019239693880081177\n",
      "Epoch 4000, Loss: 0.018429763615131378\n",
      "Epoch 4500, Loss: 0.017554575577378273\n",
      "Epoch 4999, Loss: 0.01647135801613331\n",
      "Epoch 0, Loss: 0.29234275221824646\n",
      "Epoch 500, Loss: 0.041211940348148346\n",
      "Epoch 1000, Loss: 0.034628815948963165\n",
      "Epoch 1500, Loss: 0.033150047063827515\n",
      "Epoch 2000, Loss: 0.032705046236515045\n",
      "Epoch 2500, Loss: 0.032590605318546295\n",
      "Epoch 3000, Loss: 0.030398506671190262\n",
      "Epoch 3500, Loss: 0.031204834580421448\n",
      "Epoch 4000, Loss: 0.029007818549871445\n",
      "Epoch 4500, Loss: 0.028859790414571762\n",
      "Epoch 4999, Loss: 0.030121734365820885\n",
      "Epoch 0, Loss: 0.2323300987482071\n",
      "Epoch 500, Loss: 0.03173050656914711\n",
      "Epoch 1000, Loss: 0.027010992169380188\n",
      "Epoch 1500, Loss: 0.025870095938444138\n",
      "Epoch 2000, Loss: 0.022833513095974922\n",
      "Epoch 2500, Loss: 0.027320880442857742\n",
      "Epoch 3000, Loss: 0.026648182421922684\n",
      "Epoch 3500, Loss: 0.025757845491170883\n",
      "Epoch 4000, Loss: 0.02509337291121483\n",
      "Epoch 4500, Loss: 0.026109512895345688\n",
      "Epoch 4999, Loss: 0.026039568707346916\n",
      "Epoch 0, Loss: 0.17368678748607635\n",
      "Epoch 500, Loss: 0.02964232861995697\n",
      "Epoch 1000, Loss: 0.02557053416967392\n",
      "Epoch 1500, Loss: 0.02513355016708374\n",
      "Epoch 2000, Loss: 0.02195008471608162\n",
      "Epoch 2500, Loss: 0.02338007092475891\n",
      "Epoch 3000, Loss: 0.02386864461004734\n",
      "Epoch 3500, Loss: 0.021619455888867378\n",
      "Epoch 4000, Loss: 0.022182531654834747\n",
      "Epoch 4500, Loss: 0.023346131667494774\n",
      "Epoch 4999, Loss: 0.022080719470977783\n",
      "--- Experiment 11/20 ---\n",
      "Epoch 0, Loss: 0.12691541016101837\n",
      "Epoch 500, Loss: 0.02439858391880989\n",
      "Epoch 1000, Loss: 0.020573019981384277\n",
      "Epoch 1500, Loss: 0.019771944731473923\n",
      "Epoch 2000, Loss: 0.018441561609506607\n",
      "Epoch 2500, Loss: 0.018275264650583267\n",
      "Epoch 3000, Loss: 0.017049558460712433\n",
      "Epoch 3500, Loss: 0.01757102832198143\n",
      "Epoch 4000, Loss: 0.018872778862714767\n",
      "Epoch 4500, Loss: 0.018182937055826187\n",
      "Epoch 4999, Loss: 0.01746487431228161\n",
      "Epoch 0, Loss: 0.11478365212678909\n",
      "Epoch 500, Loss: 0.03727094084024429\n",
      "Epoch 1000, Loss: 0.03953217342495918\n",
      "Epoch 1500, Loss: 0.03424014896154404\n",
      "Epoch 2000, Loss: 0.03397625684738159\n",
      "Epoch 2500, Loss: 0.032508499920368195\n",
      "Epoch 3000, Loss: 0.03125743195414543\n",
      "Epoch 3500, Loss: 0.029483972117304802\n",
      "Epoch 4000, Loss: 0.02984864078462124\n",
      "Epoch 4500, Loss: 0.03087855875492096\n",
      "Epoch 4999, Loss: 0.028704622760415077\n",
      "Epoch 0, Loss: 0.18202929198741913\n",
      "Epoch 500, Loss: 0.02931048348546028\n",
      "Epoch 1000, Loss: 0.02753591537475586\n",
      "Epoch 1500, Loss: 0.0264741200953722\n",
      "Epoch 2000, Loss: 0.026232045143842697\n",
      "Epoch 2500, Loss: 0.025713007897138596\n",
      "Epoch 3000, Loss: 0.024320261552929878\n",
      "Epoch 3500, Loss: 0.02660013735294342\n",
      "Epoch 4000, Loss: 0.024519769474864006\n",
      "Epoch 4500, Loss: 0.024696433916687965\n",
      "Epoch 4999, Loss: 0.024198109284043312\n",
      "Epoch 0, Loss: 0.19058656692504883\n",
      "Epoch 500, Loss: 0.02622484602034092\n",
      "Epoch 1000, Loss: 0.024989478290081024\n",
      "Epoch 1500, Loss: 0.0237613283097744\n",
      "Epoch 2000, Loss: 0.02139684371650219\n",
      "Epoch 2500, Loss: 0.02197703719139099\n",
      "Epoch 3000, Loss: 0.02302718535065651\n",
      "Epoch 3500, Loss: 0.023158352822065353\n",
      "Epoch 4000, Loss: 0.02257958985865116\n",
      "Epoch 4500, Loss: 0.02175329625606537\n",
      "Epoch 4999, Loss: 0.02296123281121254\n",
      "--- Experiment 12/20 ---\n",
      "Epoch 0, Loss: 0.12106091529130936\n",
      "Epoch 500, Loss: 0.020584262907505035\n",
      "Epoch 1000, Loss: 0.020814266055822372\n",
      "Epoch 1500, Loss: 0.017610885202884674\n",
      "Epoch 2000, Loss: 0.016762442886829376\n",
      "Epoch 2500, Loss: 0.018594373017549515\n",
      "Epoch 3000, Loss: 0.018243618309497833\n",
      "Epoch 3500, Loss: 0.017427459359169006\n",
      "Epoch 4000, Loss: 0.017157435417175293\n",
      "Epoch 4500, Loss: 0.017385317012667656\n",
      "Epoch 4999, Loss: 0.01564626395702362\n",
      "Epoch 0, Loss: 0.19331865012645721\n",
      "Epoch 500, Loss: 0.042823426425457\n",
      "Epoch 1000, Loss: 0.038715578615665436\n",
      "Epoch 1500, Loss: 0.03566092625260353\n",
      "Epoch 2000, Loss: 0.034619107842445374\n",
      "Epoch 2500, Loss: 0.03748835623264313\n",
      "Epoch 3000, Loss: 0.03326043859124184\n",
      "Epoch 3500, Loss: 0.03284195065498352\n",
      "Epoch 4000, Loss: 0.03371807187795639\n",
      "Epoch 4500, Loss: 0.034223102033138275\n",
      "Epoch 4999, Loss: 0.030562492087483406\n",
      "Epoch 0, Loss: 0.11397696286439896\n",
      "Epoch 500, Loss: 0.03156847134232521\n",
      "Epoch 1000, Loss: 0.027828112244606018\n",
      "Epoch 1500, Loss: 0.025831179693341255\n",
      "Epoch 2000, Loss: 0.024409830570220947\n",
      "Epoch 2500, Loss: 0.023397700861096382\n",
      "Epoch 3000, Loss: 0.022639671340584755\n",
      "Epoch 3500, Loss: 0.024580275639891624\n",
      "Epoch 4000, Loss: 0.02761118859052658\n",
      "Epoch 4500, Loss: 0.025240538641810417\n",
      "Epoch 4999, Loss: 0.02326165698468685\n",
      "Epoch 0, Loss: 0.1273021697998047\n",
      "Epoch 500, Loss: 0.028366390615701675\n",
      "Epoch 1000, Loss: 0.024943973869085312\n",
      "Epoch 1500, Loss: 0.022903144359588623\n",
      "Epoch 2000, Loss: 0.02360921911895275\n",
      "Epoch 2500, Loss: 0.02321690134704113\n",
      "Epoch 3000, Loss: 0.02370462380349636\n",
      "Epoch 3500, Loss: 0.0219563115388155\n",
      "Epoch 4000, Loss: 0.02169152721762657\n",
      "Epoch 4500, Loss: 0.023581087589263916\n",
      "Epoch 4999, Loss: 0.0251934751868248\n",
      "--- Experiment 13/20 ---\n",
      "Epoch 0, Loss: 0.13728100061416626\n",
      "Epoch 500, Loss: 0.020883653312921524\n",
      "Epoch 1000, Loss: 0.017877675592899323\n",
      "Epoch 1500, Loss: 0.017407212406396866\n",
      "Epoch 2000, Loss: 0.01820985972881317\n",
      "Epoch 2500, Loss: 0.017654772847890854\n",
      "Epoch 3000, Loss: 0.01802588813006878\n",
      "Epoch 3500, Loss: 0.01839858666062355\n",
      "Epoch 4000, Loss: 0.016332851722836494\n",
      "Epoch 4500, Loss: 0.015602508559823036\n",
      "Epoch 4999, Loss: 0.016798660159111023\n",
      "Epoch 0, Loss: 0.13646627962589264\n",
      "Epoch 500, Loss: 0.041538938879966736\n",
      "Epoch 1000, Loss: 0.03711365535855293\n",
      "Epoch 1500, Loss: 0.03729996457695961\n",
      "Epoch 2000, Loss: 0.03281201422214508\n",
      "Epoch 2500, Loss: 0.031166259199380875\n",
      "Epoch 3000, Loss: 0.029806338250637054\n",
      "Epoch 3500, Loss: 0.03228035196661949\n",
      "Epoch 4000, Loss: 0.028923291712999344\n",
      "Epoch 4500, Loss: 0.028862403705716133\n",
      "Epoch 4999, Loss: 0.029616735875606537\n",
      "Epoch 0, Loss: 0.2762061655521393\n",
      "Epoch 500, Loss: 0.032721541821956635\n",
      "Epoch 1000, Loss: 0.028697436675429344\n",
      "Epoch 1500, Loss: 0.027860432863235474\n",
      "Epoch 2000, Loss: 0.025752510875463486\n",
      "Epoch 2500, Loss: 0.024711355566978455\n",
      "Epoch 3000, Loss: 0.024614624679088593\n",
      "Epoch 3500, Loss: 0.0231376551091671\n",
      "Epoch 4000, Loss: 0.02412622980773449\n",
      "Epoch 4500, Loss: 0.023426175117492676\n",
      "Epoch 4999, Loss: 0.022521205246448517\n",
      "Epoch 0, Loss: 0.2044169306755066\n",
      "Epoch 500, Loss: 0.02653563767671585\n",
      "Epoch 1000, Loss: 0.023475565016269684\n",
      "Epoch 1500, Loss: 0.021115155890583992\n",
      "Epoch 2000, Loss: 0.023211946710944176\n",
      "Epoch 2500, Loss: 0.023268332704901695\n",
      "Epoch 3000, Loss: 0.022375378757715225\n",
      "Epoch 3500, Loss: 0.022405531257390976\n",
      "Epoch 4000, Loss: 0.020790545269846916\n",
      "Epoch 4500, Loss: 0.02257750928401947\n",
      "Epoch 4999, Loss: 0.021158158779144287\n",
      "--- Experiment 14/20 ---\n",
      "Epoch 0, Loss: 0.12659065425395966\n",
      "Epoch 500, Loss: 0.020320283249020576\n",
      "Epoch 1000, Loss: 0.01860116794705391\n",
      "Epoch 1500, Loss: 0.018161501735448837\n",
      "Epoch 2000, Loss: 0.0183207169175148\n",
      "Epoch 2500, Loss: 0.01847095787525177\n",
      "Epoch 3000, Loss: 0.01759391278028488\n",
      "Epoch 3500, Loss: 0.017719324678182602\n",
      "Epoch 4000, Loss: 0.01828887313604355\n",
      "Epoch 4500, Loss: 0.01663520559668541\n",
      "Epoch 4999, Loss: 0.017553457990288734\n",
      "Epoch 0, Loss: 0.2850392758846283\n",
      "Epoch 500, Loss: 0.04016716033220291\n",
      "Epoch 1000, Loss: 0.03976406157016754\n",
      "Epoch 1500, Loss: 0.035914383828639984\n",
      "Epoch 2000, Loss: 0.034913450479507446\n",
      "Epoch 2500, Loss: 0.03058878518640995\n",
      "Epoch 3000, Loss: 0.03087228536605835\n",
      "Epoch 3500, Loss: 0.02989363484084606\n",
      "Epoch 4000, Loss: 0.02965937741100788\n",
      "Epoch 4500, Loss: 0.02956807240843773\n",
      "Epoch 4999, Loss: 0.028297537937760353\n",
      "Epoch 0, Loss: 0.24559545516967773\n",
      "Epoch 500, Loss: 0.03277048468589783\n",
      "Epoch 1000, Loss: 0.028522025793790817\n",
      "Epoch 1500, Loss: 0.02704985812306404\n",
      "Epoch 2000, Loss: 0.023951131850481033\n",
      "Epoch 2500, Loss: 0.02491310052573681\n",
      "Epoch 3000, Loss: 0.02556312456727028\n",
      "Epoch 3500, Loss: 0.025740209966897964\n",
      "Epoch 4000, Loss: 0.024615716189146042\n",
      "Epoch 4500, Loss: 0.023935729637742043\n",
      "Epoch 4999, Loss: 0.022839125245809555\n",
      "Epoch 0, Loss: 0.16128213703632355\n",
      "Epoch 500, Loss: 0.026684783399105072\n",
      "Epoch 1000, Loss: 0.023885024711489677\n",
      "Epoch 1500, Loss: 0.020653970539569855\n",
      "Epoch 2000, Loss: 0.023220032453536987\n",
      "Epoch 2500, Loss: 0.022580944001674652\n",
      "Epoch 3000, Loss: 0.02248489484190941\n",
      "Epoch 3500, Loss: 0.02324768155813217\n",
      "Epoch 4000, Loss: 0.023231759667396545\n",
      "Epoch 4500, Loss: 0.021608863025903702\n",
      "Epoch 4999, Loss: 0.02217070199549198\n",
      "--- Experiment 15/20 ---\n",
      "Epoch 0, Loss: 0.1719343513250351\n",
      "Epoch 500, Loss: 0.026139404624700546\n",
      "Epoch 1000, Loss: 0.022395452484488487\n",
      "Epoch 1500, Loss: 0.021601509302854538\n",
      "Epoch 2000, Loss: 0.021131882444024086\n",
      "Epoch 2500, Loss: 0.019259534776210785\n",
      "Epoch 3000, Loss: 0.01883733831346035\n",
      "Epoch 3500, Loss: 0.018177786841988564\n",
      "Epoch 4000, Loss: 0.016864614561200142\n",
      "Epoch 4500, Loss: 0.019220653921365738\n",
      "Epoch 4999, Loss: 0.017608968541026115\n",
      "Epoch 0, Loss: 0.16250213980674744\n",
      "Epoch 500, Loss: 0.040065206587314606\n",
      "Epoch 1000, Loss: 0.036077581346035004\n",
      "Epoch 1500, Loss: 0.03488968312740326\n",
      "Epoch 2000, Loss: 0.031839288771152496\n",
      "Epoch 2500, Loss: 0.031225379556417465\n",
      "Epoch 3000, Loss: 0.031143035739660263\n",
      "Epoch 3500, Loss: 0.030916931107640266\n",
      "Epoch 4000, Loss: 0.02974163368344307\n",
      "Epoch 4500, Loss: 0.0341426283121109\n",
      "Epoch 4999, Loss: 0.03108133003115654\n",
      "Epoch 0, Loss: 0.21221482753753662\n",
      "Epoch 500, Loss: 0.033120047301054\n",
      "Epoch 1000, Loss: 0.02850380912423134\n",
      "Epoch 1500, Loss: 0.026223544031381607\n",
      "Epoch 2000, Loss: 0.027108194306492805\n",
      "Epoch 2500, Loss: 0.026087313890457153\n",
      "Epoch 3000, Loss: 0.026148146018385887\n",
      "Epoch 3500, Loss: 0.02450033277273178\n",
      "Epoch 4000, Loss: 0.024506252259016037\n",
      "Epoch 4500, Loss: 0.027627840638160706\n",
      "Epoch 4999, Loss: 0.027542732656002045\n",
      "Epoch 0, Loss: 0.24370820820331573\n",
      "Epoch 500, Loss: 0.02812589704990387\n",
      "Epoch 1000, Loss: 0.02444068156182766\n",
      "Epoch 1500, Loss: 0.02279454842209816\n",
      "Epoch 2000, Loss: 0.022270256653428078\n",
      "Epoch 2500, Loss: 0.021940765902400017\n",
      "Epoch 3000, Loss: 0.02085539884865284\n",
      "Epoch 3500, Loss: 0.022731324657797813\n",
      "Epoch 4000, Loss: 0.02116485685110092\n",
      "Epoch 4500, Loss: 0.021782943978905678\n",
      "Epoch 4999, Loss: 0.022846784442663193\n",
      "--- Experiment 16/20 ---\n",
      "Epoch 0, Loss: 0.13701586425304413\n",
      "Epoch 500, Loss: 0.024632830172777176\n",
      "Epoch 1000, Loss: 0.01976088061928749\n",
      "Epoch 1500, Loss: 0.018132980912923813\n",
      "Epoch 2000, Loss: 0.017814818769693375\n",
      "Epoch 2500, Loss: 0.017993774265050888\n",
      "Epoch 3000, Loss: 0.017732087522745132\n",
      "Epoch 3500, Loss: 0.018360814079642296\n",
      "Epoch 4000, Loss: 0.01802094653248787\n",
      "Epoch 4500, Loss: 0.017018673941493034\n",
      "Epoch 4999, Loss: 0.01663927547633648\n",
      "Epoch 0, Loss: 0.25965094566345215\n",
      "Epoch 500, Loss: 0.038936566561460495\n",
      "Epoch 1000, Loss: 0.037359073758125305\n",
      "Epoch 1500, Loss: 0.032799459993839264\n",
      "Epoch 2000, Loss: 0.03166312351822853\n",
      "Epoch 2500, Loss: 0.032501522451639175\n",
      "Epoch 3000, Loss: 0.02955709770321846\n",
      "Epoch 3500, Loss: 0.031507011502981186\n",
      "Epoch 4000, Loss: 0.040646057575941086\n",
      "Epoch 4500, Loss: 0.029768744483590126\n",
      "Epoch 4999, Loss: 0.031757112592458725\n",
      "Epoch 0, Loss: 0.21219325065612793\n",
      "Epoch 500, Loss: 0.03203372284770012\n",
      "Epoch 1000, Loss: 0.02929137647151947\n",
      "Epoch 1500, Loss: 0.02624485455453396\n",
      "Epoch 2000, Loss: 0.024911638349294662\n",
      "Epoch 2500, Loss: 0.024945639073848724\n",
      "Epoch 3000, Loss: 0.024572789669036865\n",
      "Epoch 3500, Loss: 0.026500340551137924\n",
      "Epoch 4000, Loss: 0.02592005394399166\n",
      "Epoch 4500, Loss: 0.02497117593884468\n",
      "Epoch 4999, Loss: 0.024707745760679245\n",
      "Epoch 0, Loss: 0.22638709843158722\n",
      "Epoch 500, Loss: 0.030599912628531456\n",
      "Epoch 1000, Loss: 0.02501261606812477\n",
      "Epoch 1500, Loss: 0.02067507803440094\n",
      "Epoch 2000, Loss: 0.022368308156728745\n",
      "Epoch 2500, Loss: 0.02280544862151146\n",
      "Epoch 3000, Loss: 0.023484356701374054\n",
      "Epoch 3500, Loss: 0.02302853949368\n",
      "Epoch 4000, Loss: 0.023573998361825943\n",
      "Epoch 4500, Loss: 0.022125232964754105\n",
      "Epoch 4999, Loss: 0.021517256274819374\n",
      "--- Experiment 17/20 ---\n",
      "Epoch 0, Loss: 0.16768871247768402\n",
      "Epoch 500, Loss: 0.023521479219198227\n",
      "Epoch 1000, Loss: 0.02358587272465229\n",
      "Epoch 1500, Loss: 0.0190038550645113\n",
      "Epoch 2000, Loss: 0.018509671092033386\n",
      "Epoch 2500, Loss: 0.019278232008218765\n",
      "Epoch 3000, Loss: 0.016942458227276802\n",
      "Epoch 3500, Loss: 0.01694013737142086\n",
      "Epoch 4000, Loss: 0.018125012516975403\n",
      "Epoch 4500, Loss: 0.01765919104218483\n",
      "Epoch 4999, Loss: 0.017463330179452896\n",
      "Epoch 0, Loss: 0.17004981637001038\n",
      "Epoch 500, Loss: 0.0430179201066494\n",
      "Epoch 1000, Loss: 0.03765343129634857\n",
      "Epoch 1500, Loss: 0.033740460872650146\n",
      "Epoch 2000, Loss: 0.033946096897125244\n",
      "Epoch 2500, Loss: 0.03224857896566391\n",
      "Epoch 3000, Loss: 0.032554492354393005\n",
      "Epoch 3500, Loss: 0.03161787614226341\n",
      "Epoch 4000, Loss: 0.03227725252509117\n",
      "Epoch 4500, Loss: 0.031058315187692642\n",
      "Epoch 4999, Loss: 0.030219171196222305\n",
      "Epoch 0, Loss: 0.17253844439983368\n",
      "Epoch 500, Loss: 0.03426828607916832\n",
      "Epoch 1000, Loss: 0.027914047241210938\n",
      "Epoch 1500, Loss: 0.02801664173603058\n",
      "Epoch 2000, Loss: 0.026057077571749687\n",
      "Epoch 2500, Loss: 0.026196008548140526\n",
      "Epoch 3000, Loss: 0.025898735970258713\n",
      "Epoch 3500, Loss: 0.023528054356575012\n",
      "Epoch 4000, Loss: 0.024070028215646744\n",
      "Epoch 4500, Loss: 0.024451080709695816\n",
      "Epoch 4999, Loss: 0.02319573238492012\n",
      "Epoch 0, Loss: 0.25100356340408325\n",
      "Epoch 500, Loss: 0.02912251278758049\n",
      "Epoch 1000, Loss: 0.02356923371553421\n",
      "Epoch 1500, Loss: 0.023249546065926552\n",
      "Epoch 2000, Loss: 0.024299796670675278\n",
      "Epoch 2500, Loss: 0.021256722509860992\n",
      "Epoch 3000, Loss: 0.02191932685673237\n",
      "Epoch 3500, Loss: 0.018595218658447266\n",
      "Epoch 4000, Loss: 0.024202078580856323\n",
      "Epoch 4500, Loss: 0.022038981318473816\n",
      "Epoch 4999, Loss: 0.024105940014123917\n",
      "--- Experiment 18/20 ---\n",
      "Epoch 0, Loss: 0.2991616427898407\n",
      "Epoch 500, Loss: 0.02428530342876911\n",
      "Epoch 1000, Loss: 0.02261749655008316\n",
      "Epoch 1500, Loss: 0.018728293478488922\n",
      "Epoch 2000, Loss: 0.019298380240797997\n",
      "Epoch 2500, Loss: 0.01766378805041313\n",
      "Epoch 3000, Loss: 0.019512811675667763\n",
      "Epoch 3500, Loss: 0.017215251922607422\n",
      "Epoch 4000, Loss: 0.01619407907128334\n",
      "Epoch 4500, Loss: 0.01819891855120659\n",
      "Epoch 4999, Loss: 0.016931887716054916\n",
      "Epoch 0, Loss: 0.14681459963321686\n",
      "Epoch 500, Loss: 0.04331165552139282\n",
      "Epoch 1000, Loss: 0.040793757885694504\n",
      "Epoch 1500, Loss: 0.04022931680083275\n",
      "Epoch 2000, Loss: 0.035078857094049454\n",
      "Epoch 2500, Loss: 0.03549709916114807\n",
      "Epoch 3000, Loss: 0.03145381435751915\n",
      "Epoch 3500, Loss: 0.03220301494002342\n",
      "Epoch 4000, Loss: 0.031256187707185745\n",
      "Epoch 4500, Loss: 0.03253224864602089\n",
      "Epoch 4999, Loss: 0.028270842507481575\n",
      "Epoch 0, Loss: 0.11452342569828033\n",
      "Epoch 500, Loss: 0.03368036076426506\n",
      "Epoch 1000, Loss: 0.029523219913244247\n",
      "Epoch 1500, Loss: 0.02711550146341324\n",
      "Epoch 2000, Loss: 0.02552645280957222\n",
      "Epoch 2500, Loss: 0.02596593275666237\n",
      "Epoch 3000, Loss: 0.025242071598768234\n",
      "Epoch 3500, Loss: 0.026475347578525543\n",
      "Epoch 4000, Loss: 0.024161674082279205\n",
      "Epoch 4500, Loss: 0.024754079058766365\n",
      "Epoch 4999, Loss: 0.024245433509349823\n",
      "Epoch 0, Loss: 0.17363983392715454\n",
      "Epoch 500, Loss: 0.029647398740053177\n",
      "Epoch 1000, Loss: 0.02569226361811161\n",
      "Epoch 1500, Loss: 0.023533614352345467\n",
      "Epoch 2000, Loss: 0.024480517953634262\n",
      "Epoch 2500, Loss: 0.022820590063929558\n",
      "Epoch 3000, Loss: 0.019670702517032623\n",
      "Epoch 3500, Loss: 0.021606743335723877\n",
      "Epoch 4000, Loss: 0.0213076863437891\n",
      "Epoch 4500, Loss: 0.025055013597011566\n",
      "Epoch 4999, Loss: 0.02372989058494568\n",
      "--- Experiment 19/20 ---\n",
      "Epoch 0, Loss: 0.22382736206054688\n",
      "Epoch 500, Loss: 0.022746657952666283\n",
      "Epoch 1000, Loss: 0.021313369274139404\n",
      "Epoch 1500, Loss: 0.019145965576171875\n",
      "Epoch 2000, Loss: 0.016748668625950813\n",
      "Epoch 2500, Loss: 0.01740676909685135\n",
      "Epoch 3000, Loss: 0.01660292036831379\n",
      "Epoch 3500, Loss: 0.017238594591617584\n",
      "Epoch 4000, Loss: 0.017920810729265213\n",
      "Epoch 4500, Loss: 0.016547834500670433\n",
      "Epoch 4999, Loss: 0.019282767549157143\n",
      "Epoch 0, Loss: 0.15163789689540863\n",
      "Epoch 500, Loss: 0.04224114865064621\n",
      "Epoch 1000, Loss: 0.03865491971373558\n",
      "Epoch 1500, Loss: 0.034160904586315155\n",
      "Epoch 2000, Loss: 0.03385898843407631\n",
      "Epoch 2500, Loss: 0.03169422969222069\n",
      "Epoch 3000, Loss: 0.030618026852607727\n",
      "Epoch 3500, Loss: 0.03185528889298439\n",
      "Epoch 4000, Loss: 0.03068753145635128\n",
      "Epoch 4500, Loss: 0.030332330614328384\n",
      "Epoch 4999, Loss: 0.02963082864880562\n",
      "Epoch 0, Loss: 0.35351547598838806\n",
      "Epoch 500, Loss: 0.03388119861483574\n",
      "Epoch 1000, Loss: 0.029813425615429878\n",
      "Epoch 1500, Loss: 0.02620742656290531\n",
      "Epoch 2000, Loss: 0.028596922755241394\n",
      "Epoch 2500, Loss: 0.023054957389831543\n",
      "Epoch 3000, Loss: 0.02715354412794113\n",
      "Epoch 3500, Loss: 0.026109065860509872\n",
      "Epoch 4000, Loss: 0.024610385298728943\n",
      "Epoch 4500, Loss: 0.024852722883224487\n",
      "Epoch 4999, Loss: 0.022570444270968437\n",
      "Epoch 0, Loss: 0.31798532605171204\n",
      "Epoch 500, Loss: 0.0316503569483757\n",
      "Epoch 1000, Loss: 0.026896841824054718\n",
      "Epoch 1500, Loss: 0.02525687776505947\n",
      "Epoch 2000, Loss: 0.02495362050831318\n",
      "Epoch 2500, Loss: 0.022565165534615517\n",
      "Epoch 3000, Loss: 0.022094205021858215\n",
      "Epoch 3500, Loss: 0.023124882951378822\n",
      "Epoch 4000, Loss: 0.024166349321603775\n",
      "Epoch 4500, Loss: 0.02448086440563202\n",
      "Epoch 4999, Loss: 0.024272389709949493\n",
      "--- Experiment 20/20 ---\n",
      "Epoch 0, Loss: 0.13058653473854065\n",
      "Epoch 500, Loss: 0.0226149745285511\n",
      "Epoch 1000, Loss: 0.0204803217202425\n",
      "Epoch 1500, Loss: 0.018368441611528397\n",
      "Epoch 2000, Loss: 0.01728902943432331\n",
      "Epoch 2500, Loss: 0.017613613978028297\n",
      "Epoch 3000, Loss: 0.018595755100250244\n",
      "Epoch 3500, Loss: 0.01748535968363285\n",
      "Epoch 4000, Loss: 0.01849265769124031\n",
      "Epoch 4500, Loss: 0.017364809289574623\n",
      "Epoch 4999, Loss: 0.017574448138475418\n",
      "Epoch 0, Loss: 0.15913069248199463\n",
      "Epoch 500, Loss: 0.03983702138066292\n",
      "Epoch 1000, Loss: 0.037206102162599564\n",
      "Epoch 1500, Loss: 0.032424792647361755\n",
      "Epoch 2000, Loss: 0.03330942243337631\n",
      "Epoch 2500, Loss: 0.03432852029800415\n",
      "Epoch 3000, Loss: 0.039708953350782394\n",
      "Epoch 3500, Loss: 0.030858471989631653\n",
      "Epoch 4000, Loss: 0.032996393740177155\n",
      "Epoch 4500, Loss: 0.03154492378234863\n",
      "Epoch 4999, Loss: 0.030082745477557182\n",
      "Epoch 0, Loss: 0.15218311548233032\n",
      "Epoch 500, Loss: 0.031251922249794006\n",
      "Epoch 1000, Loss: 0.027888676151633263\n",
      "Epoch 1500, Loss: 0.025148699060082436\n",
      "Epoch 2000, Loss: 0.02639967016875744\n",
      "Epoch 2500, Loss: 0.02508608251810074\n",
      "Epoch 3000, Loss: 0.02488144300878048\n",
      "Epoch 3500, Loss: 0.023851698264479637\n",
      "Epoch 4000, Loss: 0.032373834401369095\n",
      "Epoch 4500, Loss: 0.029299329966306686\n",
      "Epoch 4999, Loss: 0.026258744299411774\n",
      "Epoch 0, Loss: 0.16737410426139832\n",
      "Epoch 500, Loss: 0.029594291001558304\n",
      "Epoch 1000, Loss: 0.025923920795321465\n",
      "Epoch 1500, Loss: 0.02561596781015396\n",
      "Epoch 2000, Loss: 0.023429132997989655\n",
      "Epoch 2500, Loss: 0.023980263620615005\n",
      "Epoch 3000, Loss: 0.025175120681524277\n",
      "Epoch 3500, Loss: 0.021033775061368942\n",
      "Epoch 4000, Loss: 0.025013282895088196\n",
      "Epoch 4500, Loss: 0.023177092894911766\n",
      "Epoch 4999, Loss: 0.02443115971982479\n",
      "Average Validation Loss (Base):   0.017061 ± 0.000441\n",
      "Average Validation Loss (Import): 0.025574 ± 0.001264\n",
      "Average Validation Loss (GP Out): 0.022068 ± 0.001193\n",
      "Average Validation Loss (GP Res): 0.020674 ± 0.001464\n",
      "\n",
      "==== Running experiments for IC: gaussian, BC: periodic ====\n",
      "--- Experiment 1/20 ---\n",
      "Epoch 0, Loss: 0.11515452712774277\n",
      "Epoch 500, Loss: 0.02235427498817444\n",
      "Epoch 1000, Loss: 0.01996101811528206\n",
      "Epoch 1500, Loss: 0.0190771222114563\n",
      "Epoch 2000, Loss: 0.019927095621824265\n",
      "Epoch 2500, Loss: 0.01759510114789009\n",
      "Epoch 3000, Loss: 0.017893042415380478\n",
      "Epoch 3500, Loss: 0.019611161202192307\n",
      "Epoch 4000, Loss: 0.01698995754122734\n",
      "Epoch 4500, Loss: 0.01819678395986557\n",
      "Epoch 4999, Loss: 0.016455426812171936\n",
      "Epoch 0, Loss: 0.14403481781482697\n",
      "Epoch 500, Loss: 0.03791561350226402\n",
      "Epoch 1000, Loss: 0.03632707521319389\n",
      "Epoch 1500, Loss: 0.032094161957502365\n",
      "Epoch 2000, Loss: 0.03145946189761162\n",
      "Epoch 2500, Loss: 0.03178604319691658\n",
      "Epoch 3000, Loss: 0.03048708662390709\n",
      "Epoch 3500, Loss: 0.03148351609706879\n",
      "Epoch 4000, Loss: 0.028426578268408775\n",
      "Epoch 4500, Loss: 0.02934015542268753\n",
      "Epoch 4999, Loss: 0.028984133154153824\n",
      "Epoch 0, Loss: 0.2243724763393402\n",
      "Epoch 500, Loss: 0.032165274024009705\n",
      "Epoch 1000, Loss: 0.02755705639719963\n",
      "Epoch 1500, Loss: 0.02628730610013008\n",
      "Epoch 2000, Loss: 0.024832624942064285\n",
      "Epoch 2500, Loss: 0.02130943350493908\n",
      "Epoch 3000, Loss: 0.02778935804963112\n",
      "Epoch 3500, Loss: 0.025811389088630676\n",
      "Epoch 4000, Loss: 0.025808505713939667\n",
      "Epoch 4500, Loss: 0.024242345243692398\n",
      "Epoch 4999, Loss: 0.02356736548244953\n",
      "Epoch 0, Loss: 0.340238094329834\n",
      "Epoch 500, Loss: 0.0313967801630497\n",
      "Epoch 1000, Loss: 0.023487675935029984\n",
      "Epoch 1500, Loss: 0.024953827261924744\n",
      "Epoch 2000, Loss: 0.025466419756412506\n",
      "Epoch 2500, Loss: 0.022983917966485023\n",
      "Epoch 3000, Loss: 0.0247470885515213\n",
      "Epoch 3500, Loss: 0.022645968943834305\n",
      "Epoch 4000, Loss: 0.02184540592133999\n",
      "Epoch 4500, Loss: 0.021708836778998375\n",
      "Epoch 4999, Loss: 0.0209162849932909\n",
      "--- Experiment 2/20 ---\n",
      "Epoch 0, Loss: 0.11895952373743057\n",
      "Epoch 500, Loss: 0.020457390695810318\n",
      "Epoch 1000, Loss: 0.020309049636125565\n",
      "Epoch 1500, Loss: 0.01777251437306404\n",
      "Epoch 2000, Loss: 0.020511601120233536\n",
      "Epoch 2500, Loss: 0.017316989600658417\n",
      "Epoch 3000, Loss: 0.018496569246053696\n",
      "Epoch 3500, Loss: 0.018810778856277466\n",
      "Epoch 4000, Loss: 0.020786378532648087\n",
      "Epoch 4500, Loss: 0.016128312796354294\n",
      "Epoch 4999, Loss: 0.016740627586841583\n",
      "Epoch 0, Loss: 0.14978039264678955\n",
      "Epoch 500, Loss: 0.03963316231966019\n",
      "Epoch 1000, Loss: 0.035894542932510376\n",
      "Epoch 1500, Loss: 0.03260694816708565\n",
      "Epoch 2000, Loss: 0.0341736264526844\n",
      "Epoch 2500, Loss: 0.034540917724370956\n",
      "Epoch 3000, Loss: 0.03471428528428078\n",
      "Epoch 3500, Loss: 0.029496170580387115\n",
      "Epoch 4000, Loss: 0.028929729014635086\n",
      "Epoch 4500, Loss: 0.030016958713531494\n",
      "Epoch 4999, Loss: 0.029249537736177444\n",
      "Epoch 0, Loss: 0.23546138405799866\n",
      "Epoch 500, Loss: 0.03288101404905319\n",
      "Epoch 1000, Loss: 0.027877435088157654\n",
      "Epoch 1500, Loss: 0.024592027068138123\n",
      "Epoch 2000, Loss: 0.02377457730472088\n",
      "Epoch 2500, Loss: 0.025835681706666946\n",
      "Epoch 3000, Loss: 0.02482479065656662\n",
      "Epoch 3500, Loss: 0.02500140108168125\n",
      "Epoch 4000, Loss: 0.024696992710232735\n",
      "Epoch 4500, Loss: 0.02330862358212471\n",
      "Epoch 4999, Loss: 0.027660155668854713\n",
      "Epoch 0, Loss: 0.21524611115455627\n",
      "Epoch 500, Loss: 0.02910469099879265\n",
      "Epoch 1000, Loss: 0.025861095637083054\n",
      "Epoch 1500, Loss: 0.020738013088703156\n",
      "Epoch 2000, Loss: 0.022517509758472443\n",
      "Epoch 2500, Loss: 0.02280045673251152\n",
      "Epoch 3000, Loss: 0.02243335358798504\n",
      "Epoch 3500, Loss: 0.02175247296690941\n",
      "Epoch 4000, Loss: 0.0200333371758461\n",
      "Epoch 4500, Loss: 0.020306870341300964\n",
      "Epoch 4999, Loss: 0.019191225990653038\n",
      "--- Experiment 3/20 ---\n",
      "Epoch 0, Loss: 0.1276436597108841\n",
      "Epoch 500, Loss: 0.02172258496284485\n",
      "Epoch 1000, Loss: 0.01875336281955242\n",
      "Epoch 1500, Loss: 0.019965197890996933\n",
      "Epoch 2000, Loss: 0.019006099551916122\n",
      "Epoch 2500, Loss: 0.01807018369436264\n",
      "Epoch 3000, Loss: 0.01715032197535038\n",
      "Epoch 3500, Loss: 0.01772162690758705\n",
      "Epoch 4000, Loss: 0.016935767605900764\n",
      "Epoch 4500, Loss: 0.01788220927119255\n",
      "Epoch 4999, Loss: 0.01629817858338356\n",
      "Epoch 0, Loss: 0.23644277453422546\n",
      "Epoch 500, Loss: 0.04346498101949692\n",
      "Epoch 1000, Loss: 0.03663364425301552\n",
      "Epoch 1500, Loss: 0.03914244472980499\n",
      "Epoch 2000, Loss: 0.03274502605199814\n",
      "Epoch 2500, Loss: 0.03110005520284176\n",
      "Epoch 3000, Loss: 0.030752316117286682\n",
      "Epoch 3500, Loss: 0.030529581010341644\n",
      "Epoch 4000, Loss: 0.029804283753037453\n",
      "Epoch 4500, Loss: 0.030681857839226723\n",
      "Epoch 4999, Loss: 0.027332864701747894\n",
      "Epoch 0, Loss: 0.30493277311325073\n",
      "Epoch 500, Loss: 0.03521665185689926\n",
      "Epoch 1000, Loss: 0.028790947049856186\n",
      "Epoch 1500, Loss: 0.027812130749225616\n",
      "Epoch 2000, Loss: 0.0280033927410841\n",
      "Epoch 2500, Loss: 0.026022646576166153\n",
      "Epoch 3000, Loss: 0.027679264545440674\n",
      "Epoch 3500, Loss: 0.02652459777891636\n",
      "Epoch 4000, Loss: 0.025417689234018326\n",
      "Epoch 4500, Loss: 0.02546822652220726\n",
      "Epoch 4999, Loss: 0.02396583929657936\n",
      "Epoch 0, Loss: 0.15641601383686066\n",
      "Epoch 500, Loss: 0.030326325446367264\n",
      "Epoch 1000, Loss: 0.024721428751945496\n",
      "Epoch 1500, Loss: 0.022933917120099068\n",
      "Epoch 2000, Loss: 0.02375212125480175\n",
      "Epoch 2500, Loss: 0.021349072456359863\n",
      "Epoch 3000, Loss: 0.02152659371495247\n",
      "Epoch 3500, Loss: 0.022386614233255386\n",
      "Epoch 4000, Loss: 0.021045688539743423\n",
      "Epoch 4500, Loss: 0.02644171752035618\n",
      "Epoch 4999, Loss: 0.02305729314684868\n",
      "--- Experiment 4/20 ---\n",
      "Epoch 0, Loss: 0.21087254583835602\n",
      "Epoch 500, Loss: 0.02326676808297634\n",
      "Epoch 1000, Loss: 0.01944316178560257\n",
      "Epoch 1500, Loss: 0.018419621512293816\n",
      "Epoch 2000, Loss: 0.01706305332481861\n",
      "Epoch 2500, Loss: 0.018509823828935623\n",
      "Epoch 3000, Loss: 0.01804349571466446\n",
      "Epoch 3500, Loss: 0.017786504700779915\n",
      "Epoch 4000, Loss: 0.016740035265684128\n",
      "Epoch 4500, Loss: 0.015500018373131752\n",
      "Epoch 4999, Loss: 0.018231898546218872\n",
      "Epoch 0, Loss: 0.2635319232940674\n",
      "Epoch 500, Loss: 0.04282347857952118\n",
      "Epoch 1000, Loss: 0.03725796192884445\n",
      "Epoch 1500, Loss: 0.0345485545694828\n",
      "Epoch 2000, Loss: 0.033627428114414215\n",
      "Epoch 2500, Loss: 0.03263073042035103\n",
      "Epoch 3000, Loss: 0.03380268067121506\n",
      "Epoch 3500, Loss: 0.030866185203194618\n",
      "Epoch 4000, Loss: 0.030885793268680573\n",
      "Epoch 4500, Loss: 0.02949729561805725\n",
      "Epoch 4999, Loss: 0.03106863796710968\n",
      "Epoch 0, Loss: 0.12677331268787384\n",
      "Epoch 500, Loss: 0.03143367916345596\n",
      "Epoch 1000, Loss: 0.0292680487036705\n",
      "Epoch 1500, Loss: 0.02673320844769478\n",
      "Epoch 2000, Loss: 0.02762107364833355\n",
      "Epoch 2500, Loss: 0.025204265490174294\n",
      "Epoch 3000, Loss: 0.024021344259381294\n",
      "Epoch 3500, Loss: 0.024739596992731094\n",
      "Epoch 4000, Loss: 0.024277042597532272\n",
      "Epoch 4500, Loss: 0.023516014218330383\n",
      "Epoch 4999, Loss: 0.030701681971549988\n",
      "Epoch 0, Loss: 0.15084660053253174\n",
      "Epoch 500, Loss: 0.029152177274227142\n",
      "Epoch 1000, Loss: 0.025913679972290993\n",
      "Epoch 1500, Loss: 0.022459957748651505\n",
      "Epoch 2000, Loss: 0.027563396841287613\n",
      "Epoch 2500, Loss: 0.024439558386802673\n",
      "Epoch 3000, Loss: 0.022347815334796906\n",
      "Epoch 3500, Loss: 0.023933541029691696\n",
      "Epoch 4000, Loss: 0.023420311510562897\n",
      "Epoch 4500, Loss: 0.020598717033863068\n",
      "Epoch 4999, Loss: 0.02277257665991783\n",
      "--- Experiment 5/20 ---\n",
      "Epoch 0, Loss: 0.19022423028945923\n",
      "Epoch 500, Loss: 0.02504608780145645\n",
      "Epoch 1000, Loss: 0.019766829907894135\n",
      "Epoch 1500, Loss: 0.020592432469129562\n",
      "Epoch 2000, Loss: 0.017731469124555588\n",
      "Epoch 2500, Loss: 0.017014294862747192\n",
      "Epoch 3000, Loss: 0.018321111798286438\n",
      "Epoch 3500, Loss: 0.018000340089201927\n",
      "Epoch 4000, Loss: 0.017658837139606476\n",
      "Epoch 4500, Loss: 0.016434645280241966\n",
      "Epoch 4999, Loss: 0.017184194177389145\n",
      "Epoch 0, Loss: 0.22631165385246277\n",
      "Epoch 500, Loss: 0.041453536599874496\n",
      "Epoch 1000, Loss: 0.036738812923431396\n",
      "Epoch 1500, Loss: 0.035170577466487885\n",
      "Epoch 2000, Loss: 0.03259647637605667\n",
      "Epoch 2500, Loss: 0.03193477541208267\n",
      "Epoch 3000, Loss: 0.03271988406777382\n",
      "Epoch 3500, Loss: 0.030065836384892464\n",
      "Epoch 4000, Loss: 0.029732119292020798\n",
      "Epoch 4500, Loss: 0.028761953115463257\n",
      "Epoch 4999, Loss: 0.035851627588272095\n",
      "Epoch 0, Loss: 0.3106794059276581\n",
      "Epoch 500, Loss: 0.03274690359830856\n",
      "Epoch 1000, Loss: 0.02824564278125763\n",
      "Epoch 1500, Loss: 0.02654552459716797\n",
      "Epoch 2000, Loss: 0.025296133011579514\n",
      "Epoch 2500, Loss: 0.025510821491479874\n",
      "Epoch 3000, Loss: 0.023560822010040283\n",
      "Epoch 3500, Loss: 0.02469012886285782\n",
      "Epoch 4000, Loss: 0.028450001031160355\n",
      "Epoch 4500, Loss: 0.026307042688131332\n",
      "Epoch 4999, Loss: 0.025390198454260826\n",
      "Epoch 0, Loss: 0.25344234704971313\n",
      "Epoch 500, Loss: 0.029234036803245544\n",
      "Epoch 1000, Loss: 0.027200784534215927\n",
      "Epoch 1500, Loss: 0.024831723421812057\n",
      "Epoch 2000, Loss: 0.02320213057100773\n",
      "Epoch 2500, Loss: 0.02390383556485176\n",
      "Epoch 3000, Loss: 0.023474454879760742\n",
      "Epoch 3500, Loss: 0.022867653518915176\n",
      "Epoch 4000, Loss: 0.02005457505583763\n",
      "Epoch 4500, Loss: 0.019480926916003227\n",
      "Epoch 4999, Loss: 0.022285208106040955\n",
      "--- Experiment 6/20 ---\n",
      "Epoch 0, Loss: 0.19679583609104156\n",
      "Epoch 500, Loss: 0.022267580032348633\n",
      "Epoch 1000, Loss: 0.020443418994545937\n",
      "Epoch 1500, Loss: 0.020400669425725937\n",
      "Epoch 2000, Loss: 0.018661977723240852\n",
      "Epoch 2500, Loss: 0.01795284077525139\n",
      "Epoch 3000, Loss: 0.017457928508520126\n",
      "Epoch 3500, Loss: 0.017609428614377975\n",
      "Epoch 4000, Loss: 0.019946064800024033\n",
      "Epoch 4500, Loss: 0.01837901957333088\n",
      "Epoch 4999, Loss: 0.0166326891630888\n",
      "Epoch 0, Loss: 0.14216230809688568\n",
      "Epoch 500, Loss: 0.043811120092868805\n",
      "Epoch 1000, Loss: 0.038163285702466965\n",
      "Epoch 1500, Loss: 0.03546860069036484\n",
      "Epoch 2000, Loss: 0.03413303196430206\n",
      "Epoch 2500, Loss: 0.03173425421118736\n",
      "Epoch 3000, Loss: 0.03416524827480316\n",
      "Epoch 3500, Loss: 0.033292341977357864\n",
      "Epoch 4000, Loss: 0.033617567270994186\n",
      "Epoch 4500, Loss: 0.031245345249772072\n",
      "Epoch 4999, Loss: 0.029672378674149513\n",
      "Epoch 0, Loss: 0.3986618220806122\n",
      "Epoch 500, Loss: 0.03693901374936104\n",
      "Epoch 1000, Loss: 0.030983787029981613\n",
      "Epoch 1500, Loss: 0.027697376906871796\n",
      "Epoch 2000, Loss: 0.0263463594019413\n",
      "Epoch 2500, Loss: 0.02649855427443981\n",
      "Epoch 3000, Loss: 0.02306954748928547\n",
      "Epoch 3500, Loss: 0.0250110886991024\n",
      "Epoch 4000, Loss: 0.024055998772382736\n",
      "Epoch 4500, Loss: 0.024081088602542877\n",
      "Epoch 4999, Loss: 0.02431027591228485\n",
      "Epoch 0, Loss: 0.1664547622203827\n",
      "Epoch 500, Loss: 0.02765197493135929\n",
      "Epoch 1000, Loss: 0.02550290711224079\n",
      "Epoch 1500, Loss: 0.022316180169582367\n",
      "Epoch 2000, Loss: 0.020575104281306267\n",
      "Epoch 2500, Loss: 0.021399743854999542\n",
      "Epoch 3000, Loss: 0.0230928473174572\n",
      "Epoch 3500, Loss: 0.021756678819656372\n",
      "Epoch 4000, Loss: 0.021604159846901894\n",
      "Epoch 4500, Loss: 0.025047888979315758\n",
      "Epoch 4999, Loss: 0.02382669784128666\n",
      "--- Experiment 7/20 ---\n",
      "Epoch 0, Loss: 0.1729009449481964\n",
      "Epoch 500, Loss: 0.021960357204079628\n",
      "Epoch 1000, Loss: 0.019013719633221626\n",
      "Epoch 1500, Loss: 0.018290109932422638\n",
      "Epoch 2000, Loss: 0.01849294826388359\n",
      "Epoch 2500, Loss: 0.018712442368268967\n",
      "Epoch 3000, Loss: 0.017046041786670685\n",
      "Epoch 3500, Loss: 0.017092231661081314\n",
      "Epoch 4000, Loss: 0.016938526183366776\n",
      "Epoch 4500, Loss: 0.016950350254774094\n",
      "Epoch 4999, Loss: 0.01570148766040802\n",
      "Epoch 0, Loss: 0.17515410482883453\n",
      "Epoch 500, Loss: 0.04391743242740631\n",
      "Epoch 1000, Loss: 0.03835466504096985\n",
      "Epoch 1500, Loss: 0.03644432872533798\n",
      "Epoch 2000, Loss: 0.03391236439347267\n",
      "Epoch 2500, Loss: 0.03405340760946274\n",
      "Epoch 3000, Loss: 0.03384052962064743\n",
      "Epoch 3500, Loss: 0.03186408057808876\n",
      "Epoch 4000, Loss: 0.030898243188858032\n",
      "Epoch 4500, Loss: 0.028963008895516396\n",
      "Epoch 4999, Loss: 0.02964574657380581\n",
      "Epoch 0, Loss: 0.2894396483898163\n",
      "Epoch 500, Loss: 0.034208156168460846\n",
      "Epoch 1000, Loss: 0.025796515867114067\n",
      "Epoch 1500, Loss: 0.027033649384975433\n",
      "Epoch 2000, Loss: 0.02608110010623932\n",
      "Epoch 2500, Loss: 0.025442862883210182\n",
      "Epoch 3000, Loss: 0.025637798011302948\n",
      "Epoch 3500, Loss: 0.02561105415225029\n",
      "Epoch 4000, Loss: 0.024624448269605637\n",
      "Epoch 4500, Loss: 0.024897895753383636\n",
      "Epoch 4999, Loss: 0.024946555495262146\n",
      "Epoch 0, Loss: 0.15449661016464233\n",
      "Epoch 500, Loss: 0.030190013349056244\n",
      "Epoch 1000, Loss: 0.02277013100683689\n",
      "Epoch 1500, Loss: 0.02327064983546734\n",
      "Epoch 2000, Loss: 0.023523714393377304\n",
      "Epoch 2500, Loss: 0.026402942836284637\n",
      "Epoch 3000, Loss: 0.022278863936662674\n",
      "Epoch 3500, Loss: 0.023141732439398766\n",
      "Epoch 4000, Loss: 0.02302338182926178\n",
      "Epoch 4500, Loss: 0.022952139377593994\n",
      "Epoch 4999, Loss: 0.024101369082927704\n",
      "--- Experiment 8/20 ---\n",
      "Epoch 0, Loss: 0.22405152022838593\n",
      "Epoch 500, Loss: 0.023562978953123093\n",
      "Epoch 1000, Loss: 0.0199337899684906\n",
      "Epoch 1500, Loss: 0.018768109381198883\n",
      "Epoch 2000, Loss: 0.0199529230594635\n",
      "Epoch 2500, Loss: 0.018070217221975327\n",
      "Epoch 3000, Loss: 0.01751582883298397\n",
      "Epoch 3500, Loss: 0.01674303598701954\n",
      "Epoch 4000, Loss: 0.01754288747906685\n",
      "Epoch 4500, Loss: 0.016721593216061592\n",
      "Epoch 4999, Loss: 0.01755945384502411\n",
      "Epoch 0, Loss: 0.2372085154056549\n",
      "Epoch 500, Loss: 0.042492810636758804\n",
      "Epoch 1000, Loss: 0.03961193934082985\n",
      "Epoch 1500, Loss: 0.036746568977832794\n",
      "Epoch 2000, Loss: 0.034372057765722275\n",
      "Epoch 2500, Loss: 0.033575091511011124\n",
      "Epoch 3000, Loss: 0.031407471746206284\n",
      "Epoch 3500, Loss: 0.03490102291107178\n",
      "Epoch 4000, Loss: 0.030224164947867393\n",
      "Epoch 4500, Loss: 0.03035551868379116\n",
      "Epoch 4999, Loss: 0.029485393315553665\n",
      "Epoch 0, Loss: 0.235161691904068\n",
      "Epoch 500, Loss: 0.03212691470980644\n",
      "Epoch 1000, Loss: 0.02752000093460083\n",
      "Epoch 1500, Loss: 0.026330873370170593\n",
      "Epoch 2000, Loss: 0.0242922343313694\n",
      "Epoch 2500, Loss: 0.02514880895614624\n",
      "Epoch 3000, Loss: 0.024516016244888306\n",
      "Epoch 3500, Loss: 0.025018727406859398\n",
      "Epoch 4000, Loss: 0.02764667198061943\n",
      "Epoch 4500, Loss: 0.025046147406101227\n",
      "Epoch 4999, Loss: 0.0240433719009161\n",
      "Epoch 0, Loss: 0.27677127718925476\n",
      "Epoch 500, Loss: 0.032192669808864594\n",
      "Epoch 1000, Loss: 0.025471460074186325\n",
      "Epoch 1500, Loss: 0.022454265505075455\n",
      "Epoch 2000, Loss: 0.023438896983861923\n",
      "Epoch 2500, Loss: 0.022944960743188858\n",
      "Epoch 3000, Loss: 0.02219362184405327\n",
      "Epoch 3500, Loss: 0.022818362340331078\n",
      "Epoch 4000, Loss: 0.022519659250974655\n",
      "Epoch 4500, Loss: 0.02578960917890072\n",
      "Epoch 4999, Loss: 0.02194778062403202\n",
      "--- Experiment 9/20 ---\n",
      "Epoch 0, Loss: 0.1187027245759964\n",
      "Epoch 500, Loss: 0.02330886386334896\n",
      "Epoch 1000, Loss: 0.018605191260576248\n",
      "Epoch 1500, Loss: 0.019565356895327568\n",
      "Epoch 2000, Loss: 0.01761728897690773\n",
      "Epoch 2500, Loss: 0.016269225627183914\n",
      "Epoch 3000, Loss: 0.018132105469703674\n",
      "Epoch 3500, Loss: 0.017262261360883713\n",
      "Epoch 4000, Loss: 0.017316753044724464\n",
      "Epoch 4500, Loss: 0.016630057245492935\n",
      "Epoch 4999, Loss: 0.022289931774139404\n",
      "Epoch 0, Loss: 0.3335312604904175\n",
      "Epoch 500, Loss: 0.042754463851451874\n",
      "Epoch 1000, Loss: 0.03815556690096855\n",
      "Epoch 1500, Loss: 0.03380467742681503\n",
      "Epoch 2000, Loss: 0.032677534967660904\n",
      "Epoch 2500, Loss: 0.03042422980070114\n",
      "Epoch 3000, Loss: 0.03205403685569763\n",
      "Epoch 3500, Loss: 0.028987513855099678\n",
      "Epoch 4000, Loss: 0.029214629903435707\n",
      "Epoch 4500, Loss: 0.03109295293688774\n",
      "Epoch 4999, Loss: 0.0321626141667366\n",
      "Epoch 0, Loss: 0.16037966310977936\n",
      "Epoch 500, Loss: 0.0300887618213892\n",
      "Epoch 1000, Loss: 0.027860622853040695\n",
      "Epoch 1500, Loss: 0.026216767728328705\n",
      "Epoch 2000, Loss: 0.025631487369537354\n",
      "Epoch 2500, Loss: 0.02402695082128048\n",
      "Epoch 3000, Loss: 0.023272164165973663\n",
      "Epoch 3500, Loss: 0.024977579712867737\n",
      "Epoch 4000, Loss: 0.022607460618019104\n",
      "Epoch 4500, Loss: 0.024249576032161713\n",
      "Epoch 4999, Loss: 0.023316089063882828\n",
      "Epoch 0, Loss: 0.3360635042190552\n",
      "Epoch 500, Loss: 0.02937910705804825\n",
      "Epoch 1000, Loss: 0.025178013369441032\n",
      "Epoch 1500, Loss: 0.02417777292430401\n",
      "Epoch 2000, Loss: 0.021959099918603897\n",
      "Epoch 2500, Loss: 0.02165193296968937\n",
      "Epoch 3000, Loss: 0.021328818053007126\n",
      "Epoch 3500, Loss: 0.021387863904237747\n",
      "Epoch 4000, Loss: 0.01852056384086609\n",
      "Epoch 4500, Loss: 0.02343824692070484\n",
      "Epoch 4999, Loss: 0.021505078300833702\n",
      "--- Experiment 10/20 ---\n",
      "Epoch 0, Loss: 0.1295206993818283\n",
      "Epoch 500, Loss: 0.02360057458281517\n",
      "Epoch 1000, Loss: 0.02209419757127762\n",
      "Epoch 1500, Loss: 0.020991353318095207\n",
      "Epoch 2000, Loss: 0.019273266196250916\n",
      "Epoch 2500, Loss: 0.018601343035697937\n",
      "Epoch 3000, Loss: 0.01911853440105915\n",
      "Epoch 3500, Loss: 0.01705009862780571\n",
      "Epoch 4000, Loss: 0.018651988357305527\n",
      "Epoch 4500, Loss: 0.01753414049744606\n",
      "Epoch 4999, Loss: 0.017852909862995148\n",
      "Epoch 0, Loss: 0.13832353055477142\n",
      "Epoch 500, Loss: 0.04197201132774353\n",
      "Epoch 1000, Loss: 0.03300357982516289\n",
      "Epoch 1500, Loss: 0.031173475086688995\n",
      "Epoch 2000, Loss: 0.03242284804582596\n",
      "Epoch 2500, Loss: 0.029619760811328888\n",
      "Epoch 3000, Loss: 0.03074084408581257\n",
      "Epoch 3500, Loss: 0.03192383795976639\n",
      "Epoch 4000, Loss: 0.02945530042052269\n",
      "Epoch 4500, Loss: 0.028553906828165054\n",
      "Epoch 4999, Loss: 0.02962925285100937\n",
      "Epoch 0, Loss: 0.16875669360160828\n",
      "Epoch 500, Loss: 0.033877402544021606\n",
      "Epoch 1000, Loss: 0.02884865179657936\n",
      "Epoch 1500, Loss: 0.026807084679603577\n",
      "Epoch 2000, Loss: 0.02661208063364029\n",
      "Epoch 2500, Loss: 0.0255265049636364\n",
      "Epoch 3000, Loss: 0.025265533477067947\n",
      "Epoch 3500, Loss: 0.02445303276181221\n",
      "Epoch 4000, Loss: 0.02328219637274742\n",
      "Epoch 4500, Loss: 0.023365234956145287\n",
      "Epoch 4999, Loss: 0.022283809259533882\n",
      "Epoch 0, Loss: 0.37905970215797424\n",
      "Epoch 500, Loss: 0.02973235957324505\n",
      "Epoch 1000, Loss: 0.02664767950773239\n",
      "Epoch 1500, Loss: 0.024355053901672363\n",
      "Epoch 2000, Loss: 0.023241568356752396\n",
      "Epoch 2500, Loss: 0.02015785500407219\n",
      "Epoch 3000, Loss: 0.022504959255456924\n",
      "Epoch 3500, Loss: 0.022560151293873787\n",
      "Epoch 4000, Loss: 0.02597299963235855\n",
      "Epoch 4500, Loss: 0.02603737637400627\n",
      "Epoch 4999, Loss: 0.024306103587150574\n",
      "--- Experiment 11/20 ---\n",
      "Epoch 0, Loss: 0.153411865234375\n",
      "Epoch 500, Loss: 0.022420870140194893\n",
      "Epoch 1000, Loss: 0.01843242160975933\n",
      "Epoch 1500, Loss: 0.017528899013996124\n",
      "Epoch 2000, Loss: 0.018984217196702957\n",
      "Epoch 2500, Loss: 0.018914196640253067\n",
      "Epoch 3000, Loss: 0.018818538635969162\n",
      "Epoch 3500, Loss: 0.017144039273262024\n",
      "Epoch 4000, Loss: 0.016151931136846542\n",
      "Epoch 4500, Loss: 0.016812780871987343\n",
      "Epoch 4999, Loss: 0.01683802343904972\n",
      "Epoch 0, Loss: 0.22644850611686707\n",
      "Epoch 500, Loss: 0.04247894883155823\n",
      "Epoch 1000, Loss: 0.040708743035793304\n",
      "Epoch 1500, Loss: 0.03550839424133301\n",
      "Epoch 2000, Loss: 0.03343312442302704\n",
      "Epoch 2500, Loss: 0.030966952443122864\n",
      "Epoch 3000, Loss: 0.030928194522857666\n",
      "Epoch 3500, Loss: 0.03237978368997574\n",
      "Epoch 4000, Loss: 0.03029397502541542\n",
      "Epoch 4500, Loss: 0.030044447630643845\n",
      "Epoch 4999, Loss: 0.02857215143740177\n",
      "Epoch 0, Loss: 0.1172717735171318\n",
      "Epoch 500, Loss: 0.032997481524944305\n",
      "Epoch 1000, Loss: 0.030369199812412262\n",
      "Epoch 1500, Loss: 0.02843063324689865\n",
      "Epoch 2000, Loss: 0.026685280725359917\n",
      "Epoch 2500, Loss: 0.026733342558145523\n",
      "Epoch 3000, Loss: 0.02632112056016922\n",
      "Epoch 3500, Loss: 0.025998078286647797\n",
      "Epoch 4000, Loss: 0.02488107420504093\n",
      "Epoch 4500, Loss: 0.025159114971756935\n",
      "Epoch 4999, Loss: 0.024455687031149864\n",
      "Epoch 0, Loss: 0.18536534905433655\n",
      "Epoch 500, Loss: 0.029048584401607513\n",
      "Epoch 1000, Loss: 0.026034394279122353\n",
      "Epoch 1500, Loss: 0.023774689063429832\n",
      "Epoch 2000, Loss: 0.021830720826983452\n",
      "Epoch 2500, Loss: 0.022062767297029495\n",
      "Epoch 3000, Loss: 0.02136814035475254\n",
      "Epoch 3500, Loss: 0.023363087326288223\n",
      "Epoch 4000, Loss: 0.02285098284482956\n",
      "Epoch 4500, Loss: 0.022790370509028435\n",
      "Epoch 4999, Loss: 0.020897381007671356\n",
      "--- Experiment 12/20 ---\n",
      "Epoch 0, Loss: 0.11402556300163269\n",
      "Epoch 500, Loss: 0.022150779142975807\n",
      "Epoch 1000, Loss: 0.01936444267630577\n",
      "Epoch 1500, Loss: 0.01815563440322876\n",
      "Epoch 2000, Loss: 0.017704933881759644\n",
      "Epoch 2500, Loss: 0.01678778976202011\n",
      "Epoch 3000, Loss: 0.01863917149603367\n",
      "Epoch 3500, Loss: 0.01859285682439804\n",
      "Epoch 4000, Loss: 0.0185529924929142\n",
      "Epoch 4500, Loss: 0.017998313531279564\n",
      "Epoch 4999, Loss: 0.018387123942375183\n",
      "Epoch 0, Loss: 0.20787160098552704\n",
      "Epoch 500, Loss: 0.043046146631240845\n",
      "Epoch 1000, Loss: 0.035296641290187836\n",
      "Epoch 1500, Loss: 0.0354696661233902\n",
      "Epoch 2000, Loss: 0.033315509557724\n",
      "Epoch 2500, Loss: 0.03161180764436722\n",
      "Epoch 3000, Loss: 0.03113209642469883\n",
      "Epoch 3500, Loss: 0.030976131558418274\n",
      "Epoch 4000, Loss: 0.030492397025227547\n",
      "Epoch 4500, Loss: 0.028433991596102715\n",
      "Epoch 4999, Loss: 0.028445478528738022\n",
      "Epoch 0, Loss: 0.4304303824901581\n",
      "Epoch 500, Loss: 0.034745946526527405\n",
      "Epoch 1000, Loss: 0.028962116688489914\n",
      "Epoch 1500, Loss: 0.025836734101176262\n",
      "Epoch 2000, Loss: 0.025241874158382416\n",
      "Epoch 2500, Loss: 0.025948138907551765\n",
      "Epoch 3000, Loss: 0.025104863569140434\n",
      "Epoch 3500, Loss: 0.025181874632835388\n",
      "Epoch 4000, Loss: 0.02374991960823536\n",
      "Epoch 4500, Loss: 0.024266334250569344\n",
      "Epoch 4999, Loss: 0.023601382970809937\n",
      "Epoch 0, Loss: 0.2168814241886139\n",
      "Epoch 500, Loss: 0.028829369693994522\n",
      "Epoch 1000, Loss: 0.026735737919807434\n",
      "Epoch 1500, Loss: 0.02478989027440548\n",
      "Epoch 2000, Loss: 0.02398400753736496\n",
      "Epoch 2500, Loss: 0.0242332611232996\n",
      "Epoch 3000, Loss: 0.023242179304361343\n",
      "Epoch 3500, Loss: 0.02218743786215782\n",
      "Epoch 4000, Loss: 0.023610662668943405\n",
      "Epoch 4500, Loss: 0.02017853409051895\n",
      "Epoch 4999, Loss: 0.0248013436794281\n",
      "--- Experiment 13/20 ---\n",
      "Epoch 0, Loss: 0.26726120710372925\n",
      "Epoch 500, Loss: 0.0230303592979908\n",
      "Epoch 1000, Loss: 0.020322319120168686\n",
      "Epoch 1500, Loss: 0.01830759458243847\n",
      "Epoch 2000, Loss: 0.017912156879901886\n",
      "Epoch 2500, Loss: 0.019372036680579185\n",
      "Epoch 3000, Loss: 0.017809759825468063\n",
      "Epoch 3500, Loss: 0.017043333500623703\n",
      "Epoch 4000, Loss: 0.01739022508263588\n",
      "Epoch 4500, Loss: 0.01628529652953148\n",
      "Epoch 4999, Loss: 0.016901277005672455\n",
      "Epoch 0, Loss: 0.23211626708507538\n",
      "Epoch 500, Loss: 0.04472588375210762\n",
      "Epoch 1000, Loss: 0.03720160573720932\n",
      "Epoch 1500, Loss: 0.03517945110797882\n",
      "Epoch 2000, Loss: 0.03356402739882469\n",
      "Epoch 2500, Loss: 0.03178875520825386\n",
      "Epoch 3000, Loss: 0.031136400997638702\n",
      "Epoch 3500, Loss: 0.030238257721066475\n",
      "Epoch 4000, Loss: 0.030528105795383453\n",
      "Epoch 4500, Loss: 0.028247293084859848\n",
      "Epoch 4999, Loss: 0.03031175397336483\n",
      "Epoch 0, Loss: 0.26060840487480164\n",
      "Epoch 500, Loss: 0.03317580744624138\n",
      "Epoch 1000, Loss: 0.028652314096689224\n",
      "Epoch 1500, Loss: 0.028505239635705948\n",
      "Epoch 2000, Loss: 0.027043376117944717\n",
      "Epoch 2500, Loss: 0.02355603687465191\n",
      "Epoch 3000, Loss: 0.02747003547847271\n",
      "Epoch 3500, Loss: 0.028092002496123314\n",
      "Epoch 4000, Loss: 0.025493165478110313\n",
      "Epoch 4500, Loss: 0.02402191236615181\n",
      "Epoch 4999, Loss: 0.024549592286348343\n",
      "Epoch 0, Loss: 0.1660575419664383\n",
      "Epoch 500, Loss: 0.028323959559202194\n",
      "Epoch 1000, Loss: 0.024451371282339096\n",
      "Epoch 1500, Loss: 0.021587682887911797\n",
      "Epoch 2000, Loss: 0.022589121013879776\n",
      "Epoch 2500, Loss: 0.022874169051647186\n",
      "Epoch 3000, Loss: 0.022951021790504456\n",
      "Epoch 3500, Loss: 0.024130437523126602\n",
      "Epoch 4000, Loss: 0.02082209847867489\n",
      "Epoch 4500, Loss: 0.023230619728565216\n",
      "Epoch 4999, Loss: 0.023208007216453552\n",
      "--- Experiment 14/20 ---\n",
      "Epoch 0, Loss: 0.28727588057518005\n",
      "Epoch 500, Loss: 0.021982135251164436\n",
      "Epoch 1000, Loss: 0.017237147316336632\n",
      "Epoch 1500, Loss: 0.017775198444724083\n",
      "Epoch 2000, Loss: 0.018623068928718567\n",
      "Epoch 2500, Loss: 0.017566656693816185\n",
      "Epoch 3000, Loss: 0.017530081793665886\n",
      "Epoch 3500, Loss: 0.01663786545395851\n",
      "Epoch 4000, Loss: 0.01730043813586235\n",
      "Epoch 4500, Loss: 0.01694120652973652\n",
      "Epoch 4999, Loss: 0.01728806272149086\n",
      "Epoch 0, Loss: 0.13940425217151642\n",
      "Epoch 500, Loss: 0.03897697478532791\n",
      "Epoch 1000, Loss: 0.035741277039051056\n",
      "Epoch 1500, Loss: 0.033344436436891556\n",
      "Epoch 2000, Loss: 0.033418938517570496\n",
      "Epoch 2500, Loss: 0.031523995101451874\n",
      "Epoch 3000, Loss: 0.03306996077299118\n",
      "Epoch 3500, Loss: 0.03257947415113449\n",
      "Epoch 4000, Loss: 0.03056129440665245\n",
      "Epoch 4500, Loss: 0.029328955337405205\n",
      "Epoch 4999, Loss: 0.02852375991642475\n",
      "Epoch 0, Loss: 0.1661161184310913\n",
      "Epoch 500, Loss: 0.032895706593990326\n",
      "Epoch 1000, Loss: 0.028112024068832397\n",
      "Epoch 1500, Loss: 0.02712845802307129\n",
      "Epoch 2000, Loss: 0.02615131437778473\n",
      "Epoch 2500, Loss: 0.027320880442857742\n",
      "Epoch 3000, Loss: 0.025556208565831184\n",
      "Epoch 3500, Loss: 0.02581203170120716\n",
      "Epoch 4000, Loss: 0.024956967681646347\n",
      "Epoch 4500, Loss: 0.021506277844309807\n",
      "Epoch 4999, Loss: 0.024940185248851776\n",
      "Epoch 0, Loss: 0.2124287188053131\n",
      "Epoch 500, Loss: 0.029217950999736786\n",
      "Epoch 1000, Loss: 0.025041822344064713\n",
      "Epoch 1500, Loss: 0.022144321352243423\n",
      "Epoch 2000, Loss: 0.02401459962129593\n",
      "Epoch 2500, Loss: 0.02306715026497841\n",
      "Epoch 3000, Loss: 0.023149698972702026\n",
      "Epoch 3500, Loss: 0.02374914661049843\n",
      "Epoch 4000, Loss: 0.023402661085128784\n",
      "Epoch 4500, Loss: 0.02277328260242939\n",
      "Epoch 4999, Loss: 0.022362712770700455\n",
      "--- Experiment 15/20 ---\n",
      "Epoch 0, Loss: 0.14038453996181488\n",
      "Epoch 500, Loss: 0.020865337923169136\n",
      "Epoch 1000, Loss: 0.01933954283595085\n",
      "Epoch 1500, Loss: 0.018785078078508377\n",
      "Epoch 2000, Loss: 0.01839466020464897\n",
      "Epoch 2500, Loss: 0.019445810467004776\n",
      "Epoch 3000, Loss: 0.019080381840467453\n",
      "Epoch 3500, Loss: 0.018581140786409378\n",
      "Epoch 4000, Loss: 0.018095340579748154\n",
      "Epoch 4500, Loss: 0.017742831259965897\n",
      "Epoch 4999, Loss: 0.017053596675395966\n",
      "Epoch 0, Loss: 0.13214004039764404\n",
      "Epoch 500, Loss: 0.04326774924993515\n",
      "Epoch 1000, Loss: 0.034751199185848236\n",
      "Epoch 1500, Loss: 0.03267749026417732\n",
      "Epoch 2000, Loss: 0.03356774151325226\n",
      "Epoch 2500, Loss: 0.03371066972613335\n",
      "Epoch 3000, Loss: 0.03232187405228615\n",
      "Epoch 3500, Loss: 0.030596859753131866\n",
      "Epoch 4000, Loss: 0.029869332909584045\n",
      "Epoch 4500, Loss: 0.029512763023376465\n",
      "Epoch 4999, Loss: 0.028496209532022476\n",
      "Epoch 0, Loss: 0.17026475071907043\n",
      "Epoch 500, Loss: 0.03314604610204697\n",
      "Epoch 1000, Loss: 0.027151646092534065\n",
      "Epoch 1500, Loss: 0.02639658749103546\n",
      "Epoch 2000, Loss: 0.02607882209122181\n",
      "Epoch 2500, Loss: 0.024547971785068512\n",
      "Epoch 3000, Loss: 0.024005331099033356\n",
      "Epoch 3500, Loss: 0.02316407859325409\n",
      "Epoch 4000, Loss: 0.022027265280485153\n",
      "Epoch 4500, Loss: 0.02390822023153305\n",
      "Epoch 4999, Loss: 0.02451188862323761\n",
      "Epoch 0, Loss: 0.2486482709646225\n",
      "Epoch 500, Loss: 0.030422791838645935\n",
      "Epoch 1000, Loss: 0.025531195104122162\n",
      "Epoch 1500, Loss: 0.023564733564853668\n",
      "Epoch 2000, Loss: 0.022120842710137367\n",
      "Epoch 2500, Loss: 0.021020248532295227\n",
      "Epoch 3000, Loss: 0.0228057112544775\n",
      "Epoch 3500, Loss: 0.020426837727427483\n",
      "Epoch 4000, Loss: 0.022792156785726547\n",
      "Epoch 4500, Loss: 0.023014387115836143\n",
      "Epoch 4999, Loss: 0.020713606849312782\n",
      "--- Experiment 16/20 ---\n",
      "Epoch 0, Loss: 0.22899268567562103\n",
      "Epoch 500, Loss: 0.023497503250837326\n",
      "Epoch 1000, Loss: 0.021442929282784462\n",
      "Epoch 1500, Loss: 0.018470726907253265\n",
      "Epoch 2000, Loss: 0.01895471103489399\n",
      "Epoch 2500, Loss: 0.017535587772727013\n",
      "Epoch 3000, Loss: 0.017848093062639236\n",
      "Epoch 3500, Loss: 0.017047975212335587\n",
      "Epoch 4000, Loss: 0.0174453966319561\n",
      "Epoch 4500, Loss: 0.01622513309121132\n",
      "Epoch 4999, Loss: 0.015875061973929405\n",
      "Epoch 0, Loss: 0.20685821771621704\n",
      "Epoch 500, Loss: 0.039403870701789856\n",
      "Epoch 1000, Loss: 0.03605164960026741\n",
      "Epoch 1500, Loss: 0.033329255878925323\n",
      "Epoch 2000, Loss: 0.033343274146318436\n",
      "Epoch 2500, Loss: 0.03171050548553467\n",
      "Epoch 3000, Loss: 0.031546447426080704\n",
      "Epoch 3500, Loss: 0.0314427874982357\n",
      "Epoch 4000, Loss: 0.03139612078666687\n",
      "Epoch 4500, Loss: 0.03174600750207901\n",
      "Epoch 4999, Loss: 0.031101396307349205\n",
      "Epoch 0, Loss: 0.13030628859996796\n",
      "Epoch 500, Loss: 0.033612336963415146\n",
      "Epoch 1000, Loss: 0.027949221432209015\n",
      "Epoch 1500, Loss: 0.02482827752828598\n",
      "Epoch 2000, Loss: 0.02558450773358345\n",
      "Epoch 2500, Loss: 0.023273175582289696\n",
      "Epoch 3000, Loss: 0.02499815635383129\n",
      "Epoch 3500, Loss: 0.026134176179766655\n",
      "Epoch 4000, Loss: 0.025641443207859993\n",
      "Epoch 4500, Loss: 0.023244887590408325\n",
      "Epoch 4999, Loss: 0.02549239620566368\n",
      "Epoch 0, Loss: 0.1542656421661377\n",
      "Epoch 500, Loss: 0.027448086068034172\n",
      "Epoch 1000, Loss: 0.024638576433062553\n",
      "Epoch 1500, Loss: 0.022687502205371857\n",
      "Epoch 2000, Loss: 0.02267402410507202\n",
      "Epoch 2500, Loss: 0.022945722565054893\n",
      "Epoch 3000, Loss: 0.023621518164873123\n",
      "Epoch 3500, Loss: 0.022574568167328835\n",
      "Epoch 4000, Loss: 0.02263929881155491\n",
      "Epoch 4500, Loss: 0.0217101089656353\n",
      "Epoch 4999, Loss: 0.02157451957464218\n",
      "--- Experiment 17/20 ---\n",
      "Epoch 0, Loss: 0.2791840732097626\n",
      "Epoch 500, Loss: 0.024370292201638222\n",
      "Epoch 1000, Loss: 0.02096804417669773\n",
      "Epoch 1500, Loss: 0.019575610756874084\n",
      "Epoch 2000, Loss: 0.019516922533512115\n",
      "Epoch 2500, Loss: 0.01770837977528572\n",
      "Epoch 3000, Loss: 0.019337505102157593\n",
      "Epoch 3500, Loss: 0.019229937344789505\n",
      "Epoch 4000, Loss: 0.017390063032507896\n",
      "Epoch 4500, Loss: 0.016794050112366676\n",
      "Epoch 4999, Loss: 0.01751680113375187\n",
      "Epoch 0, Loss: 0.3286283314228058\n",
      "Epoch 500, Loss: 0.04167693480849266\n",
      "Epoch 1000, Loss: 0.03729164972901344\n",
      "Epoch 1500, Loss: 0.03637402504682541\n",
      "Epoch 2000, Loss: 0.03587351366877556\n",
      "Epoch 2500, Loss: 0.03200368210673332\n",
      "Epoch 3000, Loss: 0.03337043896317482\n",
      "Epoch 3500, Loss: 0.03279414027929306\n",
      "Epoch 4000, Loss: 0.03099406138062477\n",
      "Epoch 4500, Loss: 0.0317702516913414\n",
      "Epoch 4999, Loss: 0.029824167490005493\n",
      "Epoch 0, Loss: 0.21221710741519928\n",
      "Epoch 500, Loss: 0.03267371281981468\n",
      "Epoch 1000, Loss: 0.028406929224729538\n",
      "Epoch 1500, Loss: 0.027858173474669456\n",
      "Epoch 2000, Loss: 0.025371044874191284\n",
      "Epoch 2500, Loss: 0.023687733337283134\n",
      "Epoch 3000, Loss: 0.022782422602176666\n",
      "Epoch 3500, Loss: 0.024252846837043762\n",
      "Epoch 4000, Loss: 0.024093875661492348\n",
      "Epoch 4500, Loss: 0.02365676686167717\n",
      "Epoch 4999, Loss: 0.028843365609645844\n",
      "Epoch 0, Loss: 0.15464891493320465\n",
      "Epoch 500, Loss: 0.03014753945171833\n",
      "Epoch 1000, Loss: 0.027192842215299606\n",
      "Epoch 1500, Loss: 0.021485310047864914\n",
      "Epoch 2000, Loss: 0.023483814671635628\n",
      "Epoch 2500, Loss: 0.021921155974268913\n",
      "Epoch 3000, Loss: 0.02448941394686699\n",
      "Epoch 3500, Loss: 0.021741993725299835\n",
      "Epoch 4000, Loss: 0.02207908034324646\n",
      "Epoch 4500, Loss: 0.02399693801999092\n",
      "Epoch 4999, Loss: 0.024096261709928513\n",
      "--- Experiment 18/20 ---\n",
      "Epoch 0, Loss: 0.13883201777935028\n",
      "Epoch 500, Loss: 0.021680567413568497\n",
      "Epoch 1000, Loss: 0.02040368877351284\n",
      "Epoch 1500, Loss: 0.02125983126461506\n",
      "Epoch 2000, Loss: 0.018529269844293594\n",
      "Epoch 2500, Loss: 0.01863890141248703\n",
      "Epoch 3000, Loss: 0.018960600718855858\n",
      "Epoch 3500, Loss: 0.016603313386440277\n",
      "Epoch 4000, Loss: 0.018563153222203255\n",
      "Epoch 4500, Loss: 0.01679711416363716\n",
      "Epoch 4999, Loss: 0.01654810458421707\n",
      "Epoch 0, Loss: 0.20558446645736694\n",
      "Epoch 500, Loss: 0.03951634094119072\n",
      "Epoch 1000, Loss: 0.03780444711446762\n",
      "Epoch 1500, Loss: 0.04089426249265671\n",
      "Epoch 2000, Loss: 0.032967571169137955\n",
      "Epoch 2500, Loss: 0.03485352545976639\n",
      "Epoch 3000, Loss: 0.03540317341685295\n",
      "Epoch 3500, Loss: 0.03216487914323807\n",
      "Epoch 4000, Loss: 0.031558770686388016\n",
      "Epoch 4500, Loss: 0.028882529586553574\n",
      "Epoch 4999, Loss: 0.029094316065311432\n",
      "Epoch 0, Loss: 0.11643018573522568\n",
      "Epoch 500, Loss: 0.031390231102705\n",
      "Epoch 1000, Loss: 0.027225416153669357\n",
      "Epoch 1500, Loss: 0.02510637603700161\n",
      "Epoch 2000, Loss: 0.025747274979948997\n",
      "Epoch 2500, Loss: 0.02645314671099186\n",
      "Epoch 3000, Loss: 0.025818534195423126\n",
      "Epoch 3500, Loss: 0.02448412775993347\n",
      "Epoch 4000, Loss: 0.02433827519416809\n",
      "Epoch 4500, Loss: 0.024445168673992157\n",
      "Epoch 4999, Loss: 0.029976144433021545\n",
      "Epoch 0, Loss: 0.13030783832073212\n",
      "Epoch 500, Loss: 0.02889113873243332\n",
      "Epoch 1000, Loss: 0.024163059890270233\n",
      "Epoch 1500, Loss: 0.023739812895655632\n",
      "Epoch 2000, Loss: 0.025030754506587982\n",
      "Epoch 2500, Loss: 0.02279563993215561\n",
      "Epoch 3000, Loss: 0.023480135947465897\n",
      "Epoch 3500, Loss: 0.021605907008051872\n",
      "Epoch 4000, Loss: 0.0231486763805151\n",
      "Epoch 4500, Loss: 0.022004447877407074\n",
      "Epoch 4999, Loss: 0.020769156515598297\n",
      "--- Experiment 19/20 ---\n",
      "Epoch 0, Loss: 0.14512576162815094\n",
      "Epoch 500, Loss: 0.023223526775836945\n",
      "Epoch 1000, Loss: 0.02171674743294716\n",
      "Epoch 1500, Loss: 0.018016811460256577\n",
      "Epoch 2000, Loss: 0.01920541748404503\n",
      "Epoch 2500, Loss: 0.018009863793849945\n",
      "Epoch 3000, Loss: 0.01755547896027565\n",
      "Epoch 3500, Loss: 0.01770687662065029\n",
      "Epoch 4000, Loss: 0.018148008733987808\n",
      "Epoch 4500, Loss: 0.017304271459579468\n",
      "Epoch 4999, Loss: 0.01669144257903099\n",
      "Epoch 0, Loss: 0.15958888828754425\n",
      "Epoch 500, Loss: 0.03988177329301834\n",
      "Epoch 1000, Loss: 0.040057383477687836\n",
      "Epoch 1500, Loss: 0.035840947180986404\n",
      "Epoch 2000, Loss: 0.03581184521317482\n",
      "Epoch 2500, Loss: 0.034334782510995865\n",
      "Epoch 3000, Loss: 0.033956825733184814\n",
      "Epoch 3500, Loss: 0.03300902992486954\n",
      "Epoch 4000, Loss: 0.031183704733848572\n",
      "Epoch 4500, Loss: 0.030069615691900253\n",
      "Epoch 4999, Loss: 0.029746267944574356\n",
      "Epoch 0, Loss: 0.12631146609783173\n",
      "Epoch 500, Loss: 0.030320163816213608\n",
      "Epoch 1000, Loss: 0.026744067668914795\n",
      "Epoch 1500, Loss: 0.02648218721151352\n",
      "Epoch 2000, Loss: 0.0255572572350502\n",
      "Epoch 2500, Loss: 0.023463282734155655\n",
      "Epoch 3000, Loss: 0.023086894303560257\n",
      "Epoch 3500, Loss: 0.02475649118423462\n",
      "Epoch 4000, Loss: 0.023445431143045425\n",
      "Epoch 4500, Loss: 0.025318481028079987\n",
      "Epoch 4999, Loss: 0.026412446051836014\n",
      "Epoch 0, Loss: 0.13726957142353058\n",
      "Epoch 500, Loss: 0.028731178492307663\n",
      "Epoch 1000, Loss: 0.02492501586675644\n",
      "Epoch 1500, Loss: 0.02489401586353779\n",
      "Epoch 2000, Loss: 0.024834629148244858\n",
      "Epoch 2500, Loss: 0.024497658014297485\n",
      "Epoch 3000, Loss: 0.020807189866900444\n",
      "Epoch 3500, Loss: 0.022570203989744186\n",
      "Epoch 4000, Loss: 0.022404886782169342\n",
      "Epoch 4500, Loss: 0.02259780839085579\n",
      "Epoch 4999, Loss: 0.02261519245803356\n",
      "--- Experiment 20/20 ---\n",
      "Epoch 0, Loss: 0.20029915869235992\n",
      "Epoch 500, Loss: 0.021198157221078873\n",
      "Epoch 1000, Loss: 0.02031894028186798\n",
      "Epoch 1500, Loss: 0.018445147201418877\n",
      "Epoch 2000, Loss: 0.018328789621591568\n",
      "Epoch 2500, Loss: 0.016960088163614273\n",
      "Epoch 3000, Loss: 0.017105359584093094\n",
      "Epoch 3500, Loss: 0.017824392765760422\n",
      "Epoch 4000, Loss: 0.01839989796280861\n",
      "Epoch 4500, Loss: 0.018272951245307922\n",
      "Epoch 4999, Loss: 0.017591850832104683\n",
      "Epoch 0, Loss: 0.2692628800868988\n",
      "Epoch 500, Loss: 0.04414184018969536\n",
      "Epoch 1000, Loss: 0.0379701629281044\n",
      "Epoch 1500, Loss: 0.03489748015999794\n",
      "Epoch 2000, Loss: 0.03440200164914131\n",
      "Epoch 2500, Loss: 0.03327044099569321\n",
      "Epoch 3000, Loss: 0.03408273309469223\n",
      "Epoch 3500, Loss: 0.03301189839839935\n",
      "Epoch 4000, Loss: 0.03167106956243515\n",
      "Epoch 4500, Loss: 0.03356798365712166\n",
      "Epoch 4999, Loss: 0.033012475818395615\n",
      "Epoch 0, Loss: 0.12260285019874573\n",
      "Epoch 500, Loss: 0.030870871618390083\n",
      "Epoch 1000, Loss: 0.02762017771601677\n",
      "Epoch 1500, Loss: 0.02650189772248268\n",
      "Epoch 2000, Loss: 0.02616344578564167\n",
      "Epoch 2500, Loss: 0.025029679760336876\n",
      "Epoch 3000, Loss: 0.02536463923752308\n",
      "Epoch 3500, Loss: 0.025866102427244186\n",
      "Epoch 4000, Loss: 0.02473672479391098\n",
      "Epoch 4500, Loss: 0.02510090172290802\n",
      "Epoch 4999, Loss: 0.02269786037504673\n",
      "Epoch 0, Loss: 0.12623289227485657\n",
      "Epoch 500, Loss: 0.026765678077936172\n",
      "Epoch 1000, Loss: 0.021284379065036774\n",
      "Epoch 1500, Loss: 0.02521607093513012\n",
      "Epoch 2000, Loss: 0.02267685905098915\n",
      "Epoch 2500, Loss: 0.024634914472699165\n",
      "Epoch 3000, Loss: 0.022746702656149864\n",
      "Epoch 3500, Loss: 0.021698908880352974\n",
      "Epoch 4000, Loss: 0.022495664656162262\n",
      "Epoch 4500, Loss: 0.02054646983742714\n",
      "Epoch 4999, Loss: 0.024245118722319603\n",
      "Average Validation Loss (Base):   0.016998 ± 0.000273\n",
      "Average Validation Loss (Import): 0.025713 ± 0.000797\n",
      "Average Validation Loss (GP Out): 0.023413 ± 0.002294\n",
      "Average Validation Loss (GP Res): 0.020281 ± 0.001518\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "N_f = 1000\n",
    "L, T = 1.0, 1.0\n",
    "N_bc = 100\n",
    "epochs = 5000\n",
    "threshold = 0.001\n",
    "\n",
    "x_f, t_f = generate_collocation_points(N_f, L, T)\n",
    "x_val, t_val = generate_collocation_points(10000, L, T)\n",
    "\n",
    "initial_conditions = [\"sin\", \"step\", \"gaussian\"]\n",
    "\n",
    "# initial_conditions = [\"sin\"]\n",
    "boundary_conditions = [\"dirichlet\", \"neumann\", \"periodic\"]\n",
    "results_base = []\n",
    "results_import = []\n",
    "results_gauss = []\n",
    "results_gauss_res = []\n",
    "\n",
    "num_experiments = 20  # <-- adjust to desired experiment count\n",
    "\n",
    "for ic in initial_conditions:\n",
    "    for bc in boundary_conditions:\n",
    "        print(f\"\\n==== Running experiments for IC: {ic}, BC: {bc} ====\")\n",
    "\n",
    "        val_losses_base = []\n",
    "        val_losses_import = []\n",
    "        val_losses_gauss = []\n",
    "        val_losses_gauss_res = []\n",
    "\n",
    "        threshold_base_val = []\n",
    "        threshold_import_val = []\n",
    "        threshold_gauss_val = []\n",
    "        threshold_gauss_val_res = []\n",
    "\n",
    "        base_all_scores = []\n",
    "        import_all_scores = []\n",
    "        output_all_scores = []\n",
    "        res_all_scores = []\n",
    "\n",
    "        x_bc = torch.linspace(0, L, N_bc).view(-1, 1).to(device)\n",
    "        t_bc = torch.zeros_like(x_bc).to(device)\n",
    "        u_bc = initial_condition(x_bc, ic)\n",
    "\n",
    "        for exp in range(num_experiments):\n",
    "            print(f\"--- Experiment {exp+1}/{num_experiments} ---\")\n",
    "\n",
    "            #Base\n",
    "            model_base = PINN(layers).to(device)\n",
    "            optimizer = optim.Adam(model_base.parameters(), lr=1e-3)\n",
    "            thresh_base, base_scores = train(model_base, optimizer, N_f,x_f, t_f, x_val, t_val, x_bc, t_bc, u_bc, epochs=epochs, threshold=threshold)\n",
    "            val_losses_base.append(val_loss(model_base, x_val, t_val, x_bc, t_bc, u_bc).item())\n",
    "            threshold_base_val.append(thresh_base)\n",
    "            base_all_scores.append(base_scores)\n",
    "            del model_base, optimizer\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            #Importance\n",
    "            model_import = PINN(layers).to(device)\n",
    "            optimizer = optim.Adam(model_import.parameters(), lr=1e-3)\n",
    "            thresh_import, import_scores = train_import(model_import, optimizer, N_f, x_f, t_f, x_val, t_val, x_bc, t_bc, u_bc, epochs=epochs, threshold=threshold)\n",
    "            val_losses_import.append(val_loss(model_import, x_val, t_val, x_bc, t_bc, u_bc).item())\n",
    "            threshold_import_val.append(thresh_import)\n",
    "            import_all_scores.append(import_scores)\n",
    "            del model_import, optimizer\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            #GP Output\n",
    "            model_GP = PINN(layers).to(device)\n",
    "            optimizer = optim.Adam(model_GP.parameters(), lr=1e-3)\n",
    "            thresh_gp, output_scores = train_GP(model_GP, optimizer, N_f, x_f, t_f, x_val, t_val, x_bc, t_bc, u_bc, epochs=epochs, resample_every=100, threshold=threshold)\n",
    "            val_losses_gauss.append(val_loss(model_GP, x_val, t_val, x_bc, t_bc, u_bc).item())\n",
    "            threshold_gauss_val.append(thresh_gp)\n",
    "            output_all_scores.append(output_scores)\n",
    "            del model_GP, optimizer\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            #GP Residual\n",
    "            model_GP_res = PINN(layers).to(device)\n",
    "            optimizer = optim.Adam(model_GP_res.parameters(), lr=1e-3)\n",
    "            thresh_gp_res, res_scores = train_GP_res(model_GP_res, optimizer, N_f, x_f, t_f, x_val, t_val, x_bc, t_bc, u_bc, epochs=epochs, resample_every=100, threshold=threshold)\n",
    "            val_losses_gauss_res.append(val_loss(model_GP_res, x_val, t_val, x_bc, t_bc, u_bc).item())\n",
    "            threshold_gauss_val_res.append(thresh_gp_res)\n",
    "            res_all_scores.append(res_scores)\n",
    "            del model_GP_res, optimizer\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        avg_base, std_base = np.mean(val_losses_base), np.std(val_losses_base)\n",
    "        avg_import, std_import = np.mean(val_losses_import), np.std(val_losses_import)\n",
    "        avg_gauss, std_gauss = np.mean(val_losses_gauss), np.std(val_losses_gauss)\n",
    "        avg_gauss_res, std_gauss_res = np.mean(val_losses_gauss_res), np.std(val_losses_gauss_res)\n",
    "\n",
    "        avg_t_base, std_t_base = np.mean(threshold_base_val), np.std(threshold_base_val)\n",
    "        avg_t_import, std_t_import = np.mean(threshold_import_val), np.std(threshold_import_val)\n",
    "        avg_t_gauss, std_t_gauss = np.mean(threshold_gauss_val), np.std(threshold_gauss_val)\n",
    "        avg_t_gauss_res, std_t_gauss_res = np.mean(threshold_gauss_val_res), np.std(threshold_gauss_val_res)\n",
    "\n",
    "        print(f\"Average Validation Loss (Base):   {avg_base:.6f} ± {std_base:.6f}\")\n",
    "        print(f\"Average Validation Loss (Import): {avg_import:.6f} ± {std_import:.6f}\")\n",
    "        print(f\"Average Validation Loss (GP Out): {avg_gauss:.6f} ± {std_gauss:.6f}\")\n",
    "        print(f\"Average Validation Loss (GP Res): {avg_gauss_res:.6f} ± {std_gauss_res:.6f}\")\n",
    "\n",
    "        results_base.append([ic, bc, avg_base, std_base, avg_t_base, std_t_base])\n",
    "        results_import.append([ic, bc, avg_import, std_import, avg_t_import, std_t_import])\n",
    "        results_gauss.append([ic, bc, avg_gauss, std_gauss, avg_t_gauss, std_t_gauss])\n",
    "        results_gauss_res.append([ic, bc, avg_gauss_res, std_gauss_res, avg_t_gauss_res, std_t_gauss_res])\n",
    "\n",
    "        # === Compute Epoch-wise Means and STDs ===\n",
    "        # epochs_range = [i * 500 for i in range(len(base_all_scores[0]))]\n",
    "\n",
    "        # base_all_scores_np = np.stack(base_all_scores)\n",
    "        # import_all_scores_np = np.stack(import_all_scores)\n",
    "        # output_all_scores_np = np.stack(output_all_scores)\n",
    "        # res_all_scores_np = np.stack(res_all_scores)\n",
    "\n",
    "        # base_mean_curve = np.mean(base_all_scores_np, axis=0)\n",
    "        # base_std_curve = np.std(base_all_scores_np, axis=0)\n",
    "\n",
    "        # import_mean_curve = np.mean(import_all_scores_np, axis=0)\n",
    "        # import_std_curve = np.std(import_all_scores_np, axis=0)\n",
    "\n",
    "        # output_mean_curve = np.mean(output_all_scores_np, axis=0)\n",
    "        # output_std_curve = np.std(output_all_scores_np, axis=0)\n",
    "\n",
    "        # res_mean_curve = np.mean(res_all_scores_np, axis=0)\n",
    "        # res_std_curve = np.std(res_all_scores_np, axis=0)\n",
    "\n",
    "        # # === Plot Mean ± Std ===\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # plt.plot(epochs_range, base_mean_curve, label='Base PINN')\n",
    "        # plt.fill_between(epochs_range, base_mean_curve - base_std_curve, base_mean_curve + base_std_curve, alpha=0.2)\n",
    "\n",
    "        # plt.plot(epochs_range, import_mean_curve, label='Importance Sampling')\n",
    "        # plt.fill_between(epochs_range, import_mean_curve - import_std_curve, import_mean_curve + import_std_curve, alpha=0.2)\n",
    "\n",
    "        # # plt.plot(epochs_range, output_mean_curve, label='GP Output')\n",
    "        # # plt.fill_between(epochs_range, output_mean_curve - output_std_curve, output_mean_curve + output_std_curve, alpha=0.2)\n",
    "\n",
    "        # plt.plot(epochs_range, res_mean_curve, label='GP Residual')\n",
    "        # plt.fill_between(epochs_range, res_mean_curve - res_std_curve, res_mean_curve + res_std_curve, alpha=0.2)\n",
    "\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.ylabel('Mean Validation Loss')\n",
    "        # plt.title(f'Mean Validation Loss ± Std vs Epochs\\nIC: {ic}, BC: {bc}')\n",
    "        # plt.legend()\n",
    "        # plt.grid(True)\n",
    "        # plt.tight_layout()\n",
    "\n",
    "        # # Save figure\n",
    "        # # filename = f\"/mnt/data/val_loss_plot_ic_{ic}_bc_{bc}.png\"\n",
    "        # # plt.savefig(filename)\n",
    "        # plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8eecd5f-979e-45da-aaa5-d5d399544825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrames\n",
    "df_base = pd.DataFrame(results_base, columns=[\"Initial Condition\", \"Boundary Condition\", \"Avg Validation Loss\", \"Std Validation Loss\", \"Avg Threshold\",\"Std Threshold\"])\n",
    "df_import = pd.DataFrame(results_import, columns=[\"Initial Condition\", \"Boundary Condition\", \"Avg Validation Loss\", \"Std Validation Loss\", \"Avg Threshold\",\"Std Threshold\"])\n",
    "df_gauss = pd.DataFrame(results_gauss, columns=[\"Initial Condition\", \"Boundary Condition\", \"Avg Validation Loss\", \"Std Validation Loss\", \"Avg Threshold\",\"Std Threshold\"])\n",
    "df_gauss_res = pd.DataFrame(results_gauss_res, columns=[\"Initial Condition\", \"Boundary Condition\", \"Avg Validation Loss\", \"Std Validation Loss\", \"Avg Threshold\",\"Std Threshold\"])\n",
    "# # Save to CSV\n",
    "df_base.to_csv(\"Results_Experiments/base_05_27_2025.csv\", index=False)\n",
    "df_import.to_csv(\"Results_Experiments/import_05_27_2025.csv\", index=False)\n",
    "df_gauss.to_csv(\"Results_Experiments/GP_05_27_2025.csv\", index=False)\n",
    "df_gauss_res.to_csv(\"Results_Experiments/GP_final_05_27_2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e5d282c-93f8-48af-b14d-60cb271dd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "df_base = pd.read_csv(\"Results_Experiments/base_05_27_2025.csv\")\n",
    "df_import = pd.read_csv(\"Results_Experiments/import_05_27_2025.csv\")\n",
    "df_gauss = pd.read_csv(\"Results_Experiments/GP_05_27_2025.csv\")\n",
    "df_gauss_res = pd.read_csv(\"Results_Experiments/GP_final_05_27_2025.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ed2879b-445a-43ca-ae2f-b88a1a929d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initial Condition</th>\n",
       "      <th>Boundary Condition</th>\n",
       "      <th>Avg Validation Loss</th>\n",
       "      <th>Std Validation Loss</th>\n",
       "      <th>Avg Threshold</th>\n",
       "      <th>Std Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sin</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>3124.7</td>\n",
       "      <td>780.638847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sin</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>3197.8</td>\n",
       "      <td>727.776587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sin</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>3273.0</td>\n",
       "      <td>880.022897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>step</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.047884</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>step</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.048095</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>step</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.047940</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.017101</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.017061</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.016998</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Initial Condition Boundary Condition  Avg Validation Loss  \\\n",
       "0               sin          dirichlet             0.000650   \n",
       "1               sin            neumann             0.000644   \n",
       "2               sin           periodic             0.000660   \n",
       "3              step          dirichlet             0.047884   \n",
       "4              step            neumann             0.048095   \n",
       "5              step           periodic             0.047940   \n",
       "6          gaussian          dirichlet             0.017101   \n",
       "7          gaussian            neumann             0.017061   \n",
       "8          gaussian           periodic             0.016998   \n",
       "\n",
       "   Std Validation Loss  Avg Threshold  Std Threshold  \n",
       "0             0.000215         3124.7     780.638847  \n",
       "1             0.000199         3197.8     727.776587  \n",
       "2             0.000197         3273.0     880.022897  \n",
       "3             0.000758         5000.0       0.000000  \n",
       "4             0.000735         5000.0       0.000000  \n",
       "5             0.000614         5000.0       0.000000  \n",
       "6             0.000478         5000.0       0.000000  \n",
       "7             0.000441         5000.0       0.000000  \n",
       "8             0.000273         5000.0       0.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c1c72c0-424b-439d-b2f9-90133e190451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initial Condition</th>\n",
       "      <th>Boundary Condition</th>\n",
       "      <th>Avg Validation Loss</th>\n",
       "      <th>Std Validation Loss</th>\n",
       "      <th>Avg Threshold</th>\n",
       "      <th>Std Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sin</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>4425.5</td>\n",
       "      <td>752.103816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sin</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>4496.2</td>\n",
       "      <td>563.973545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sin</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>4626.7</td>\n",
       "      <td>431.268953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>step</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.063162</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>step</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.063659</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>step</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.063368</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.025574</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.025713</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Initial Condition Boundary Condition  Avg Validation Loss  \\\n",
       "0               sin          dirichlet             0.001443   \n",
       "1               sin            neumann             0.001462   \n",
       "2               sin           periodic             0.001437   \n",
       "3              step          dirichlet             0.063162   \n",
       "4              step            neumann             0.063659   \n",
       "5              step           periodic             0.063368   \n",
       "6          gaussian          dirichlet             0.025441   \n",
       "7          gaussian            neumann             0.025574   \n",
       "8          gaussian           periodic             0.025713   \n",
       "\n",
       "   Std Validation Loss  Avg Threshold  Std Threshold  \n",
       "0             0.000445         4425.5     752.103816  \n",
       "1             0.000426         4496.2     563.973545  \n",
       "2             0.000415         4626.7     431.268953  \n",
       "3             0.001924         5000.0       0.000000  \n",
       "4             0.001540         5000.0       0.000000  \n",
       "5             0.001567         5000.0       0.000000  \n",
       "6             0.000819         5000.0       0.000000  \n",
       "7             0.001264         5000.0       0.000000  \n",
       "8             0.000797         5000.0       0.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47d59f07-3ad6-4bfe-af11-d851d51b04b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initial Condition</th>\n",
       "      <th>Boundary Condition</th>\n",
       "      <th>Avg Validation Loss</th>\n",
       "      <th>Std Validation Loss</th>\n",
       "      <th>Avg Threshold</th>\n",
       "      <th>Std Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sin</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.019498</td>\n",
       "      <td>0.014795</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sin</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.016104</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sin</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.026916</td>\n",
       "      <td>0.020239</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>step</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.057072</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>step</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>step</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.055806</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.022224</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.023413</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Initial Condition Boundary Condition  Avg Validation Loss  \\\n",
       "0               sin          dirichlet             0.019498   \n",
       "1               sin            neumann             0.016104   \n",
       "2               sin           periodic             0.026916   \n",
       "3              step          dirichlet             0.057072   \n",
       "4              step            neumann             0.056300   \n",
       "5              step           periodic             0.055806   \n",
       "6          gaussian          dirichlet             0.022224   \n",
       "7          gaussian            neumann             0.022068   \n",
       "8          gaussian           periodic             0.023413   \n",
       "\n",
       "   Std Validation Loss  Avg Threshold  Std Threshold  \n",
       "0             0.014795         5000.0            0.0  \n",
       "1             0.010140         5000.0            0.0  \n",
       "2             0.020239         5000.0            0.0  \n",
       "3             0.005609         5000.0            0.0  \n",
       "4             0.001599         5000.0            0.0  \n",
       "5             0.002336         5000.0            0.0  \n",
       "6             0.001295         5000.0            0.0  \n",
       "7             0.001193         5000.0            0.0  \n",
       "8             0.002294         5000.0            0.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36fd38ac-7968-4bf4-803d-ca0d3f15dc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initial Condition</th>\n",
       "      <th>Boundary Condition</th>\n",
       "      <th>Avg Validation Loss</th>\n",
       "      <th>Std Validation Loss</th>\n",
       "      <th>Avg Threshold</th>\n",
       "      <th>Std Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sin</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2002.95</td>\n",
       "      <td>415.757198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sin</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>2183.90</td>\n",
       "      <td>565.794300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sin</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>2023.20</td>\n",
       "      <td>488.306215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>step</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.053176</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>step</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>step</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.056691</td>\n",
       "      <td>0.006769</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>dirichlet</td>\n",
       "      <td>0.020037</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>neumann</td>\n",
       "      <td>0.020674</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>periodic</td>\n",
       "      <td>0.020281</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Initial Condition Boundary Condition  Avg Validation Loss  \\\n",
       "0               sin          dirichlet             0.000367   \n",
       "1               sin            neumann             0.000405   \n",
       "2               sin           periodic             0.000373   \n",
       "3              step          dirichlet             0.053176   \n",
       "4              step            neumann             0.054435   \n",
       "5              step           periodic             0.056691   \n",
       "6          gaussian          dirichlet             0.020037   \n",
       "7          gaussian            neumann             0.020674   \n",
       "8          gaussian           periodic             0.020281   \n",
       "\n",
       "   Std Validation Loss  Avg Threshold  Std Threshold  \n",
       "0             0.000089        2002.95     415.757198  \n",
       "1             0.000145        2183.90     565.794300  \n",
       "2             0.000090        2023.20     488.306215  \n",
       "3             0.002083        5000.00       0.000000  \n",
       "4             0.004868        5000.00       0.000000  \n",
       "5             0.006769        5000.00       0.000000  \n",
       "6             0.001869        5000.00       0.000000  \n",
       "7             0.001464        5000.00       0.000000  \n",
       "8             0.001518        5000.00       0.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gauss_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
