{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421e303-15ed-4e79-befe-89f2d9e3430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import qmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b40f8e-796c-4f9b-b12c-bf274a877370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the Berger Viscous Equation parameters\n",
    "c = 1.0    # Wave speed\n",
    "mu = 0.1   # Viscosity coefficient\n",
    "lam = 1.0  # Nonlinearity coefficient\n",
    "\n",
    "# Define the neural network\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(*[\n",
    "            nn.Sequential(nn.Linear(layers[i], layers[i+1]), nn.Tanh())\n",
    "            for i in range(len(layers)-2)\n",
    "        ] + [nn.Linear(layers[-2], layers[-1])])\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inputs = torch.cat((x, t), dim=1)\n",
    "        return self.net(inputs)\n",
    "\n",
    "def generate_collocation_points(N_f, L=1.0, T=1.0):\n",
    "    # Use Latin Hypercube Sampling for better distribution\n",
    "    sampler = qmc.LatinHypercube(d=2)  # 2D (x, t) space\n",
    "    sample = sampler.random(N_f)  # Generate N_f samples in [0, 1]^2\n",
    "\n",
    "    # Scale samples: x in [-L, L], t in [0, T]\n",
    "    x_f = torch.tensor((sample[:, 0] * 2 - 1) * L, dtype=torch.float32).reshape(-1, 1)\n",
    "    t_f = torch.tensor(sample[:, 1] * T, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    return x_f.to(device), t_f.to(device)\n",
    "def compute_pde_residual(model, x, t):\n",
    "    x.requires_grad = True\n",
    "    t.requires_grad = True\n",
    "\n",
    "    u = model(x, t)  # Predict u(x, t)\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    # Inviscid Burgers' equation: u_t + u * u_x = 0\n",
    "    f = u_t + u * u_x\n",
    "    return f\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "def loss_function(model, x_f, t_f, x_bc, t_bc, u_bc):\n",
    "    f_residual = compute_pde_residual(model, x_f, t_f)\n",
    "    loss_pde = torch.mean(f_residual**2)\n",
    "\n",
    "    u_pred_bc = model(x_bc, t_bc)\n",
    "    loss_bc = torch.mean((u_pred_bc - u_bc)**2)\n",
    "\n",
    "    return loss_pde + loss_bc\n",
    "\n",
    "# Training loop\n",
    "def train(model, optimizer, x_f, t_f, valX, valT, x_bc, t_bc, u_bc, epochs=5000,threshold = 0.001):\n",
    "    val_scores = []\n",
    "    thresh_e = epochs\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = loss_function(model, valX, valT, x_bc, t_bc, u_bc)\n",
    "\n",
    "        if loss_val.item() < threshold and thresh_e >= epochs:\n",
    "            print(\"Threshold reach at:\",epoch)\n",
    "            print(\"Val loss:\",loss_val)\n",
    "            thresh_e = epoch\n",
    "\n",
    "        if epoch % 500 == 0 or epoch == epochs-1:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            val_scores.append(loss_val.item())\n",
    "\n",
    "    return thresh_e, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a43de-0523-49f7-9ae4-96cf18e0d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_points_import(model, N_f, L=1.0, T=1.0):\n",
    "    x_f = (torch.rand(N_f, 1, device=device, requires_grad=True) * 2 - 1) * L  # x in [-L, L]\n",
    "    t_f = torch.rand(N_f, 1, device=device, requires_grad=True) * T  # t in [0, T]\n",
    "    \n",
    "    if model is not None:  # Perform importance sampling\n",
    "        residuals = compute_pde_residual(model, x_f, t_f).detach()\n",
    "        probabilities = residuals.abs() / torch.sum(residuals.abs())\n",
    "        sampled_indices = torch.multinomial(probabilities.view(-1), N_f, replacement=True)\n",
    "        x_f, t_f = x_f[sampled_indices], t_f[sampled_indices]\n",
    "    \n",
    "    return x_f, t_f\n",
    "\n",
    "def train_import(model, optimizer, N_f, x_f,t_f,valX ,valT ,x_bc, t_bc, u_bc, epochs=10000, resample_every=5000,threshold = 0.001):\n",
    "\n",
    "    val_scores = []\n",
    "    thresh_e = epochs\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % resample_every == 0 and epoch >=500:\n",
    "            x_f, t_f = gen_points_import(model, N_f)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = loss_function(model, valX, valT, x_bc, t_bc, u_bc)\n",
    "\n",
    "        if loss_val.item() < threshold and thresh_e >= epochs:\n",
    "            print(\"Threshold reach at:\",epoch)\n",
    "            print(\"Val loss:\",loss_val)\n",
    "            thresh_e = epoch\n",
    "        if epoch % 500 == 0 or epoch == epochs-1:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            val_scores.append(loss_val.item())\n",
    "\n",
    "    return thresh_e, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37484f6d-556d-4dcc-bb69-a08999975634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Process Model for Importance Sampling\n",
    "class ResidualGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ResidualGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "    gpytorch.kernels.MaternKernel(nu=1.5)\n",
    ")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f663f-0f93-4830-b833-771880b76128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def generate_collocation_points_with_gp(model, N_f, x_f, t_f, x_bc=None, t_bc=None, u_bc=None, \n",
    "                                        L=1.0, T=1.0, alpha=0.5, fraction_gp=0.5, residual_thresh=1e-3):\n",
    "    device = x_f.device\n",
    "\n",
    "    # === Step 1: Prepare GP training data ===\n",
    "    x_train = x_f\n",
    "    t_train = t_f\n",
    "    xt_train = torch.cat([x_train, t_train], dim=1).detach()\n",
    "\n",
    "    if model is not None:\n",
    "        with torch.no_grad():\n",
    "            u_train = model(x_train, t_train).detach().view(-1)\n",
    "            xt_all = xt_train\n",
    "            u_all = u_train\n",
    "\n",
    "            if x_bc is not None and t_bc is not None and u_bc is not None:\n",
    "                xt_bc = torch.cat([x_bc, t_bc], dim=1).detach()\n",
    "                u_bc = u_bc.detach().view(-1)\n",
    "                xt_all = torch.cat([xt_all, xt_bc], dim=0)\n",
    "                u_all = torch.cat([u_all, u_bc], dim=0)\n",
    "\n",
    "        # === Train GP ===\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        gp_model = ResidualGP(xt_all, u_all, likelihood).to(device)\n",
    "\n",
    "        gp_model.train()\n",
    "        likelihood.train()\n",
    "        optimizer = torch.optim.Adam(gp_model.parameters(), lr=0.001)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
    "\n",
    "        for _ in range(1000):\n",
    "            optimizer.zero_grad()\n",
    "            output = gp_model(xt_all)\n",
    "            loss = -mll(output, u_all)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        gp_model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # === Step 2: Generate candidate points ===\n",
    "        sampler = qmc.LatinHypercube(d=2)\n",
    "        sample = sampler.random(10 * N_f)\n",
    "        x_cand = torch.tensor(sample[:, 0] * L, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "        t_cand = torch.tensor(sample[:, 1] * T, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "        xt_cand = torch.cat([x_cand, t_cand], dim=1)\n",
    "\n",
    "        # === Step 3: Sample from GP posterior ===\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            dist = gp_model(xt_cand)\n",
    "            gp_samples = dist.rsample(torch.Size([1])) # shape: [1, N_cand]\n",
    "            gp_sample_abs = gp_samples.squeeze(0).abs().detach()  # [N_cand]\n",
    "\n",
    "        # === Step 4: Compute PDE residuals ===\n",
    "        x_cand.requires_grad_()\n",
    "        t_cand.requires_grad_()\n",
    "        residual = compute_pde_residual(model, x_cand, t_cand).detach().abs().view(-1)\n",
    "\n",
    "        # === Step 5: Normalize and combine scores ===\n",
    "        residual = torch.where(residual < residual_thresh, torch.tensor(0.0, device=device), residual)\n",
    "\n",
    "        sample_score = gp_sample_abs / (gp_sample_abs.sum() + 1e-8)\n",
    "        residual_score = residual / (residual.sum() + 1e-8)\n",
    "\n",
    "        sampling_score = alpha * sample_score + (1 - alpha) * residual_score\n",
    "        sampling_score = sampling_score / (sampling_score.sum() + 1e-8)\n",
    "\n",
    "        # === Step 6: Hybrid sampling ===\n",
    "        N_gp = int(fraction_gp * N_f)\n",
    "        N_rand = N_f - N_gp\n",
    "\n",
    "        sampled_indices_gp = torch.multinomial(sampling_score, N_gp, replacement=False)\n",
    "        sampled_indices_rand = torch.randint(0, len(x_cand), (N_rand,), device=device)\n",
    "        sampled_indices = torch.cat([sampled_indices_gp, sampled_indices_rand], dim=0)\n",
    "\n",
    "        x_f_new = x_cand[sampled_indices].detach().clone().requires_grad_()\n",
    "        t_f_new = t_cand[sampled_indices].detach().clone().requires_grad_()\n",
    "        uncertainty_top = gp_sample_abs[sampled_indices].detach().cpu()\n",
    "\n",
    "    else:\n",
    "        x_f_new = x_f.clone().detach().requires_grad_()\n",
    "        t_f_new = t_f.clone().detach().requires_grad_()\n",
    "        uncertainty_top = None\n",
    "        gp_model = None\n",
    "\n",
    "    return x_f_new, t_f_new, uncertainty_top, gp_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7884cc-8df1-4190-9460-107c83af5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GP(model, optimizer, N_f, x_f,t_f,valX, valT,x_bc, t_bc, u_bc, epochs=5000, resample_every=500,threshold = 0.001):\n",
    "    thresh_e = epochs\n",
    "    val_scores = []\n",
    "    for epoch in range(epochs):\n",
    "        # if epoch % resample_every == 0 and epoch >=500:\n",
    "        #     x_f, t_f = gen_points_import(model, N_f)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = loss_function(model, valX, valT, x_bc, t_bc, u_bc)\n",
    "\n",
    "        if loss_val.item() < threshold and thresh_e >= epochs:\n",
    "            print(\"Threshold reach at:\",epoch)\n",
    "            print(\"Val loss:\",loss_val)\n",
    "            thresh_e = epoch\n",
    "\n",
    "        if epoch % 500 == 0 or epoch == epochs-1:\n",
    "            # x_f,t_f = fit_GP(x_f,t_f)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            val_scores.append(loss_val.item())\n",
    "            # x_uncertain, t_uncertain,x,g = generate_collocation_points_with_gp(model,N_f,x_f, t_f) \n",
    "        if epoch % resample_every == 0 and epoch > 0 :\n",
    "            x_uncertain, t_uncertain,uncertainties,gp_model = generate_collocation_points_with_gp(model,N_f,x_f, t_f,x_bc,t_bc,u_bc)\n",
    "\n",
    "            x_f = x_uncertain\n",
    "            t_f = t_uncertain\n",
    "\n",
    "    return thresh_e, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f874c-9b12-44f8-a6a6-6d27357837ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def generate_collocation_points_with_gp_res(model, N_f, x_f, t_f, x_bc=None, t_bc=None, u_bc=None, \n",
    "                                            L=2.0, T=1.0, alpha=0.5, fraction_gp=0.5, residual_thresh=1e-3):\n",
    "\n",
    "    device = x_f.device\n",
    "\n",
    "    x_train = x_f\n",
    "    t_train = t_f\n",
    "    xt_train = torch.cat([x_train, t_train], dim=1).detach()\n",
    "\n",
    "    if model is not None:\n",
    "        # ✅ Compute residuals (no torch.no_grad here)\n",
    "        residual_train = compute_pde_residual(model, x_train.requires_grad_(), t_train.requires_grad_()).detach().view(-1)\n",
    "\n",
    "        xt_all = xt_train\n",
    "        residual_all = residual_train\n",
    "\n",
    "        if x_bc is not None and t_bc is not None and u_bc is not None:\n",
    "            xt_bc = torch.cat([x_bc, t_bc], dim=1).detach()\n",
    "            residual_bc = compute_pde_residual(model, x_bc.requires_grad_(), t_bc.requires_grad_()).detach().view(-1)\n",
    "\n",
    "            xt_all = torch.cat([xt_all, xt_bc], dim=0)\n",
    "            residual_all = torch.cat([residual_all, residual_bc], dim=0)\n",
    "\n",
    "        # === Train GP on residuals ===\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        gp_model = ResidualGP(xt_all, residual_all, likelihood).to(device)\n",
    "\n",
    "        gp_model.train()\n",
    "        likelihood.train()\n",
    "        optimizer = torch.optim.Adam(gp_model.parameters(), lr=0.001)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
    "\n",
    "        for _ in range(1000):\n",
    "            optimizer.zero_grad()\n",
    "            output = gp_model(xt_all)\n",
    "            loss = -mll(output, residual_all)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        gp_model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # === Step 2: Candidate points ===\n",
    "        sampler = qmc.LatinHypercube(d=2)\n",
    "        sample = sampler.random(10 * N_f)\n",
    "\n",
    "        # x ∈ [-L/2, L/2], t ∈ [0, T]\n",
    "        x_cand = torch.tensor((sample[:, 0] * 2 - 1) * (L/2), dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "        t_cand = torch.tensor(sample[:, 1] * T, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "        xt_cand = torch.cat([x_cand, t_cand], dim=1)\n",
    "\n",
    "        # === Step 3 (Modified): Sample from GP posterior ===\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            dist = gp_model(xt_cand)\n",
    "            gp_samples = dist.rsample(torch.Size([1]))  # [1, N_cand]\n",
    "            gp_sample_abs = gp_samples.squeeze(0).abs().detach()  # [N_cand]\n",
    "  # [N_cand]\n",
    "\n",
    "        # === Step 4: Compute PDE residuals at candidate points\n",
    "        x_cand.requires_grad_()\n",
    "        t_cand.requires_grad_()\n",
    "        residual = compute_pde_residual(model, x_cand, t_cand).detach().abs().view(-1)\n",
    "\n",
    "        # === Step 5: Normalize and threshold ===\n",
    "        residual = torch.where(residual < residual_thresh, torch.tensor(0.0, device=device), residual)\n",
    "\n",
    "        sample_score = gp_sample_abs / (gp_sample_abs.sum() + 1e-8)\n",
    "        residual_score = residual / (residual.sum() + 1e-8)\n",
    "\n",
    "        sampling_score = alpha * sample_score + (1 - alpha) * residual_score\n",
    "        sampling_score = sampling_score / (sampling_score.sum() + 1e-8)\n",
    "\n",
    "        # === Step 6: Hybrid sampling ===\n",
    "        N_gp = int(fraction_gp * N_f)\n",
    "        N_rand = N_f - N_gp\n",
    "\n",
    "        sampled_indices_gp = torch.multinomial(sampling_score, N_gp, replacement=False)\n",
    "        sampled_indices_rand = torch.randint(0, len(x_cand), (N_rand,), device=device)\n",
    "        sampled_indices = torch.cat([sampled_indices_gp, sampled_indices_rand], dim=0)\n",
    "\n",
    "        x_f_new = x_cand[sampled_indices].detach().clone().requires_grad_()\n",
    "        t_f_new = t_cand[sampled_indices].detach().clone().requires_grad_()\n",
    "        uncertainty_top = gp_sample_abs[sampled_indices].detach().cpu()\n",
    "\n",
    "    else:\n",
    "        x_f_new = x_f.clone().detach().requires_grad_()\n",
    "        t_f_new = t_f.clone().detach().requires_grad_()\n",
    "        uncertainty_top = None\n",
    "        gp_model = None\n",
    "\n",
    "    return x_f_new, t_f_new, uncertainty_top, gp_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db3c46-3c5a-4bc4-a7d7-81558bf68576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GP_res(model, optimizer, N_f, x_f,t_f,valX, valT,x_bc, t_bc, u_bc, epochs=5000, resample_every=500,threshold = 0.001):\n",
    "    val_scores = []\n",
    "    thresh_e = epochs\n",
    "    for epoch in range(epochs):\n",
    "        # if epoch % resample_every == 0 and epoch >=500:\n",
    "        #     x_f, t_f = gen_points_import(model, N_f)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, x_f, t_f, x_bc, t_bc, u_bc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = loss_function(model, valX, valT, x_bc, t_bc, u_bc)\n",
    "\n",
    "        if loss_val.item() < threshold and thresh_e >= epochs:\n",
    "            print(\"Threshold reach at:\",epoch)\n",
    "            print(\"Val loss:\",loss_val)\n",
    "            thresh_e = epoch\n",
    "\n",
    "        if epoch % 500 == 0 or epoch == epochs-1:\n",
    "            # x_f,t_f = fit_GP(x_f,t_f)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            val_scores.append(loss_val.item())\n",
    "            # x_uncertain, t_uncertain,x,g = generate_collocation_points_with_gp(model,N_f,x_f, t_f) \n",
    "        if epoch % resample_every == 0 and epoch > 0 :\n",
    "            x_uncertain, t_uncertain,uncertainties,gp_model = generate_collocation_points_with_gp_res(model,N_f,x_f, t_f,x_bc,t_bc,u_bc)\n",
    "\n",
    "            x_f = x_uncertain\n",
    "            t_f = t_uncertain\n",
    "\n",
    "    return thresh_e,val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a960c9a-7afd-4977-9793-aea4195061f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_residual(model, x, t):\n",
    "    x.requires_grad = True\n",
    "    t.requires_grad = True\n",
    "\n",
    "    u = model(x, t)  # Predict u(x, t)\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    # Inviscid Burgers' equation: u_t + u * u_x = 0\n",
    "    f = u_t + u * u_x\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b85cc-9b23-49de-897a-2dad4f3c2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_residual(model, x, t):\n",
    "    x = x.detach().requires_grad_()\n",
    "    t = t.detach().requires_grad_()\n",
    "\n",
    "    u = model(x, t)  # Predict u(x, t)\n",
    "\n",
    "    # Compute ∂u/∂t\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    # Compute ∂u/∂x\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    # Compute ∂²u/∂x²\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    # Burgers' equation residual: u_t + u * u_x - ν * u_xx\n",
    "    nu = 0.01 / np.pi\n",
    "    # residual = u_t + u * u_x - nu * u_xx\n",
    "    residual = u_t + u * u_x \n",
    "    return residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259b57d-2896-42ba-a676-d29978919577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#different initial/boundary conditions\n",
    "def initial_condition(x, condition_type=\"sin\"):\n",
    "    if condition_type == \"sin\":\n",
    "        return (-10 * torch.sin(np.pi * x.cpu())).to(device)\n",
    "    elif condition_type == \"gaussian\":\n",
    "        # return torch.exp(-10 * (x - 0.5) ** 2).to(device)\n",
    "        mu=0.5\n",
    "        sigma=0.1\n",
    "        return torch.exp(-((x - mu)**2) / (2 * sigma**2))\n",
    "    elif condition_type == \"step\":\n",
    "        return torch.where(x < 0.5, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown initial condition type\")\n",
    "\n",
    "def boundary_condition(x, t, boundary_type=\"dirichlet\"):\n",
    "    if boundary_type == \"dirichlet\":\n",
    "        return torch.zeros_like(x).to(device)  # u(0,t) = 0, u(L,t) = 0\n",
    "    elif boundary_type == \"neumann\":\n",
    "        return torch.autograd.grad(model(x, t), x, torch.ones_like(x), create_graph=True)[0]\n",
    "    elif boundary_type == \"periodic\":\n",
    "        return model(x, t) - model(x + L, t)  # u(0,t) = u(L,t)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown boundary condition type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d54f7c-761e-4574-8a9c-fae8981bc435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import gc\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "N_f = 1000\n",
    "L, T = 1.0, 1.0\n",
    "N_bc = 100\n",
    "epochs = 5000\n",
    "threshold = 0.05\n",
    "\n",
    "x_f, t_f = generate_collocation_points(N_f, L, T)\n",
    "x_val, t_val = generate_collocation_points(10000, L, T)\n",
    "\n",
    "initial_conditions = [\"sin\", \"step\" ,\"gaussian\"]\n",
    "boundary_conditions = [\"dirichlet\", \"neumann\", \"periodic\"]\n",
    "\n",
    "\n",
    "results_base = []\n",
    "results_import = []\n",
    "results_gauss = []\n",
    "results_gauss_res = []\n",
    "\n",
    "num_experiments = 20\n",
    "\n",
    "for ic in initial_conditions:\n",
    "    for bc in boundary_conditions:\n",
    "        print(f\"\\n==== Running experiments for IC: {ic}, BC: {bc} ====\")\n",
    "        \n",
    "        val_losses_base = []\n",
    "        val_losses_import = []\n",
    "        val_losses_gauss = []\n",
    "        val_losses_gauss_res = []\n",
    "        \n",
    "        threshold_base_val = []\n",
    "        threshold_import_val = []\n",
    "        threshold_gauss_val = []\n",
    "        threshold_gauss_val_res = []\n",
    "\n",
    "        x_bc = torch.linspace(0, L, N_bc).view(-1, 1).to(device)\n",
    "        t_bc = torch.zeros_like(x_bc).to(device)\n",
    "        u_bc = initial_condition(x_bc, ic)\n",
    "        for exp in range(num_experiments):\n",
    "            \n",
    "            print(f\"--- Experiment {exp+1}/{num_experiments} ---\")\n",
    "            \n",
    "            # Boundary/initial condition setup\n",
    "            \n",
    "            \n",
    "            # ---- BASE ----\n",
    "            model_base = PINN(layers).to(device)\n",
    "            optimizer = optim.Adam(model_base.parameters(), lr=1e-3)\n",
    "            thresh_base,base_scores = train(model_base, optimizer, x_f, t_f, x_val, t_val, x_bc, t_bc, u_bc, epochs=epochs, threshold=threshold)\n",
    "            loss_val = loss_function(model_base, x_val, t_val, x_bc, t_bc, u_bc).item()\n",
    "            print(loss_val)\n",
    "            val_losses_base.append(loss_val)\n",
    "            threshold_base_val.append(thresh_base)\n",
    "            del model_base, optimizer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # ---- IMPORTANCE ----\n",
    "            model_import = PINN(layers).to(device)\n",
    "            optimizer = optim.Adam(model_import.parameters(), lr=1e-3)\n",
    "            thresh_import,import_scores = train_import(model_import, optimizer, N_f, x_f, t_f, x_val, t_val, x_bc, t_bc, u_bc, epochs=epochs, resample_every=500, threshold=threshold)\n",
    "            loss_val = loss_function(model_import, x_val, t_val, x_bc, t_bc, u_bc).item()\n",
    "            print(loss_val)\n",
    "            val_losses_import.append(loss_val)\n",
    "            threshold_import_val.append(thresh_import)\n",
    "            del model_import, optimizer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # ---- GAUSSIAN PROCESS ----\n",
    "            model_GP = PINN(layers).to(device)\n",
    "            optimizer = optim.Adam(model_GP.parameters(), lr=1e-3)\n",
    "            thresh_gp,output_scores = train_GP(model_GP, optimizer, N_f, x_f, t_f, x_val, t_val, x_bc, t_bc, u_bc, epochs=epochs, resample_every=500, threshold=threshold)\n",
    "            loss_val = loss_function(model_GP, x_val, t_val, x_bc, t_bc, u_bc).item()\n",
    "            print(loss_val)\n",
    "            val_losses_gauss.append(loss_val)\n",
    "            threshold_gauss_val.append(thresh_gp)\n",
    "            del model_GP, optimizer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "            # ---- GAUSSIAN PROCESS RESIDUALS----\n",
    "            model_GP_res = PINN(layers).to(device)\n",
    "            optimizer = optim.Adam(model_GP_res.parameters(), lr=1e-3)\n",
    "            thresh_gp,res_scores = train_GP_res(model_GP_res, optimizer, N_f, x_f, t_f, x_val, t_val, x_bc, t_bc, u_bc, epochs=epochs, resample_every=500, threshold=threshold)\n",
    "            loss_val = loss_function(model_GP_res, x_val, t_val, x_bc, t_bc, u_bc).item()\n",
    "            print(loss_val)\n",
    "            val_losses_gauss_res.append(loss_val)\n",
    "            threshold_gauss_val_res.append(thresh_gp)\n",
    "            del model_GP_res, optimizer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        # === AVERAGE + STD ===\n",
    "        avg_base, std_base = np.mean(val_losses_base), np.std(val_losses_base)\n",
    "        avg_import, std_import = np.mean(val_losses_import), np.std(val_losses_import)\n",
    "        avg_gauss, std_gauss = np.mean(val_losses_gauss), np.std(val_losses_gauss)\n",
    "        avg_gauss_res, std_gauss_res = np.mean(val_losses_gauss_res), np.std(val_losses_gauss_res)\n",
    "\n",
    "        avg_t_base, std_t_base = np.mean(threshold_base_val), np.std(threshold_base_val)\n",
    "        avg_t_import, std_t_import = np.mean(threshold_import_val), np.std(threshold_import_val)\n",
    "        avg_t_gauss, std_t_gauss = np.mean(threshold_gauss_val), np.std(threshold_gauss_val)\n",
    "        avg_t_gauss_res, std_t_gauss_res = np.mean(threshold_gauss_val_res), np.std(threshold_gauss_val_res)\n",
    "\n",
    "        print(f\"Average Validation Loss (Base):   {avg_base:.6f} ± {std_base:.6f}\")\n",
    "        print(f\"Average Validation Loss (Import): {avg_import:.6f} ± {std_import:.6f}\")\n",
    "        print(f\"Average Validation Loss (Gauss):  {avg_gauss:.6f} ± {std_gauss:.6f}\")\n",
    "        print(f\"Average Validation Loss (Gauss Res):  {avg_gauss_res:.6f} ± {std_gauss_res:.6f}\")\n",
    "        \n",
    "        results_base.append([ic, bc, avg_base, std_base, avg_t_base, std_t_base])\n",
    "        results_import.append([ic, bc, avg_import, std_import, avg_t_import, std_t_import])\n",
    "        results_gauss.append([ic, bc, avg_gauss, std_gauss, avg_t_gauss, std_t_gauss])\n",
    "        results_gauss_res.append([ic, bc, avg_gauss_res, std_gauss_res, avg_t_gauss_res, std_t_gauss_res])\n",
    "\n",
    "        \n",
    "\n",
    "        a = base_scores\n",
    "        b = import_scores\n",
    "        c = output_scores\n",
    "        d = res_scores\n",
    "\n",
    "        epochs_range = [i * 500 for i in range(11)] \n",
    "        # Plot them all on the same figure\n",
    "        plt.plot(epochs_range, a, label='Base PINN loss')\n",
    "        plt.plot(epochs_range,b, label='Importance Sampling PINN')\n",
    "        plt.plot(epochs_range,c, label='GP Output PINN')\n",
    "        plt.plot(epochs_range,d, label='GP Residual PINN')\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Average Loss Over Time')\n",
    "        plt.title(f'Loss Over time for IC: {ic}, BC: {bc}')\n",
    "        plt.xticks(ticks=[i * 500 for i in range(11)])\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc399d85-59f0-4c5c-b003-183f1a5680be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1D arrays\n",
    "        a = base_scores\n",
    "        b = import_scores\n",
    "        c = output_scores\n",
    "        d = res_scores\n",
    "\n",
    "        # Plot them all on the same figure\n",
    "        plt.plot(a, label='base_scores')\n",
    "        plt.plot(b, label='import_scores')\n",
    "        plt.plot(c, label='output_scores')\n",
    "        plt.plot(d, label='res_scores')\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss Over time for IC: {ic}, BC: {bc}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eecd5f-979e-45da-aaa5-d5d399544825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrames\n",
    "df_base = pd.DataFrame(results_base, columns=[\"Initial Condition\", \"Boundary Condition\", \"Avg Validation Loss\", \"Std Validation Loss\", \"Avg Threshold\",\"Std Threshold\"])\n",
    "df_import = pd.DataFrame(results_import, columns=[\"Initial Condition\", \"Boundary Condition\", \"Avg Validation Loss\", \"Std Validation Loss\", \"Avg Threshold\",\"Std Threshold\"])\n",
    "df_gauss = pd.DataFrame(results_gauss, columns=[\"Initial Condition\", \"Boundary Condition\", \"Avg Validation Loss\", \"Std Validation Loss\", \"Avg Threshold\",\"Std Threshold\"])\n",
    "df_gauss_res = pd.DataFrame(results_gauss_res, columns=[\"Initial Condition\", \"Boundary Condition\", \"Avg Validation Loss\", \"Std Validation Loss\", \"Avg Threshold\",\"Std Threshold\"])\n",
    "# Save to CSV\n",
    "df_base.to_csv(\"Results_Experiments/base_final55.csv\", index=False)\n",
    "df_import.to_csv(\"Results_Experiments/import_final55.csv\", index=False)\n",
    "df_gauss.to_csv(\"Results_Experiments/GP_final55.csv\", index=False)\n",
    "df_gauss_res.to_csv(\"Results_Experiments/GP_final_res55.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d282c-93f8-48af-b14d-60cb271dd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_base = pd.read_csv(\"Results_Experiments/base_final55.csv\")\n",
    "df_import = pd.read_csv(\"Results_Experiments/import_final55.csv\")\n",
    "df_gauss = pd.read_csv(\"Results_Experiments/GP_final55.csv\")\n",
    "df_gauss_res = pd.read_csv(\"Results_Experiments/GP_final_res55.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2879b-445a-43ca-ae2f-b88a1a929d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c72c0-424b-439d-b2f9-90133e190451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d59f07-3ad6-4bfe-af11-d851d51b04b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd38ac-7968-4bf4-803d-ca0d3f15dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gauss_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c6a15-d4ec-4aac-b323-512a39f6ffe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
